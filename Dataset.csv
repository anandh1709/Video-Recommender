Approx Duration (ms),Video Description (Original),Video Title (Original),Video Publish Timestamp
281000,"If you are a student or researcher in any field, you'll eventually run into the need to learn to code. Programming will make you self sufficient, it will teach you how to think, it improves your collaborative skills and it can take your career to new heights. If you want to stay ahead of your peers and relevant in your field, overcome your fears and start coding!",01 - Why do you need to learn programming?,2019-05-05T22:09:40+00:00
394000,"What is programming?
A few programming terms.
What is an IDE?",02 - What is programming?,2019-05-05T22:23:57+00:00
579000,As a coder you need to understand the basics of command prompt. You need not be an expert but you should be able to change folders or run a python file from command line. This video provides a short explanation of command prompt.,03 - What is command prompt?,2019-05-06T23:02:25+00:00
644000,It is very important to understand what a digital image means before you can learn about processing these images. This video provides a short introduction to digital images. Spoiler alert - they are numpy arrays!,04 - What is a digital image?,2019-05-06T23:07:35+00:00
519000,If you are an absolute beginner programmer then you may need a bit of introduction to Python. The video also introduces a few Python terms like integrated development environment (IDE).,05 - What is Python?,2019-05-06T23:14:26+00:00
1145000,"This video provides an overview of various development environments (IDE) and explains Anaconda Spyder interface. It also introduces a few core Python operators. You will also learn about different data types; int, float and str.",06 - Python basics - IDE & operators,2019-05-06T23:28:29+00:00
1214000,"This is a continuation of previous video and introduces logical operators. You will also learn to code your first program using E = mC**2 and F = ma formule. In addition, you will learn how to take input via keyboard (provided by the user) which involves converting data type from string to an integer.",07 - Python basics - logical operators and basic math,2019-05-06T23:37:05+00:00
148000,This video warns you about potential round-off errors in Python.,08 - A warning about round off errors in Python,2019-05-06T23:37:41+00:00
716000,In this video you'll learn about using if and else statements in Python. These are very common statements in many languages.,09 - if else elif statements in Python,2019-05-07T00:01:55+00:00
1284000,"Images are nothing but a list of numbers. Therefore, it is important to understand various types of lists in Python. This video introduces lists, tuples, dictionaries, and numpy arrays.",10 - lists tuples and dictionaries,2019-05-07T00:24:18+00:00
2487000,"Images are multi-dimensional numpy arrays. This video introduces you to numpy arrays, including opening an image and looking at the array values associated with that image.",11 - numpy arrays,2019-05-07T02:00:45+00:00
1007000,Images are multidimensional lists or numpy arrays. This video summarizes information from the previous tutorials on lists and numpy arrays.,12 - Python Recap of lists and numpy arrays,2019-05-07T14:36:39+00:00
1365000,"Loops are used to perform useful tasks over and over until a condition is met. With loops you can repeat a set of statements even on each item in a list, tuple, dictionary or numpy array. This video provides an introduction to loops in Python including, for and while loops.",13 - for and while Loops in Python,2019-05-07T14:43:48+00:00
1298000,"A function in Python enables modularity and makes blocks of your code reusable. print() is a built in function that gets used quite often. As a coder, you can build your own functions in Python. This video introduces you to functions and explains its use with basic image processing examples.",14 - Python Functions,2019-05-07T16:06:54+00:00
1376000,"Functions allow us to modularize the code and make it reusable. But functions do not store any information, for example like variables do. This is solved by object oriented programming where it puts functions and variables together in a class. This video provides an introduction to classes in Python and uses cell positions and distance measurement as an example.",15 - Python Classes,2019-05-07T16:16:13+00:00
1111000,"Digital image processing in Python is mostly done via numpy array manipulation. This video provides a quick overview of digital images, their data types and numpy array manipulation to modify images.",16 - Understanding digital images for Python processing,2019-05-07T22:43:43+00:00
3298000,"There are many ways to reading images in Python. This video explains image reading using popular libraries in Python. It also explains a couple of ways to read non-standard images including Carl Zeiss' CZI and Bioformats' OME-TIFF images.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",17 - Reading images in Python,2019-05-08T20:20:32+00:00
1871000,"Pillow (PIL) is an image manipulation and processing library in Python. This video covers the basics of pillow library using a few examples for image resizing, cropping, and applying them to multiple images.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",18 - Image processing using pillow in Python,2019-05-09T18:17:13+00:00
1634000,"Scipy is a python library that is part of numpy stack. It contains modules for linear algebra, FFT, signal processing and image processing. Scipy is not specifically designed for image processing but it has a few useful tools. This video covers the basic image processing functionality of scipy library in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",19 - image processing using scipy in Python,2019-05-09T23:16:23+00:00
2245000,"Scikit-image is a Python library dedicated towards image processing. This video explains a few useful functions from the scikit-image library including, resize, reshape, edge detectors and segmentation process for a microscopy based assay (wound healing or scratch assay).

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",20 - Introduction to image processing using scikit-image in Python,2019-05-13T23:51:16+00:00
1688000,"Scratch assay or Wound healing assay is one of the common assays in biology. It involves segmenting time series images for 'clean' vs cell-filled regions. This task can be very challenging as histogram segmentation does not yield desired results. This video tutorial explains the segmentation process using Entropy filter and Otsu thresholding in Python. It also explains the process of looping through images to analyze and plot the results.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",21 - Scratch assay analysis with just 5 lines code in Python,2019-05-15T01:40:42+00:00
1211000,"A compromise on signal to noise for speed often results in noisy microscope images. These images can be denoised to extract relevant information. There are many choices for denoising algorithms in Python but some are better than others. This video demonstrates a few denoising functions from Sciki-image and numpy libraries.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",22 - Denoising microscope images in Python,2019-05-16T21:18:51+00:00
1452000,"Microscope images are acquired to extract information about a sample. In order to properly quantify the information the images often need to be segmented for various features of interest. This tutorial explains the process of image segmentation in Python using histogram based thresholding. In other words, this video tutorial explains the process of defining grey level ranges for each feature of interest to perform segmentation. The tutorial also covers basic image processing operations in order to clean up the segmented regions.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",23 - Histogram based image segmentation in Python,2019-05-17T16:29:43+00:00
1618000,"Histogram based image segmentation is not possible in many cases. This tutorial explains the use of Random Walker segmentation in Python using a noisy backscattered electron (BSE) image collected from an alloy.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",24 - Random Walker segmentation in Python,2019-05-17T22:10:26+00:00
1210000,"openCV is a library of programming functions mainly aimed at computer vision but also very helpful for processing microscope images. This tutorial explains a few basic operations in openCV to read & resize images and to split & merge channels.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists","25 - Reading Images, Splitting Channels, Resizing using openCV in Python",2019-05-20T21:47:26+00:00
1259000,"Cleaning up noise without losing information is important for microscope images (micrographs). This tutorial explains a few ways to perform denoising in openCV. It also explains Canny edge detection, also part of opencv.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",26 - Denoising and edge detection using opencv in Python,2019-05-20T22:12:47+00:00
1232000,"Histogram equalization often makes images easy to threshold and further segment. This tutorial demonstrates the use of Contrast Limited Adaptive Histogram Equalization (CLAHE) and subsequent thresholding using openCV library in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",27 - CLAHE and Thresholding using opencv in Python,2019-05-21T01:56:46+00:00
1231000,"It becomes necessary to cleanup 'noise' after image thresholding. This tutorial explains the use of morphological operators like erosion, dilation and opening to cleanup post-thresholding noise.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",28 - Thresholding and morphological operations using openCV in Python,2019-05-22T00:32:41+00:00
1826000,"Keypoints are points of interest in an image that can be used to compare images and perform tasks such as image alignment and registration. These points can be automatically detected (defined) by the system using algorithms such as SIFT, SURF, and ORB. Some of these algorithms can also define the descriptors to make key points really useful for image processing tasks. This tutorial introduces a few key point detectors and descriptors.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists","29 - Key points, detectors and descriptors in openCV",2019-05-25T16:08:40+00:00
2802000,"Image registration is one of the common tasks performed by microscopists working with 3D data sets. There are excellent algorithms available for registration. Homography allows image registration by using descriptors of keypoints generated by SIFT, SURF, ORB or similar keypoint descriptors. This tutorial demonstrates the process using a distorted image and corresponding reference image.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",30 - Image registration using homography in openCV,2019-05-28T22:32:40+00:00
1097000,"Image (data) analysis may require you to save results into a csv file. Or if you would like to perform data analysis in Python you may need to read/import csv files. This tutorial explains a few ways to read and write csv files into and out of Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",31 - A few ways to read and write csv files in Python,2019-05-30T17:57:49+00:00
2469000,"Understanding grains in metal alloys is important for quality control and research. Optical and electron microscopes are used to image well-polished alloy surfaces to capture grain structure. Automating the process of grain analysis not only speeds up QC/research but also yields consistent & repeatable results. 

This video tutorial explains the image processing tasks required to generate grain analysis summary and to capture it in a csv file.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",32 - Grain size analysis in Python using a microscope image,2019-05-31T20:33:46+00:00
2328000,"Threshold based segmentation will not yield good results if the features of interest cannot be easily distinguished using the histogram of pixel values. For example, grains in a microscope image (or cells) will not be efficiently separated thus resulting in wrong insights about the sample. 

Watershed assisted segmentation is ideal for these situations. The image can be thresholded first using traditional approach to identify definitely positive and definitely negative regions. Then, watershed algorithm can be used to fill the gaps. This video demonstrates the use of watershed algorithm using a microscope image showing grains and boundaries (same example as previous video in this playlist).

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",33 - Grain size analysis in Python using watershed,2019-06-04T15:00:18+00:00
1103000,"This tutorial is an extension of the previous tutorial where grain size analysis was performed using a single image. In this tutorial you will learn 2 ways to apply the grain analysis process to multiple images in a directory. Please remember that there are many ways to handle multiple images but this video shows two easiest approaches, especially for beginner programmers.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",34 - Grain size analysis in Python using watershed - multiple images,2019-06-05T16:56:23+00:00
924000,"This tutorial explains the process of cell nuclei segmentation followed by counting and sizing the nuclei. The results are exported into a csv file for further analysis. The tutorial is a follow up to the previous 3 videos in this playlist where grain segmentation was performed using traditional and watershed segmentation, respectively. The video is dedicated to all biologists who are wasting their valuable research time in doing these task manually!

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",35 - Cell Nuclei analysis in Python using watershed segmentation,2019-06-06T22:34:31+00:00
1379000,"Data analysis is an essential step after image processing. While Excel provides an easy interface for data analysis and plotting it does come with a lot of limitations, especially when it comes to handling big data. Pandas library in Python provides great tools for data manipulation and analysis. This tutorial provides an introduction to Pandas and covers the topics of data loading (using a csv file) and basic handling of data.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",36 - Introduction to Pandas - Data reading and handling,2019-06-25T22:29:04+00:00
905000,"This tutorial explains the basic data manipulation tasks using Pandas library in Python. These tasks include editing rows and columns.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",37 - Introduction to Pandas - Data Manipulation,2019-06-25T23:02:58+00:00
1274000,"This tutorial explains data sorting tasks using Pandas library in Python. Sorting and filtering can help you understand and interpret data in a meaningful way.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",38 - Introduction to Pandas - Data Sorting,2019-06-26T22:30:35+00:00
731000,"This tutorial explains data grouping using Pandas library in Python. Grouping can help with better understanding the information contained within the data. For example, if you have bi-modal distribution of cells or grains, grouping them as small & large enables you to better understand the morphology information for each cell/grain type.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",39 - Introduction to Pandas -  Grouping Data,2019-06-26T22:35:54+00:00
906000,Data comes in many forms and encountering missing data is inevitable. Pandas is very flexible when it comes to handling missing data. This tutorial explains ways to check for missing data (a.k.a Null values or NaN) and also demonstrates a couple of ways to handle missing data (delete rows/columns or fill with a meaningful value).,40 - Introduction to Pandas - Dealing with missing (null) data,2019-06-26T22:44:12+00:00
1663000,"Plotting is the best way to communicate information contained in images and other forms of data. Pandas is an excellent library in Python for data analysis and it also contains a few plotting tools (powered by Matplotlib). This tutorial covers the core plotting tools in Pandas library.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",41 - Introduction to Pandas  - Plotting,2019-07-01T16:04:29+00:00
1743000,"Seaborn is a complimentary plotting library to Matplotlib for statistical data visualization. It enhances the visualization capabilities and makes working with Pandas DataFrames easy. This tutorial covers the basics of Seaborn library and goes through a few example plots.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",42 - Introduction to Seaborn Plotting in Python,2019-07-01T16:23:00+00:00
1281000,"A high level explanation of machine learning - for absolute beginners.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",43 - What is machine learning anyway?,2019-07-09T22:11:15+00:00
1016000,"Linear regression is a type of predictive analysis that is used in many fields including sciences, engineering and forecasting. This video explains linear regression and introduces relevant terminology such as least squares, cost function, and gradient descent.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",44 - What is linear regression?,2019-09-13T22:47:39+00:00
1520000,"This is a follow up to the previous video where I explained the theory behind Linear Regression. In this video, I go through the process of using Sci-kit learn in Python to perform Linear Regression on data imported from a csv file.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",45 - Linear regression using Sci-Kit Learn in Python,2019-09-17T18:18:51+00:00
828000,"When you build a model using machine learning or other means it is important to validate it with a test data set. It is important to test the model on data that the algorithm did not use for training purposes. It is also important for the test data set to follow similar probability distribution as the training data set. The easiest way to achieve this is by splitting the data into training and testing data sets. This video tutorial explains this process in Python using Scikit-learn library.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",46 - Splitting data into training and testing sets for machine learning,2019-09-20T23:23:38+00:00
798000,"In the previous video I've explained the concept of linear regression where a single independent variable (X) was used to model the dependent variable (Y). In reality, multiple independent factors (variables) may influence the response. This tutorial explains the use of regression to model single independent variable using multiple dependent variables.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",47 - Multiple Linear Regression with SciKit-Learn in Python,2019-09-24T20:53:10+00:00
827000,"Despite its name, Logistic regression is primarily used for classification type of problems. In other words, it is a statistical method in which there are one or more independent variables that determine an outcome.  Binary logistic regression examples include, sick vs healthy cell, defective vs good part or small vs large particle. This tutorial explains the basics of logistic regression.",48 - What is logistic regression?,2019-09-24T21:02:25+00:00
2316000,"This tutorial explains the few lines to code logistic regression in Python using scikit-learn library.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",49 - Logistic Regression using scikit-learn in Python,2019-09-24T21:23:04+00:00
1015000,"k-means is probably the simplest unsupervised machine learning algorithm that uses clustering techniques to sort data into various clusters (classes). This video tutorial explains the basics of k-means and walks you through the process of coding it in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",50 - What is k-means clustering and how to code it in Python?,2019-09-26T19:27:53+00:00
1097000,k-means is one of the best unsupervised machine learning algorithms. Do you know that it can be used to segment images? This tutorial explains the use of k-means to automatically segment microscope images.,51 - Image Segmentation using K-means,2019-09-27T23:28:18+00:00
1768000,"Gaussian mixture model (GMM) is a type of clustering algorithm that falls under the umbrella of unsupervised machine learning techniques. As the name indicates, GMM models each cluster to a Gaussian distribution with specific mean and variance. It follows the expectation maximization algorithm principles to find the required parameters to model different Gaussians. 

This tutorial explains the basics of GMM and also goes through the process of writing few lines of code to segment a microscope image.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",52 - What is GMM and how to use it for Image segmentation?,2019-09-30T21:40:43+00:00
820000,"Unsupervised machine learning (e.g. k-means, GMM) can be used to segment data into various clusters. But how do you know the optimal number of clusters to divide your data? This video explains the use of AIC / BIC to identify the optimal number of parameters for unsupervised models.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",53  - How to pick optimal number of parameters for your unsupervised machine learning model?,2019-10-04T23:47:39+00:00
836000,"This video provides a quick explanation of unsupervised and supervised machine learning learning approaches, respectively. As the name suggests, supervised requires information from the user in order to train the algorithm and unsupervised works directly with minimal input from the user.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",54 - Unsupervised and supervised machine learning  - a reminder,2019-10-07T20:07:25+00:00
897000,"There are many reasons for instrument manufacturers to invent proprietary file formats for their microscope images. The primary reason is to pack relevant information into their images so you can readily work with them in their respective software packages. 

This makes it difficult, sometimes near impossible, to handle these images outside of their respective software packages. Luckily, a few of these manufacturers share libraries to handle images in other software platforms.  Also, a few good Samaritans develop libraries by working closely with the instrument manufacturers. 

This tutorial explains the process of opening such proprietary files in Python, by using .czi file as an example.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",55 - How to read proprietary microscope images into Python,2019-10-08T23:18:44+00:00
595000,"Feature engineering is an essential part of traditional machine learning. Selecting appropriate features not only speeds up the analysis time but also helps improve your analysis accuracy. This tutorial explains features from an image processing perspective.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",56 - What are features in machine learning?,2019-10-10T22:07:31+00:00
1063000,"Features describe the data you're trying to model. For image processing and machine vision, features can be defined and extracted via digital image filters. This tutorial explains the process of generating features in Python for machine learning purposes.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",57 - How to generate features in Python for machine learning?,2019-10-11T20:25:26+00:00
1559000,"Segmenting your images is an easy task if you can separate regions of interest using histogram (pixel values). But life is not that easy, most scientific images contain regions of interest that require additional information for segmentation. Gabor is a bandpass filter that can be used to extract texture, orientation and other relevant information from your images. This information can then be supplied to your favorite machine learning algorithm. This tutorial explains the basics of Gabor filter, including its implementation in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",58 - What are Gabor filters?,2019-10-14T17:32:32+00:00
1306000,"There are many machine learning classifiers that have been developed over years for various purposes. While deep learning is slowly replacing these traditional classifiers, Random Forest still beats deep learning for applications with limited training data. Microscope image segmentation is one such application where users often work with limited training data. This tutorial provides a quick introduction to Random Forest - no coding!

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",59 - What is Random Forest classifier?,2019-10-15T22:11:38+00:00
1937000,"This video explains the implementation of Random Forest in Python using data imported from a csv file. Image segmentation using feature engineering and Random Forest will be covered in the upcoming videos.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",60 - How to use Random Forest in Python?,2019-10-18T00:39:12+00:00
964000,Gabor is a great kernel to generate (extract) features from images for machine learning based image segmentation. This tutorial explains the process of generating feature banks using Gabor in Python. The tutorial also explains the process of applying these filters to an image.,61 - How to create Gabor feature banks for machine learning,2019-10-18T22:47:36+00:00
390000,"This video provides an overview of the tasks required for image segmentation using traditional machine learning approach. The follow up series of videos (total 5) will go through each step in detail including, feature extraction, Random Forest training and validation, feature ranking, saving model and segmenting multiple images in a folder.

Dataset for semantic segmentation can be downloaded from here: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view",62 - Image Segmentation using traditional machine learning - The plan,2019-10-20T03:24:21+00:00
1372000,"This is part 1 of the 5 part series tutorials that covers the topic of image segmentation using feature engineering and random forest classification. In this tutorial you'll learn how to extract features from your training images and organize the data in Pandas data frame to be ready for machine learning classification.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists

Dataset for semantic segmentation can be downloaded from here: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view",63 - Image Segmentation using traditional machine learning Part1 - FeatureExtraction,2019-10-21T03:02:07+00:00
1075000,"This is part 2 of the 5 part series of videos on image segmentation using traditional machine learning. This video explains the process of training a Random Forest classifier using the feature vectors generated from the training image(s).

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists

Dataset for semantic segmentation can be downloaded from here: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view",64 - Image Segmentation using traditional machine learning - Part2 Training RF,2019-10-23T11:49:17+00:00
452000,"This is part 3 of the 5 part series of videos on image segmentation using traditional machine learning. 

Not all features used for machine learning contribute equally towards building the model. The feature importance depends on the type of image and regions of interest for segmentation. This video explains the process of reporting feature rankings during Random Forest training process. 

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists

Dataset for semantic segmentation can be downloaded from here: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view",65 - Image Segmentation using traditional machine learning - Part3 Feature Ranking,2019-10-23T11:57:13+00:00
443000,"This is part 4 of the 5 part series of videos on image segmentation using traditional machine learning. This video explains the process of saving (Pickling) the trained model and using it to segment an image.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists

Dataset for semantic segmentation can be downloaded from here: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view",66 - Image Segmentation using traditional machine learning - Part4 Pickling Model,2019-10-23T12:02:00+00:00
1061000,"This is part 5 of the 5 part series of videos on image segmentation using traditional machine learning. 

This video explains the process of segmenting multiple images by loading a trained model.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists

Dataset for semantic segmentation can be downloaded from here: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view",67 - Image Segmentation using traditional machine learning - Part5 Segmenting Images,2019-10-23T12:08:06+00:00
735000,"Support Vector Machines (SVM) is one of the many 'traditional' machine algorithms used in data / image analysis. In fact, SVM and Random Forest are the two primary algorithms for traditional machine learning. This tutorial provides a quick overview of SVM.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",68 - Quick introduction to Support Vector Machines (SVM),2019-11-13T16:39:38+00:00
901000,"SVM and Random Forest are the two primary 'traditional' machine learning algorithms used for data / image analysis. Therefore, it only makes sense to compare both approaches for the task of image segmentation. Random Forest based image segmentation has been covered in a previous tutorial. This tutorial goes through the process of switching the classifier from Random Forest to SVM in Python and then comparing results between both classifiers.  Spoiler alert - Random Forest is much faster and more accurate than SVM for image segmentation purposes.",68b - SVM vs. Random Forest for image segmentation,2019-11-13T16:55:32+00:00
2289000,"Bag of words (BOW) model is used in natural language processing for document classification where the frequency of each word is used as a feature to train a classifier. This approach can be extended to classify images by treating each feature in the image as a word, bag of visual words (BOVW). 

For BOVW approach, the features from image can be extracted using one of many feature extractors such as SIFT, HOG, SURF, and ORB. The extracted features can then be clustered into various bins using a clustering algorithm such as k-means. Finally, the features can be used to train a classifier such as Random Forest or SVM.

This tutorial explains the Python approach for BOVW based image classification.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists

Malaria cell data set: https://www.kaggle.com/datasets/iarunava/cell-images-for-detecting-malaria",69 - Image classification using Bag of Visual Words (BOVW),2019-11-15T11:00:06+00:00
2028000,"This video tutorial provides a quick overview of deep learning and neural networks. It also provides brief explanation to various terms used in deep learning such as Convolution, Max pooling, batch normalization, dropout, flatten, activation, optimizer, and loss function.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",70 - An overview of deep learning and neural networks,2019-11-18T13:00:03+00:00
2950000,"This video goes through the process of classifying malarial cells into parasitized and healthy cells using convolutional neural networks (CNN) in Python. The dataset can be downloaded directly from the national library of medicine website - https://lhncbc.nlm.nih.gov/publication/pub9932.

OR 
from here:
https://www.kaggle.com/datasets/iarunava/cell-images-for-detecting-malaria

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",71 - Malarial cell classification using CNN,2019-11-20T13:00:04+00:00
732000,"Training machine learning models using deep learning can take hours to days depending on the data set size. Therefore, GPU acceleration is a must to cut down training time from days to hours and from hours to minutes. Just because your workstation contains a graphics card doesn't mean it is ready for GPU computation. This tutorial explains an easy way to get your GPU ready for Tensorflow (& Keras) based deep learning. 

To find out GPU compute compatibility of your graphics card: https://developer.nvidia.com/cuda-gpus

 Enter this command in your Anaconda prompt window to install tensorflow for GPU: conda create --name tf_gpu tensorflow-gpu
This creates a new environment for your Anaconda that lets you use GPU for deep learning.",72 - Getting Windows10 system ready for GPU accelerated deep learning,2019-11-22T11:00:02+00:00
1093000,"Many deep learning architectures have been proposed to solve various image processing challenges. SOme of the well known architectures include LeNet, ALexNet, VGG, and Inception. U-net is a relatively new architecture proposed by Ronneberger et al. for semantic image segmentation. This video explains the U-Net architecture; a good understanding is essential before coding. 

Link to the original U-Net paper: https://arxiv.org/abs/1505.04597

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",73 - Image Segmentation using U-Net - Part1 (What is U-net?),2019-12-03T11:00:01+00:00
1155000,"The previous video in this playlist (labeled Part 1) explains U-Net architecture. This video tutorial explains the process of defining U-Net in Python using Keras API.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",74 - Image Segmentation using U-Net - Part 2 (Defining U-Net in Python using Keras),2019-12-05T11:00:01+00:00
410000,"This short video tutorial explains the meaning of trainable parameters using a simple example calculation. In summary, trainable parameters are the weights and biases that get updated during the training process.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",75 - Image Segmentation using U-Net - Part 3 (What are trainable parameters?),2019-12-06T11:00:18+00:00
838000,"This part 4 video of the 6 part series explains the process of fitting a model and other useful actions as part of U-Net model training process. Since deep learning training can take hours or even days, it is necessary to define relevant checkpoints to save work in the event something happens during training. Also, it is important to define early stopping if the model accuracy doesn't improve much after each epoch. This video tutorial explains the process of defining these parameters in Python (using Keras API).

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists","76 - Image Segmentation using U-Net - Part 4 (Model fitting, checkpoints, and callbacks)",2019-12-09T17:00:01+00:00
1740000,"In the previous 4 videos of this playlist we learnt about U-Net and how to define it in Python. In this video we'll go through the data set and understand the images that need to be segmented. The data set can be downloaded from here: https://www.kaggle.com/c/data-science-bowl-2018/data

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",77 - Image Segmentation using U-Net - Part 5 (Understanding the data),2019-12-11T13:00:19+00:00
1273000,"This is the final episode of the 6 part video series on U-Net based image segmentation. In this video we'll execute the Python code to train the model and segment a few random images. We'll also have a look at Tensorboard to visualize epoch accuracy and loss as a function of number of epochs.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",78 - Image Segmentation using U-Net - Part 6 (Running the code and understanding results),2019-12-12T13:00:04+00:00
553000,"How do you make sure you application works on other systems or even in the cloud? Docker is an emerging piece of technology that enables easy potability of your applications. This tutorial provides a basic introduction to Docker, why you need it, what it is and how to download it (link below). 

Watch the next 4 videos in this playlist to find out more about Docker and to learn about Dockerizing your Python code and running it in the cloud. 

Link to download Docker Desktop: https://www.docker.com/products/docker-desktop

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",79 - What is Docker?,2019-12-13T13:00:04+00:00
356000,"It all starts with Dockerfile, a text file with a few lines of instructions. This tutorial explains the terms Dockerfile, Docker Image, Docker Container, and Docker repository. It also explains the connection between these terms and puts things in perspective for better understanding. 

Links mentioned in the video:
Docker desktop download: https://www.docker.com/products/docker-desktop
Docker hub: https://hub.docker.com/

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists","80 - What is Dockerfile, Docker Image, and Docker Container",2019-12-16T13:00:17+00:00
902000,"This video walks you through the process of verifying Docker installation on your Windows 10 system and running your first hello world Docker container. It also introduces you to a few common Docker commands to view a list of Docker Images / Containers on your system and remove (delete) selected ones. 

Relevant URL links:
Download Docker Desktop: https://www.docker.com/products/docker-desktop
Docker repository of images: https://hub.docker.com/

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",81 - Testing Docker on Windows and introduction to basic commands,2019-12-17T13:00:04+00:00
1120000,"You've written your Python code and you installed Docker, now what? This video explains the process of Dockerizing your Python code (application). This includes defining your Dockerfile, building an image and running a container. Python code for non-local means denoising has been used as an example. 
Link to NLM from scikit-image documentation: https://scikit-image.org/docs/dev/auto_examples/filters/plot_nonlocal_means.html

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",82 - Dockerizing your python application,2019-12-18T19:22:25+00:00
904000,"What fun is creating a Docker if you cannot deploy it as an application in the cloud? While there are many cloud services (e.g. AWS, Azure) that offer infrastructure to run Dockers they all need a bit of expertise which only an advanced software engineer can handle. This tutorial provides a brief introduction to APEER, cloud based microscopy image processing platform that lets you deploy your Docker for private or public use. Of course, it also lets you connect your Docker with others' for customized workflow building. 

Relevant links: www.apeer.com

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",83 - Running your Docker in the cloud,2019-12-19T18:42:54+00:00
2536000,"Dockerizing makes your code portable enabling you to deploy it as an application in the cloud. While there are many cloud services (AWS, Azure, etc.) that can run Dockers none of them are designed for microscopy or image processing. In fact, none of them make it easy for you to connect your application with other existing applications shared by your peers. 

Apeer.com makes it easy for you to deploy your application in the cloud and customize end-to-end workflow by connecting various applications (modules). This tutorial explains the process of creating an application (module) from scratch using your Python code. No advanced coding knowledge is necessary but it helps to have some basic understanding of Python and Windows command prompt. 

If you do not have any coding knowledge then do not create a module but rather use existing modules on APEER. 

It is free to use for personal or non-profit purposes so please go ahead and check it out: www.apeer.com.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",84 - How to build a Docker (module) with your code and run it on APEER?,2019-12-20T18:21:01+00:00
1276000,"Autoencoders are artificial neural networks that can be trained to encode data (e.g. images) into a compressed representation and then trained to reconstruct the encoded data back to its original shape (& size). Autoencoders can be designed for various applications including, denoising, image colorization, and anomaly detection. This tutorial explains the basics of autoencoders and walks you through the process of building an autoencoder in Python to compress and reconstruct an input image.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",85b - An introduction to autoencoders - in Python,2020-01-13T12:00:02+00:00
835000,"Autoencoders are artificial neural networks that can be trained to encode data (e.g. images) into a compressed representation and then trained to reconstruct the encoded data back to its original shape (& size). 

Autoencoders can be designed for various applications including, denoising, image colorization, and anomaly detection. This tutorial provides and explanation of autoencoders and their uses.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",85a - What are Autoencoders and what are they used for?,2020-01-13T21:39:13+00:00
1165000,"Denoising or noise reduction in images is one of the many applications of autoencoders. This tutorial explains the process of building a denoising autoencoder in Python and testing it on images from the MNIST database.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",86 - Applications of Autoencoders - Denoising using MNIST dataset,2020-01-15T12:00:08+00:00
1188000,"Denoising or noise reduction in images is one of the many applications of autoencoders. This tutorial explains the process of building a denoising autoencoder in Python and testing it on your own images.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",87 - Applications of Autoencoders - Denoising using custom images,2020-01-17T11:00:11+00:00
1115000,"Autoencoders can be used for anomaly detection by setting limits on the reconstruction error. All 'good' data points fall within the acceptable error and any outliers are considered anomalies. This approach can be used for images or other forms of data. This video tutorial explains the process using a synthetic dataset stored in a csv file.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",88 - Applications of Autoencoders - Anomaly Detection,2020-01-20T11:00:09+00:00
1011000,"The term 'domain adaptation' can be interpreted many ways. In this video tutorial, the term refers to encoding an image into a different image. Autoencoders can be tricked by training on one set of images to decode a different set of images. For example, with enough training data and long enough training time, it is possible to artificially generate electron microscopy images using light microscopy input images.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",89 - Applications of Autoencoders - Domain Adaptation,2020-01-22T11:00:06+00:00
1251000,"Autoencoders can be tricked by training on one set of images to reconstruct a slightly different variation of those images. Using this technique, black and white images can be artificially colorized. This tutorial explains this process of image colorization using autoencoders in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",90 - Application of Autoencoders - Image colorization,2020-01-24T11:00:02+00:00
1250000,"The accuracy of deep learning depends on many factors including the amount of training data and training time (# of epochs). Unfortunately, for many microscopy (and other) applications we do not have the luxury of large amount of labeled training data. Certain tricks can be applied to augment training data but it still isn't enough. Transfer learning can help partially address this situation. Transfer learning allows for adapting networks and models trained for certain specific applications to other applications. For example, feature extraction portion of a network that's designed to discriminate cats and dogs can be used towards discriminating healthy vs malarial cells. 

This tutorial provides an introduction to transfer learning.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",91 - Introduction to transfer learning,2020-01-27T11:00:04+00:00
1574000,"The accuracy of deep learning depends on many factors including the amount of training data and training time (# of epochs). Unfortunately, for many microscopy (and other) applications we do not have the luxury of large amount of labeled training data. Certain tricks can be applied to augment training data but it still isn't enough. Transfer learning can help partially address this situation. Transfer learning allows for adapting networks and models trained for certain specific applications to other applications. For example, feature extraction portion of a network that's designed to discriminate cats and dogs can be used towards discriminating healthy vs malarial cells. 

This tutorial explains the process of using transfer learning to design an autoencoder for image colorization.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",92 - Autoencoders using transfer learning - Image colorization,2020-01-29T11:00:11+00:00
1259000,"DUE TO POOR AUDIO QUALITY I UPLOADED THE VIDEO AGAIN. 
Find the updated video here: https://youtu.be/5V6Dg1-PuqA

Many people seem to think that deep learning is going to magically solve their image processing other data analysis challenges. This is not true, especially if you do not have the right labeled data. For example, you cannot create a usable deep learning model for image denoising unless you have 100s (1000s?) of noisy and corresponding clean images. You may get better results by using traditional filters such as non-local means or anisotropic diffusion or plain old gaussian filters. This video elaborates this topic using a few image processing examples.",93 - Do not waste your time with deep learning,2020-01-31T11:00:17+00:00
2602000,"Denoising is the first step any image processing engineer working with MRI images performs. While deep learning approaches for denoising sound promising, it still remains an actively researched application (for MRI images). Until someone publishes a deep learning trained model that works on all MRI images we have resort to traditional methods for denoising. In fact, traditional methods may turn out to be far more efficient and accurate compared to deep learning. This tutorial covers a few ways to denoise MRI images using heavily researched and validated approaches such as Gaussian smoothing, bilateral filtering, anisotropic diffusion, non-local means, and Block-matching and 3D filtering (BM3D) algorithms. 

Please find the code and test images on my Github page: https://github.com/bnsreenu/python_for_microscopists",94 - Denoising MRI images (also CT & microscopy images),2020-02-03T11:00:11+00:00
1259000,"Many people seem to think that deep learning is going to magically solve their image processing other data analysis challenges. This is not true, especially if you do not have the right labeled data. For example, you cannot create a usable deep learning model for image denoising unless you have 100s (1000s?) of noisy and corresponding clean images. You may get better results by using traditional filters such as non-local means or anisotropic diffusion or plain old gaussian filters. This video elaborates this topic using a few image processing examples.",93 - Do not waste your time with deep learning (updated),2020-01-31T21:22:11+00:00
1457000,"Most digital image processing tasks involve the convolution of a kernel with the image. This tutorial explains the basics of the convolution operation by using a couple of kernels as example. It also explains the implementation of colvolution in Python using 3 different approaches, opencv, scipy.signal and scipy.ndimage, respectively.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",95 - What is digital image filtering and image convolution?,2020-02-20T08:00:01+00:00
927000,"Noise is an unfortunate result of data acquisition and it comes in many forms and from many sources. For scientific images (e.g. microscope, MRI, and EBSD),Gaussian noise arises from electronic components including detectors and sensors. In addition, salt & pepper noise may also show up from analog to digital conversion errors. Therefore, image denoising is one of the primary preprocessing operations that a researcher performs before proceeding with extracting information out of these images.

This tutorial explains Gaussian denoising filter and walks you through the process of writing a couple of lines of code in Python to implement the filter.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",96 - What is Gaussian Denoising Filter?,2020-02-21T16:58:14+00:00
589000,"Noise is an unfortunate result of data acquisition and it comes in many forms and from many sources. For scientific images (e.g. microscope, MRI, and EBSD),Gaussian noise arises from electronic components including detectors and sensors. In addition, salt & pepper noise may also show up from analog to digital conversion errors. Therefore, image denoising is one of the primary preprocessing operations that a researcher performs before proceeding with extracting information out of these images.

This tutorial explains Median filter, it's implementation in Python and also demonstrates how it is effective at cleaning up salt and pepper noise.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",97 - What is median denoising filter?,2020-02-24T10:00:11+00:00
724000,"Noise is an unfortunate result of data acquisition and it comes in many forms and from many sources. For scientific images (e.g. microscope, MRI, and EBSD),Gaussian noise arises from electronic components including detectors and sensors. In addition, salt & pepper noise may also show up from analog to digital conversion errors. Therefore, image denoising is one of the primary preprocessing operations that a researcher performs before proceeding with extracting information out of these images.

This tutorial explains the basics of bilateral filter and its use in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",98 - What is bilateral denoising filter?,2020-02-26T10:00:15+00:00
628000,"Noise is an unfortunate result of data acquisition and it comes in many forms and from many sources. For scientific images (e.g. microscope, MRI, and EBSD),Gaussian noise arises from electronic components including detectors and sensors. In addition, salt & pepper noise may also show up from analog to digital conversion errors. Therefore, image denoising is one of the primary preprocessing operations that a researcher performs before proceeding with extracting information out of these images.

This tutorial explains the non-local means (NLM) filter and its use in Python. NLM filter is especially recommended to denoise CT, uCT, and MRI images.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",99 - What is Non-local means (NLM) denoising filter?,2020-02-28T10:00:14+00:00
634000,"Noise is an unfortunate result of data acquisition and it comes in many forms and from many sources. For scientific images (e.g. microscope, MRI, and EBSD),Gaussian noise arises from electronic components including detectors and sensors. In addition, salt & pepper noise may also show up from analog to digital conversion errors. Therefore, image denoising is one of the primary preprocessing operations that a researcher performs before proceeding with extracting information out of these images.

This tutorial explains the total variation denoising filter and its use in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",100 - What is total variation (TV) denoising filter?,2020-03-02T10:00:05+00:00
659000,"Noise is an unfortunate result of data acquisition and it comes in many forms and from many sources. For scientific images (e.g. microscope, MRI, and EBSD),Gaussian noise arises from electronic components including detectors and sensors. In addition, salt & pepper noise may also show up from analog to digital conversion errors. Therefore, image denoising is one of the primary preprocessing operations that a researcher performs before proceeding with extracting information out of these images.

This tutorial explains one of the latest traditional denoising algorithms, block matching and 3D filtering (BM3D). The implementation is based on the recent (2019) publication by Ymir Mkinen et. al.: http://www.cs.tut.fi/~foi/papers/ICIP2019_Ymir.pdf

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",101 - What is block matching and 3D filtering (BM3D)?,2020-03-04T10:00:01+00:00
18000,,Swiss alps,
588000,"Unsharp mask, despite its name, is the most common image sharpening tool.  It is available in nearly every image processing software, from imageJ to Photoshop. This tutorial explains the workings of unsharp mask and demonstrates its use in Python. 

Git hub link for code: https://github.com/bnsreenu/python_for_microscopists",102 - What is unsharp mask?,2020-03-11T09:00:08+00:00
1373000,"Edge filters are often used in image processing to emphasize edges. There are many libraries in Python that offer a variety of edge filters. Most filters yield similar results and the choice of the filter depends on the application of interest. This video tutorial explains a few common edge filters including Roberts, Sobel, Prewitt, and Canny. 

Code available here: https://github.com/bnsreenu/python_for_microscopists",103 - Edge filters for image processing,2020-03-13T09:00:09+00:00
325000,"In microscopy, it is quite common to segment ridge like structures especially for researchers working with neurons or vessels. Scikit-image made it easy to detect these ridge-like structures using various filters available in their filters library. This tutorial explains the process of using these filters in Python.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",104 - Ridge Filters to detect tube like structures in images,2020-03-16T07:00:02+00:00
1594000,"Image processing filters can operate in spatial domain or frequency domain. High pass filter is an example filter that operates in the frequency domain. Fourier transform converts an image (or any signal) from spatial (or time) domain to frequency domain. 

This tutorial explains Fourier Transform by using minimal math.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",105 - What is Fourier Transform?,2020-03-18T07:00:31+00:00
910000,"Image processing filters can operate in spatial domain or frequency domain. High pass filter is an example filter that operates in the frequency domain. Fourier transform converts an image (or any signal) from spatial (or time) domain to frequency domain. 

This tutorial explains the use of Fourier transform filtering for image processing by creating a mask and blocking certain frequency signals.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",106 - Image filters using discrete Fourier transform (DFT),2020-03-20T07:00:12+00:00
691000,"Discrete cosine transformation (DCT) is similar to discrete Fourier transform except DCT uses real cosine coefficients. Most of the information for DCT is contained in a handful of coefficients making it ideal for data compression, e.g. JPEG compression. DCT is a linear function with respect to spatial domain therefore, any image processing operation would yield similar results when performed in spatial or DCT domains.

This tutorial demonstrates image denoising by averaging images in real and DCT spaces, respectively.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",112 - Averaging image stack in real and DCT space for denoising,2020-03-30T07:00:11+00:00
1023000,"If the image histogram is confined only to a small region (low contrast images), histogram equalization can be used to stretch the histogram to include all ranges. But, this type of stretching may not result in ideal results and gives too bright and too dark regions in the image. This can be especially very bad for images with large intensity variations. 

Contrast limited adaptive histogram equalization (CLAHE)
Regular histogram equalization uses global contrast of the image. This results in too bright and too dark regions as the histogram stretches and is not confined to specific region.

Adaptive histogram equalization divides the image into small tiles and within 
each tile the histogram is equalized. Tile size is typically 8x8. If the image contains noise, it gets amplified during this process. Therefore, 
contrast limiting is applied to limit the contrast below a specific limit. Bilinear interpolation is performed between tile borders. 

This tutorial demonstrates the use of histogram equalization and CLAHE in Python to enhance low contrast images.

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",113 - Histogram equalization and CLAHE,2020-04-01T07:00:27+00:00
551000,"BRISQUE calculates the no-reference image quality score for an image using the Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE). 

BRISQUE score is computed using a support vector regression (SVR) model trained on an image database with corresponding differential mean opinion score (DMOS) values. The database contains images with known distortion such as compression artifacts, blurring, and noise, and it contains pristine versions of the distorted images. The image to be scored must have at least one of the distortions for which the model was trained.

This tutorial explains the use of BRISQUE in Python. 

Mittal, A., A. K. Moorthy, and A. C. Bovik. ""No-Reference Image Quality Assessment in the Spatial Domain.
"" IEEE Transactions on Image Processing. Vol. 21, Number 12, December 2012, pp. 46954708.

https://live.ece.utexas.edu/publications/2012/TIP%20BRISQUE.pdf

To install imquality:
https://pypi.org/project/image-quality/

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",114 - Automatic image quality assessment using BRISQUE,2020-04-03T07:00:09+00:00
462000,"Otsu is a well known approach for image segmentation but it is limited to binary segmentation only. Most images contain information about multiple features requiring segmentation of multiple regions. Multi-Otsu calculates several thresholds based on the number supplied by the user. It is almost magical!

This tutorial explains the use of multi-otsu in Python followed by image segmentation. 

Liao, P-S., Chen, T-S. and Chung, P-C.,  A fast algorithm for multilevel thresholding, 
Journal of Information Science and Engineering 17 (5): 713-727, 2001

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists",115 - Auto segmentation using multi-otsu,2020-04-06T07:00:16+00:00
1282000,"Social distancing is a social responsibility that helps minimize the rapid spread of a disease during pandemics. This video explains the importance of social distancing and how it can reduce the load on the healthcare system in a given amount of time. A few lines of python code were also written as part of the video to explain the SEIR epidemiological model. 

I urge every citizen of the world to maintain social distancing for the next few weeks (or months) and help save valuable lives. 
(Video recorded and uploaded on: 19th May 2020)

Code available on Github: https://github.com/bnsreenu/python_for_microscopists

Please subscribe to my channel: https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w

References:
1. Christian Hubbs' blog: https://towardsdatascience.com/social-distancing-to-slow-the-coronavirus-768292f04296

2. Feasibility of controlling 2019-nCoV outbreaks by isolation of cases and contacts. Hellewell et al.

3. Epidemic analysis of COVID-19 in China by dynamical modeling. Peng et al.",Effect of Social Distancing on the spread of COVID-19 pandemic - A quick Python simulation,2020-03-20T00:54:20+00:00
1147000,"Data analysis is a key step that often follows image processing. This video provides a quick review of Pandas using an actively developing COVID-19 data set. In the video you'll learn about downloading the latest data from an online server, pre-process the data for plotting, and study the plots. 

This is part 1 of the 2 part videos. The second part covers more plotting using Seaborn. 

Code available at: https://github.com/bnsreenu/python_for_microscopists

References:
https://github.com/midas-network/COVID-19/tree/master/parameter_estimates/2019_novel_coronavirus",107 - Analysis of COVID-19 data using Python - Part 1,2020-03-23T07:00:01+00:00
1610000,"Data analysis is a key step that often follows image processing. This video provides a quick review of Pandas using an actively developing COVID-19 data set. In the video you'll learn about a few ways to plot the latest COVID-19 data downloaded from an online server.  

This is part 2 of the 2 part videos. 

Code available at: https://github.com/bnsreenu/python_for_microscopists

References:
https://github.com/midas-network/COVID-19/tree/master/parameter_estimates/2019_novel_coronavirus",108 - Analysis of COVID-19 data using Python - Part 2,2020-03-25T07:00:13+00:00
914000,"This video explains the process of fitting data to a curve (e.g. exponential) to model data and make predictions. Actively developing COVID-19 data set has been used for demonstration. 

Code available at: https://github.com/bnsreenu/python_for_microscopists

References:
https://github.com/midas-network/COVID-19/tree/master/parameter_estimates/2019_novel_coronavirus",109 - Predicting COVID-19 cases using Python,2020-03-26T05:00:01+00:00
1259000,"This video describes the process of reading an actively developing COVID-19 data set into Python and plotting the data many ways for easy analysis.  

Code available at: https://github.com/bnsreenu/python_for_microscopists

References:
Data set: https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series

Thanks to the blog article by Mr. Santiago.
https://blog.rmotr.com/learn-data-science-by-analyzing-covid-19-27a063d7f442",110 - Visualizing COVID-19 cases & death information using Python and plotly,2020-03-27T23:20:09+00:00
505000,"This video covers the topic of digging into the evolving COVID-19 data-set to find answers to questions like, what are the top 10 countries with highest cases and deaths. We will be using Python and plotly to visualize the results. 

Code available at: https://github.com/bnsreenu/python_for_microscopists

References:
Data set: https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series

Thanks to the blog article by Mr. Santiago.
https://blog.rmotr.com/learn-data-science-by-analyzing-covid-19-27a063d7f442",111 - What are the top 10 countries with highest COVID-19 cases and deaths?,2020-03-28T09:00:18+00:00
661000,"This video explains the process of measuring and reporting properties of labeled / segmented objects in images. The process is plain and simple in Python.

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",116 - Measuring properties of labeled / segmented regions,2020-04-14T07:00:14+00:00
568000,"Microscope (or other) images acquired under non-uniform illumination conditions make it challenging, sometimes impossible, to extract information via thresholding and segmentation. These images can be corrected using rolling ball background subtraction. This video explains the process of applying rolling ball background subtraction and CLAHE in Python.

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",117 - Shading correction using rolling ball background subtraction,2020-04-16T07:00:15+00:00
728000,"This video explains the process of detecting objects in an image based on a template reference  image. This process is not scale invariant, the template image must be of the same scale as the objects of interest. 

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",118 - Object detection by template matching,2020-04-21T07:00:06+00:00
363000,"This video explains the process of registering images to subpixel accuracy using python.

The register_translation function uses cross-correlation in Fourier space, 
and also by employing an upsampled matrix-multiplication DFT to achieve subpixel precision. 

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists

Reference:
Manuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, 
Efficient subpixel image registration algorithms, Optics Letters 33, 156-158 (2008). 
https://pdfs.semanticscholar.org/b597/8b756bdcad061e3269eafaa69452a0c43e1b.pdf",119 - Sub-pixel image registration in Python,2020-04-23T07:00:11+00:00
742000,"There are many useful libraries in python with functions to register images. This video explains 4 different ways to register images using the functions available in image_registration and scikit-image packages. It specifically covers image registration using chi squared, cross correlation, optical flow and register_translation methods.

References:
https://image-registration.readthedocs.io/en/latest/image_registration.html
https://scikit-image.org/docs/dev/api/skimage.registration.html

Code for this tutorial is available at: https://github.com/bnsreenu/python_for_microscopists",120 - Image registration methods in python,2020-04-28T07:00:15+00:00
568000,"This video provides a review of COVID19 in India as of April 22, 2020. I've used Python based data analysis and plotting for the review. The data for the video can be downloaded from Kaggle; you need to sign up for an account to download. 
https://www.kaggle.com/sudalairajkumar/covid19-in-india

Code used in this video is available at:
https://github.com/bnsreenu/python_for_microscopists",A review of COVID19 situation in India using Python data analysis and plotting,2020-04-22T23:30:06+00:00
980000,"Pystackreg may be one-stop solution for your image registration needs. This is because it allows you to register individual images or slices in tif stacks using many registration types. Images can be registered using one of the following five methods:
1. Translation
2. Rigid body (translation + rotation)
3. Scaled rotation (translation + rotation + scaling)
4. Affine (translation + rotation + scaling + shearing)
5. Bilinear (non-linear transformation; does not preserve straight lines)

This video tutorial uses adapted code from the pystackreg documentation to demonstrate registration of microscope images. 

References: https://pypi.org/project/pystackreg/

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",121 - Image registration using pystackreg library in Python,2020-04-30T07:00:02+00:00
805000,"Hematoxylin and eosin (H&E) stain is one of the primary tissue stains for histology. This is because H stain makes nuclei easily visible in blue against a pink background of cytoplasm (and other issue regions). This enables a pathologist to easily identify and evaluate the tissue; a highly manual process.

For automated image analysis these H&E stained images need to be normalized. This is because of the significant variation in image colors arising from both sample preparation and imaging conditions.  

This video explains the python code written to normalize H&E images and also to digitally separate images corresponding H and E stains, respectively. 

The code used in the video is a python adaptation of original MATLAB code. The original code can be found here:
https://github.com/mitkovetta/staining-normalization/blob/master/normalizeStaining.m

Also, original publications and links to additional references are given below.
Workflow based on the following papers:

A method for normalizing histology slides for quantitative analysis. M. Macenko et al., ISBI 2009
http://wwwx.cs.unc.edu/~mn/sites/default/files/macenko2009.pdf

Efficient nucleus detector in histopathology images. J.P. Vink et al., J Microscopy, 2013

Other useful references:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5226799/
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169875

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",122 - Normalizing H&E images and digitally separating Hematoxylin and Eosin components,2020-05-05T07:00:24+00:00
775000,"This video explains the use of a few image quality metrics in Python. 

References:
https://scikit-image.org/docs/dev/api/skimage.measure.html
https://pypi.org/project/sewar/
https://sewar.readthedocs.io/en/latest/_modules/sewar/full_ref.html#ergas

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",123 - Reference based image quality metrics,2020-05-14T07:00:22+00:00
429000,"This video explains the implementation of 'Sharpness Estimation for Document and Scene Images' by Jayant Kumar et.al

In summary, the use of difference of differences in grayscale values 
of a median-filtered image as an indicator of edge sharpness. 

References:
Sharpness Estimation for Document and Scene Images
by Jayant Kumar , Francine Chen , David Doermann
http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=33CD0038A0D2D24AE2C4F1A30B6EF1A4?doi=10.1.1.359.7002&rep=rep1&type=pdf

https://github.com/umang-singhal/pydom
pip install git+https://github.com/umang-singhal/pydom.git

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",124 - Image quality by estimating sharpness,2020-05-19T07:00:06+00:00
721000,"Generative adversarial networks (GANs) are deep learning architectures that use two neural networks (Generator and Discriminator), competing one against the other. The generator tries to create realistic looking fake data (e.g. images) and the discriminator tries to classify whether the data is real or fake. After a few thousand (or million) epochs, the generator trained model can be used to create new fake data that can pass for real data.

This tutorial provides a quick overview of GANs. The next tutorial in the playlist covers the implemetation of GAN using Keras in Python.

References from the video: 
https://www.thispersondoesnotexist.com/
http://www.wisdom.weizmann.ac.il/~vision/courses/2018_2/Advanced_Topics_in_Computer_Vision/files/DomainTransfer.pdf

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",125 - What are Generative Adversarial Networks (GAN)?,2020-05-21T07:00:30+00:00
2014000,"Generative adversarial networks (GANs) are deep learning architectures that use two neural networks (Generator and Discriminator), competing one against the other. The generator tries to create realistic looking fake data (e.g. images) and the discriminator tries to classify whether the data is real or fake. After a few thousand (or million) epochs, the generator trained model can be used to create new fake data that can pass for real data.

This tutorial the implementation of GAN using Keras in Python. It uses fully connected dense layers for both the generator and discriminator. It also explains the use of trained model in generating realistic looking fake handwritten digits. 

References from the video: 
https://www.thispersondoesnotexist.com/
http://www.wisdom.weizmann.ac.il/~vision/courses/2018_2/Advanced_Topics_in_Computer_Vision/files/DomainTransfer.pdf

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",126 - Generative Adversarial Networks (GAN) using keras in python,2020-05-25T07:00:14+00:00
1210000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,127 - Data augmentation using keras,2020-05-26T07:00:08+00:00
772000,"Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists

Dataset link: https://www.kaggle.com/datasets/iarunava/cell-images-for-detecting-malaria",128 - Malarial cell classification using CNN and data augmentation,2020-05-28T07:00:18+00:00
606000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,"129 - What are Callbacks, Checkpoints and Early Stopping in deep learning (Keras and TensorFlow)",2020-06-01T07:00:25+00:00
590000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,130 - Evaluating the deep learning trained model (Keras and TensorFlow),2020-06-04T07:00:04+00:00
354000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,131 - How to load a partially trained deep learning model and continue training?,2020-06-08T07:00:01+00:00
425000,,132 - What are Activation functions in deep learning (Keras & Tensorflow)?,2020-06-11T07:00:02+00:00
410000,,133 - What are Loss functions in machine learning?,2020-06-15T07:00:00+00:00
516000,,134 - What are Optimizers in deep learning? (Keras & TensorFlow),2020-06-18T07:00:06+00:00
614000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,135 - A quick introduction to Metrics in deep learning. (Keras & TensorFlow),2020-06-22T07:00:19+00:00
698000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,136 understanding deep learning parameters batch size,2020-06-25T07:00:11+00:00
738000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,137 - What is one hot encoding in machine learning?,2020-06-29T07:00:16+00:00
892000,"Scaling / Normalizing
Batchnormalization
Dropout
Using Keras

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists","138 - The need for scaling, dropout, and batch normalization in deep learning",2020-07-02T07:00:09+00:00
1597000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,"139 - The topology of deep neural networks, designing your model.",2020-07-07T07:00:06+00:00
834000,,"140 - What in the world is regression, multi-label, multi-class and binary classification?",2020-07-09T07:00:07+00:00
1307000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,141 - Regression using Neural Networks and comparison to other models,2020-07-14T07:00:00+00:00
1163000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,142 - Multilabel classification using Keras,2020-07-16T07:00:08+00:00
695000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,143 - Multiclass classification using Keras,2020-07-21T07:00:08+00:00
878000,"Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists

Data set link : https://www.kaggle.com/datasets/iarunava/cell-images-for-detecting-malaria",144 - Binary classification using Keras,2020-07-23T07:00:08+00:00
1500000,Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists,"145 - Confusion matrix,  ROC and AUC in machine learning",2020-07-28T07:00:02+00:00
1506000,"This video explains the process of extracting features from multiple training images, fitting a classifier (e.g Random Forest or SVM), and segmenting images using a saved model. The video builds on topics discussed in videos 57-67 except applies it on multiple training images. 

The code from this video is available at: https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing",67b - Feature based image segmentation using traditional machine learning. (Multi-training images),2020-06-24T20:15:22+00:00
444000,"The western world can afford to purchase notebooks, Macbooks and workstations for python and deep learning. But our friends from other regions do not have the same luxury. This video provides a very brief overview of Raspberry Pi and sets stage for the next tutorial, Getting started with Google Colaboratory.",146 - Raspberry Pi - Learning python and deep learning on a tight budget,2020-08-04T07:00:15+00:00
906000,"Most of us cannot afford to purchase our own Nvidia graphics card for deep learning. Luckily, Google makes it possible for us to get started with deep learning using Tensorflow and GPU/TPU. 

This video provides a quick overview of Google Colab and walks you through the process of mounting Google Drive and getting started with deep learning using GPU acceleration.",147 - Getting started with Google Colaboratory for deep learning,2020-08-06T07:00:09+00:00
2204000,"Imbalanced data is part of life! With a proper knowledge of the data set and a few techniques from this video imbalanced data can be easily managed. 

Prerequisites: Pick the right metrics as overall accuracy does not provide information about the accuracy of individual classes. Look at confusion matrix and ROC_AUC.

Technique 0: Collect more data, if possible. 
Technique 1: Pick decision tree based approaches as they work better than logistic regression or SVM. Random Forest is a good algorithm to try but beware of over fitting. 
Technique 2: Up-sample minority class
Technique 3: Down-sample majority class 
Technique 4: A combination of Over and under sampling. 
Technique 5: Penalize learning algorithms that increase cost of classification 
mistakes on minority classes.
Technique 6: Generate synthetic data (SMOTE, ADASYN)
Technique 7: Add appropriate weights to your deep learning model. 

References:
https://imbalanced-learn.org/stable/over_sampling.html?highlight=smote
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",148 - 7 techniques to work with imbalanced data for machine learning in python,2020-08-11T07:00:07+00:00
1620000,"Imbalanced data is part of life! With a proper knowledge of the data set and appropriate techniques imbalanced data can be easily managed. 

This video uses the Indian Liver Disease data (link below) to demonstrate the use of data up-scaling (and SMOTE) for minority class to improve accuracy. 

References:
Dataset:
https://www.kaggle.com/uciml/indian-liver-patient-records

ROCAUC using Yellowbrick:
https://www.scikit-yb.org/en/latest/api/classifier/rocauc.html

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",149 - Working with imbalanced data for ML - Demonstrated using liver disease data,2020-08-13T07:00:03+00:00
659000,"Image transformation operations, including the ones provided by ImageDataGenerator class from keras, modify numpy arrays based on the user description. The modification of these arrays may yield unintended results as the data may be modified from interpolation. If the user is not careful in spotting these issues the results may turn out to be meaningless. This is especially true when working with categorical labels where each label value needs to be conserved throughout the training and prediction process. 

This video walks you through this issue and also offers suggestions on how to handle data augmentation for categorical labels. Custom code for augmentation can be downloaded here: https://github.com/bnsreenu/python_for_microscopists",150 - Warning about keras' data augmentation when working with categorical labels,2020-08-18T07:00:05+00:00
387000,"Images in JPG format are excellent for social media sharing and quick email exchanges. They may also be used for scientific image analysis if the features of interest are large enough and are relatively unaffected by JPG compression. JPG cannot be used for applications requiring categorical labels, applications such as semantic segmentation. This video demonstrates the reasons behind the warning. 

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists",151 Warning about JPG files when working with categorical labels,2020-08-20T07:00:07+00:00
664000,"This tutorial explains the few lines of code to visualize outputs of convolutional layers in any deep learning model. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",152 - How to visualize convolutional filter outputs in your deep learning model?,2020-08-24T23:10:31+00:00
1915000,"Want to learn about artificial neural network (ANN) from the perspective of linear regression? This tutorial explains ANN using heart disease data set and walks you through the process of calculating results using trained weights and biases. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",153 - Artificial Neural Networks - Explanation for those who understand linear regression,2020-08-26T23:44:44+00:00
1667000,"Loss curves contain a lot of information about training of an artificial neural network. This video goes through the interpretation of various loss curves generated using the Wisconsin breast cancer data set. 
https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",154 - Understanding the training and validation loss curves,2020-09-01T05:00:07+00:00
358000,"A very short video to explain the process of assigning GPU memory for TensorFlow calculations. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",156 - How to limit GPU memory usage for TensorFlow?,2020-09-08T06:00:03+00:00
1133000,"TensorBoard can be very useful to view training model and loss/accuracy curves in a dashboard. This video explains the process of setting up TensorBoard callback during training and also to open the recorded logs in a browser. 

Dataset downloaded from: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",157 - What is TensorBoard and how to launch it in a browser?,2020-09-10T06:00:02+00:00
1634000,"Deep learning is far superior to traditional machine learning with loads of training data. But, for limited training data traditional machine learning (e.g. Random Forest or SVM) may outperform deep learning. For image processing applications features need to be extracted / engineered for improved accuracy. Alternatively, features can be extracted from convolutional filters that are part of convolutional neural networks. 

This video goes through the process of extracting features using convolutional filters and using them as inputs to a traditional Random Forest classifier to develop an image classification solution. This approach is specifically designed for image classification of custom datasets.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",158 - Convolutional filters + Random Forest for image classification.,2020-09-15T06:00:02+00:00
1144000,"Deep learning is far superior to traditional machine learning with loads of training data. But, for limited training data traditional machine learning (e.g. Random Forest or SVM) may outperform deep learning. For image processing applications features need to be extracted / engineered for improved accuracy. Alternatively, features can be extracted from convolutional filters that are part of convolutional neural networks. 

This video goes through the process of extracting features using convolutional filters and using them as inputs to a traditional Random Forest classifier to develop an image segmentation solution. You will find this approach to be easier to implement compared to U-net and you'll also find the results to be spectacular!!!


Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing",159 - Convolutional filters + Random Forest for image segmentation.,2020-09-17T09:00:03+00:00
1446000,"No one can give a definite answer to the question about number of neurons and hidden layers. This is because the answer depends on the data itself. This video goes through the thought process of determining the number of hidden layers and neurons using simple code as example. 

#Dataset link:
https://cdn.scribbr.com/wp-content/uploads//2020/02/heart.data_.zip?_ga=2.217642335.893016210.1598387608-409916526.1598387608

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",155 - How many hidden layers and neurons do you need in your artificial neural network?,2020-09-03T06:00:04+00:00
1488000,"Data set link: https://www.epfl.ch/labs/cvlab/data/data-em/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",160 - When to retrain your ML model and what is the best way to re-train?,2020-09-22T08:00:06+00:00
1099000,"Dataset from: https://www.kaggle.com/rakannimer/air-passengers

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",162 - An introduction to time series forecasting - Part 2 Exploring data using python,2020-09-29T08:00:16+00:00
1280000,"Dataset from: https://www.kaggle.com/rakannimer/air-passengers

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",163 - An introduction to time series forecasting - Part 3 Using ARIMA in python,2020-10-01T08:00:03+00:00
1256000,"Dataset from: https://www.kaggle.com/rakannimer/air-passengers

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",164 - An introduction to time series forecasting - Part 4 Using feed forward neural networks,2020-10-06T08:00:06+00:00
1172000,"RNN - Recurrent neural networks

Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feed forward neural networks, LSTM has feedback connections.",165 - An introduction to RNN and LSTM,2020-10-08T08:00:03+00:00
1422000,"Dataset from: https://www.kaggle.com/rakannimer/air-passengers

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",166 - An introduction to time series forecasting - Part 5 Using LSTM,2020-10-13T08:00:00+00:00
778000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",161 - An introduction to time series forecasting - Part 1,2020-09-24T08:00:04+00:00
1534000,"This video explains the process of using pretrained weights (VGG16) as feature extractors for traditional machine learning classifiers (Random Forest). 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Keras pretrained models: https://keras.io/api/applications/",158b - Transfer learning using CNN (VGG16) as feature extractor and Random Forest classifier,2020-09-22T21:52:36+00:00
397000,A brief summary about myself...,Who is Sreeni and what does he do with the ad-revenue from his YouTube channel?,2020-09-29T17:22:09+00:00
1749000,"LSTMs are great for timeseries forecasting and sequence predictions. This makes them appropriate for natural language prediction. This tutorial explains the process of training an LSTM network on English text and predicting letters based on the training. 

Reference: https://www.gutenberg.org/
Text file link: http://www.gutenberg.org/ebooks/236

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",167 - Text prediction using LSTM (English text),2020-10-15T07:00:13+00:00
1135000,"LSTMs are great for timeseries forecasting and sequence predictions. This makes them appropriate for natural language prediction. This tutorial explains the process of training an LSTM network on Hindi and Telugu text and predicting letters based on the training. 

Reference: Reference: https://www.gutenberg.org/
Telugu text file: http://www.gutenberg.org/ebooks/39561

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",168 - Text prediction using LSTM - Non-English example using Hindi and Telugu letters.,2020-10-20T07:00:08+00:00
1310000,"For image annotation and to run this code as a workflow online:
www.apeer.com

NOTE: APEER is free to use for individuals, students, researchers and non-profits. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

This video explains the process of extracting features using pretrained VGG16 imagenet weights and using the feature to train a Random Forest model for semantic segmentation. You find find the results to be better than U-net for limited training data.",159b - Pretrained CNN (VGG16 - imagenet) features for semantic segmentation using Random Forest,2020-10-02T22:43:09+00:00
811000,"AutoKeras is an AutoML system based on Keras. The goal of AutoKeras is to make machine learning accessible for everyone. It suggests the best machine learning model by fitting data to various models and hyperparameters. 

This video explains the process of getting your system ready for AutoKeras installation and verifying the installation by running AutoKeras on mnist data set. 

Installation instructions: https://autokeras.com/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",169 - Deep Learning made easy with AutoKeras,2020-10-22T07:00:01+00:00
948000,"AutoKeras is an AutoML system based on Keras. The goal of AutoKeras is to make machine learning accessible for everyone. It suggests the best machine learning model by fitting data to various models and hyperparameters. 

This video explains the use of AutoKeras for structured data classification. 

Installation instructions: https://autokeras.com/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",170 - AutoKeras for structured data classification using the Wisconsin breast cancer data set,2020-10-27T07:00:03+00:00
730000,"AutoKeras is an AutoML system based on Keras. The goal of AutoKeras is to make machine learning accessible for everyone. It suggests the best machine learning model by fitting data to various models and hyperparameters. 

This video explains the use of AutoKeras for image classification. 

Installation instructions: https://autokeras.com/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",171 - AutoKeras for image classification using cifar10 data set,2020-10-29T07:00:05+00:00
97000,,Digital Sreeni Channel name change from Python for Microscopists,2020-10-07T00:19:18+00:00
426000,"Top k accuracy is how many times the correct label is within 
the top k classes predicted by the network.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",172 - Top k accuracy for multiclass machine learning classification,2020-11-05T08:00:03+00:00
893000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing",173 - Intersection over Union (IoU) for semantic segmentation,2020-11-10T08:00:00+00:00
488000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",174  - What is PCA and how to use it to speed up machine learning training,2020-11-12T08:00:10+00:00
942000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",175 - Speeding up ML training using PCA - Breast cancer data set example,2020-11-17T08:00:07+00:00
754000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",176 -- Speeding up ML training using PCA - Multiclass image classification example,2020-11-19T08:00:08+00:00
551000,Make a plan and learn the basics.,This is the proper way to get started with python,2020-10-23T07:00:04+00:00
1445000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Segmentation Models library info:
pip install segmentation-models

https://github.com/qubvel/segmentation_models
Recommended for colab execution
TensorFlow ==2.1.0
keras ==2.3.1


For this demo it is working on a local workstation...
Python 3.5
TensorFlow ==1.
keras ==2

Dataset link: https://www.epfl.ch/labs/cvlab/data/data-em/",177 - Semantic segmentation made easy (using segmentation models library),2020-11-24T08:00:04+00:00
1030000,"Data download links: 
https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv

https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",166b - Forecasting COVID-19 cases using LSTM networks,2020-11-03T08:00:01+00:00
1059000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",178 - An introduction to variational autoencoders (VAE),2020-11-26T08:00:05+00:00
2063000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",179 - Variational autoencoders using keras on MNIST data,2020-12-01T08:00:01+00:00
1613000,"LSTM encoder - decoder network for anomaly detection.
Just look at the reconstruction error (MAE) of the autoencoder, define a threshold value for the error and tag any data above the threshold as anomaly.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",180 - LSTM Autoencoder for anomaly detection,2020-12-03T08:00:01+00:00
1360000,"For a dataset just search online for 'yahoo finance GE' or any other stock of your interest. Then select history and download csv for the dates you are interested. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",181 - Multivariate time series forecasting using LSTM,2020-12-08T08:00:18+00:00
1317000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",182 - How to batch process multiple images in python?,2020-12-10T08:00:04+00:00
443000,"Optical character recognition (OCR)

References:
https://keras-ocr.readthedocs.io/en/latest/
https://github.com/clovaai/CRAFT-pytorch

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",183 - OCR in python using keras-ocr,2020-12-15T08:00:13+00:00
971000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",184 - Scheduling learning rate in keras,2020-12-17T08:00:08+00:00
283000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",185 - Hyperparameter tuning using GridSearchCV,2020-12-22T08:00:06+00:00
329000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",186 - A note about parallelization during hyperparameter search using GridSearchCV,2020-12-24T08:00:11+00:00
934000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",187 - Hyperparameter tuning for learning rate and momentum,2020-12-29T08:00:28+00:00
782000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists","188 - Hyperparameter tuning for activation function, optimizer and weight initialization",2020-12-31T08:00:31+00:00
624000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists","189 - Hyperparameter tuning for dropout, # neurons, batch size, # epochs, and weight constraint",2021-01-05T08:00:02+00:00
758000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",190 - Finding the best model between Random Forest & SVM via hyperparameter tuning,2021-01-07T08:00:06+00:00
743000,"Comparing images using using SIFT/ORB key point descriptors and SSIM. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",191 - Measuring image similarity in python,2021-01-12T08:00:00+00:00
1224000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",192 - Working with 3D and multi-dimensional images in python,2021-01-14T08:00:10+00:00
1262000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset used in the video: https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)

XGBoost documentation:
https://xgboost.readthedocs.io/en/latest/

Video by original author: https://youtu.be/ufHo8vbk6g4",193 - What is XGBoost and is it really better than Random Forest and Deep Learning?,2021-01-19T08:00:06+00:00
1962000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

XGBoost documentation:
https://xgboost.readthedocs.io/en/latest/

Video by original author: https://youtu.be/ufHo8vbk6g4",194 - Semantic segmentation using XGBoost and VGG16 imagenet as feature extractor,2021-01-21T08:00:04+00:00
1033000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

XGBoost documentation:
https://xgboost.readthedocs.io/en/latest/

Video by original author: https://youtu.be/ufHo8vbk6g4",195 - Image classification using XGBoost and VGG16 imagenet as feature extractor,2021-01-26T08:00:00+00:00
1145000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

XGBoost documentation:
https://xgboost.readthedocs.io/en/latest/

https://lightgbm.readthedocs.io/en/latest/
pip install lightgbm

Dataset:
https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)",196 - What is Light GBM and how does it compare against XGBoost?,2021-01-28T08:00:27+00:00
1565000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

Note: Annotate images at www.apeer.com to create labels. 

XGBoost documentation:
https://xgboost.readthedocs.io/en/latest/

https://lightgbm.readthedocs.io/en/latest/
pip install lightgbm",197 - Light GBM vs XGBoost for semantic image segmentation,2021-02-02T08:00:29+00:00
1010000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

https://pypi.org/project/Boruta/
pip install Boruta

XGBoost documentation:
https://xgboost.readthedocs.io/en/latest/

Dataset:
https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)",198 - Feature selection using Boruta in python,2021-02-04T08:00:06+00:00
705000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

https://en.wikipedia.org/wiki/Hough_transform

The origin is the top left corner of the original image. X and Y axis are 
horizontal and vertical edges respectively. The distance is the minimal 
algebraic distance from the origin to the detected line. The angle accuracy 
can be improved by decreasing the step size in the theta array.

https://scikit-image.org/docs/dev/auto_examples/edges/plot_line_hough_transform.html#sphx-glr-auto-examples-edges-plot-line-hough-transform-py

Image downloaded from: https://geometryhelp.net/wp-content/uploads/2019/04/intersecting-lines.jpg
Then inverted to dark background.",199 - Detecting straight lines using Hough transform in python,2021-02-09T08:00:01+00:00
1460000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

To install rasterio:
Download and install GDAL first
https://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal
Since I have python 3.7 I downloaded: GDAL-3.1.4-cp37-cp37m-win_amd64.whl
cp37 stands for python3.7

Download and install rasterio
https://www.lfd.uci.edu/~gohlke/pythonlibs/#rasterio
Since I have python 3.7 I downloaded: rasterio-1.1.8-cp37-cp37m-win_amd64.whl
cp37 stands for python3.7

Images from: https://landsatonaws.com/L8/042/034/LC08_L1TP_042034_20180619_20180703_01_T1",201 - Working with geotiff files using rasterio in python (also quick demo of NDVI calculation),2021-02-16T08:00:10+00:00
1603000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset link:
https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000
Data description: 
https://arxiv.org/ftp/arxiv/papers/1803/1803.10417.pdf

The 7 classes of skin cancer lesions included in this dataset are:
Melanocytic nevi (nv)
Melanoma (mel)
Benign keratosis-like lesions (bkl)
Basal cell carcinoma (bcc) 
Actinic keratoses (akiec)
Vascular lesions (vas)
Dermatofibroma (df)",202 - Two ways to read HAM10000 dataset into python for skin cancer lesion classification,2021-02-18T02:28:38+00:00
1722000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset link:
https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000
Data description: 
https://arxiv.org/ftp/arxiv/papers/1803/1803.10417.pdf

The 7 classes of skin cancer lesions included in this dataset are:
Melanocytic nevi (nv)
Melanoma (mel)
Benign keratosis-like lesions (bkl)
Basal cell carcinoma (bcc) 
Actinic keratoses (akiec)
Vascular lesions (vas)
Dermatofibroma (df)",203 - Skin cancer lesion classification using the HAM10000 dataset,2021-02-23T08:00:16+00:00
1401000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Reference: 
https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_glcm.html",200 - Image classification using gray-level co-occurrence matrix (GLCM) features and LGBM classifier,2021-02-11T08:00:00+00:00
1620000,"Do not apply a model trained on smaller images to directly segment large images -will not work!!! (Code shared to prove this point).

Divide images into patches of smaller sizes and then perform the prediction. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset info: Electron microscopy (EM) dataset from
https://www.epfl.ch/labs/cvlab/data/data-em/

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com 

To install patchify: pip install patchify",206 - The right way to segment large images by applying a trained U-Net model on smaller patches,2021-03-04T08:00:05+00:00
1143000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset info: Electron microscopy (EM) dataset from
https://www.epfl.ch/labs/cvlab/data/data-em/

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",207 - Using IoU (Jaccard) as loss function to train U-Net for semantic segmentation,2021-03-09T08:00:02+00:00
1880000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",208 - Multiclass semantic segmentation using U-Net,2021-03-11T08:00:08+00:00
988000,"Multiclass semantic segmentation using U-Net - prediction on large images
and 3D volumes (slice by slice)

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing


To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",209 - Multiclass semantic segmentation using U-Net: Large images and 3D volumes (slice by slice),2021-03-17T07:00:07+00:00
2271000,"Multiclass semantic segmentation using U-Net with VGG, ResNet, and Inception as backbones.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

Segmentation models library documentation: 
https://github.com/qubvel/segmentation_models

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com","210 - Multiclass U-Net using VGG, ResNet, and Inception as backbones",2021-03-24T07:00:02+00:00
1915000,"Multiclass semantic segmentation using Linknet and how does it compare against unet

Original paper on Unet: (2015)
    https://arxiv.org/pdf/1505.04597.pdf
    
Original paper on Linknet: (2017)
    https://arxiv.org/pdf/1707.03718.pdf
    
Can learn a bit more about backbone comparison here....
https://iopscience.iop.org/article/10.1088/1742-6596/1544/1/012196/pdf

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 
https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing



To annotate images and generate labels, you can use APEER (for free):
www.apeer.com

Segmentation models library documentation: 
https://github.com/qubvel/segmentation_models",211 - U-Net vs LinkNet for multiclass semantic segmentation,2021-03-31T07:00:10+00:00
1332000,"Classification of mnist hand sign language alphabets into 25 classes
(Z is not included as it includes a wave motion, not captured using a single image)
Dataset: https://www.kaggle.com/datamunge/sign-language-mnist

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset: https://www.kaggle.com/datamunge/sign-language-mnist",212 - Classification of mnist sign language alphabets using deep learning,2021-04-07T07:00:00+00:00
1531000,"Classification of mnist hand sign language alphabets into 25 classes.
An ensemble of network results may provide improved accuracy compared to any single network. This video goes through the code that explains the ensemble process of the network predictions. 

Dataset: https://www.kaggle.com/datamunge/sign-language-mnist

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset: https://www.kaggle.com/datamunge/sign-language-mnist",213 - Ensemble of networks for improved accuracy in deep learning,2021-04-14T09:00:00+00:00
1773000,"Improving semantic segmentation (U-Net) performance via ensemble of multiple trained networks.
ResNet34 + Inception V3 + VGG16

If you get an error while loading Segmentation models library, please follow these steps to fix the issue: https://youtu.be/syJZxDtLujs

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 
https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing


To annotate images and generate labels, you can use APEER (for free):
www.apeer.com 

Segmentation models library documentation: 
https://github.com/qubvel/segmentation_models",214 - Improving semantic segmentation (U-Net) performance via ensemble of multiple trained networks,2021-04-21T07:00:04+00:00
1474000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset info: Electron microscopy (EM) dataset from
https://www.epfl.ch/labs/cvlab/data/data-em/

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",204 - U-Net for semantic segmentation of mitochondria,2021-02-25T08:00:08+00:00
1666000,"This video explains U-Net  segmentation of images followed by watershed
based separation of objects. Object properties will also be calculated.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset info: Electron microscopy (EM) dataset from
https://www.epfl.ch/labs/cvlab/data/data-em/

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",205 - U-Net plus watershed for instance segmentation,2021-03-02T08:00:09+00:00
3000000,"Can be applied to 3D volumes from FIB-SEM, CT, MRI, etc. (e.g., BRATS dataset). 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

If you get an error while loading Segmentation models library, please follow these steps to fix the issue: https://youtu.be/syJZxDtLujs

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 
https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com 

Segmentation models 3D library documentation: 
https://pypi.org/project/segmentation-models-3D/",215 - 3D U-Net for semantic segmentation,2021-04-28T07:00:03+00:00
86000,Just got bored of spending time at home so decided to get in the car and drive. Im glad I found beautiful places that give some peace to mind!,Hidden gems around the Bay Area - Santa Cruz - Feb2021,2021-02-21T23:13:36+00:00
407000,"Link to sign up for Mito:
https://hubs.ly/H0H0Gsz0",Python Tips and Tricks - 1: Mito (trymito.io) for structured data,2021-02-22T20:04:04+00:00
408000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/02_Tips_Tricks_python_tips_and_tricks_2_google_image_download.py

This script downloads images from Google search (or Bing search). 

As with any download, please make sure you are not violating any copyright terms. I use this script to download images that help me practice deep learning based image classification. 

DO NOT use downloaded images to train a commercial product, this most certainly violates copyright terms. 

Do not pip install google_images_download

this gives an error that some images could not be downloadable. 
Google changed some things on their side...

The updated repo can be installed using the following command. 
pip install git+https://github.com/Joeclinton1/google-images-download.git

Please remember that this method has a limit of 100 images. 

OR

You can use bing.
Does not seem to have a limit on the number of images to download. 
pip install bing-image-downloader",Python tips and tricks - 2: Downloading images from online search,2021-02-23T19:02:13+00:00
133000,"In summary: DigitalSreeni is my personal channel and Apeer_Micro is my work channel. I appreciate if you subscribe to both.

Link to Apeer_Micro channel.
https://www.youtube.com/channel/UCVrG0AsRMb0pPcxzX75SusA",Clarification about my YouTube channels,2021-02-26T00:33:15+00:00
683000,"Image augmentation may hurt your model accuracy if you're not careful. Always test you augmentation operations first on a smaller dataset and then incrementally verify its accuracy before using it in your final model training. 

Link to the file from this video: https://github.com/bnsreenu/python_for_microscopists/blob/master/tips_tricks_3_data_augmentation.ipynb

Link to my GitHub account:
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 3: Be conservative with image augmentation,2021-03-05T23:07:51+00:00
543000,"imageJ (Fiji): https://imagej.net/Fiji

ZEN Lite: https://www.zeiss.com/microscopy/us/products/microscope-software/zen-lite.html",Python tips and tricks - 4: Best free software for image visualization and processing,2021-03-15T10:00:12+00:00
448000,"Link to my GitHub account:
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 5: Extracting patches from large images and masks for semantic segmentation,2021-03-23T10:00:02+00:00
2234000,"What to expect when you perform semantic segmentation using small datasets (less than 100 images) and U-net architecture? Does augmentation help and how about transfer learning?

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",216 - Semantic segmentation using a small dataset for training (& U-Net),2021-05-05T09:00:00+00:00
854000,"Step 1: Install Visual Studio community edition: I installed VS2019
https://visualstudio.microsoft.com/vs/community/

Step 2: Install your IDE if you haven't already done so. I installed Spyder 4.1.5 via Anaconda individual edition.
https://www.anaconda.com/products/individual

Step 3: Setup a separate environment for your IDE. I created an env. with python 3.7
I skipped 3.8 as it is relatively new and some of my libraries may not be tested or available. 
On Anaconda you can use Anaconda Navigaor to create a new environment. 
If you only see 3.8 as option on Navigator you can create new env from anaconda prompt: 
conda create -n py37gpu python=3.7 anaconda

For the following steps verify instructions from www.tensorflow.org/install/gpu

Step 4: Verify your GPU is actually supported for deep learning: 
https://developer.nvidia.com/cuda-gpus

Step 5: Figure out your GPU model: I have P5000 Quadro
and Update the GPU driver: For me it updated to Version 461.72 
https://www.nvidia.com/download/index.aspx?lang=en-us

Step 6: Download and install CUDA Toolkit. I installed Version 11.0
https://developer.nvidia.com/cuda-toolkit-archive

Step 7: Download cuDNN. I downloaded version 8.0.4
You need to sign up for Nvidia developer program (free)
Extract all folder contents from cudnn you just downloaded to 
C:\program files\Nvidia GPU computing toolkit\CUDA\v11.0

Step 8: Install tensorflow
Open spyder in the new environment that got named as: py37gpu (or whatever name you assigned)
- pip install tensorflow

Step 9: Verify the installation
Create a new python file and run these lines to test if GPU is recognized by tensorflow. 
import tensorflow as tf
tf.test.is_gpu_available(
    cuda_only=False, min_cuda_compute_capability=None
)

The last few lines of my output shows...
physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:03:00.0, compute capability: 6.1)
physical GPU (device: 1, name: Quadro P5000, pci bus id: 0000:a1:00.0, compute capability: 6.1)

I have 2 GPUs. 

But on Windows you will not be able to efficiently use both GPUs for training. 

#On windows sysems you cannot install NCCL that is required for multi GPU
#So we need to follow Heirarchial copy method or reduce to single GPU.
 
strategy = tf.distribute.MirroredStrategy(devices=devices_names[:n_gpus],
                                           cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

#The following will raise an error on Windows about NCCL. 
#strategy = tf.distribute.MirroredStrategy(devices=devices_names[:n_gpus])",217 - 9 steps to installing TensorFlow GPU on Windows 10,2021-05-12T07:00:25+00:00
1088000,"Difference between UpSampling2D and Conv2DTranspose

These are the two common types of layers that can be used to increase the dimensions of arrays.

UpSampling2D is like the opposite of pooling where it repeats rows and columns of the input.

Conv2DTranspose performs up-sampling and convolution. 

Conv2DTranspose has been reported to result in Checkerboard artifacts but 
unfortunately not much information on the comparison of UpSampling2D vs Conv2DTranspose.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",218 - Difference between UpSampling2D and Conv2DTranspose used in U-Net and GAN,2021-05-19T07:00:20+00:00
2257000,"Understanding U-Net architecture and building it from scratch. 

This tutorial should clear any doubts you may have regarding the architecture of U-Net. It should also inform you on the process of building your own U-Net using functional blocks for encoder and decoder. 

Example use case: Segmentation of mitochondria using only 12 images and about 150 labeled objects.

Dataset: https://www.epfl.ch/labs/cvlab/data/data-em/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",219 - Understanding U-Net architecture and building it from scratch,2021-05-27T07:00:04+00:00
467000,"Note: Importing segmentation models library may give you generic_utils 
error on TF2.x

If you get an error about generic_utils...

change 
keras.utils.generic_utils.get_custom_objects().update(custom_objects) 
to 
keras.utils.get_custom_objects().update(custom_objects) 
in 
.../lib/python3.7/site-packages/efficientnet/__init__.py 

For Anaconda users:
Use this code to find out the location of site-packages directory 
under your current environment in anaconda. 

from distutils.sysconfig import get_python_lib
print(get_python_lib())


For Colab users:
You can click on the direct link provided on Colab for __init__.py and edit it.
Remember to restart the runtime after editing the file. 

Alternatively you can work with Tensorflow 1.x that doesn't throw
the generic_utils error. 
In google colab, add this as your first line.
%tensorflow_version 1.x
(Or just create a new environment in your local IDE to use TF1.x)

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 6: Fixing generic_utils error while importing segmentation models library,2021-03-30T07:00:07+00:00
2697000,"This video refers to semantic segmentation use case. 
Dataset: https://www.epfl.ch/labs/cvlab/data/data-em/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/222_working_with_large_data_that_does_not_fit_memory_semantic_segm

Code for all videos: 
https://github.com/bnsreenu/python_for_microscopists

For semantic segmentation the folder structure needs to look like below
if you want to use ImageDatagenerator.

Data/
    train_images/
                train/
                    img1, img2, img3, ......
    
    train_masks/
                train/
                    msk1, msk, msk3, ......
                    
    val_images/
                val/
                    img1, img2, img3, ......                

    val_masks/
                val/
                    msk1, msk, msk3, ......
      
    test_images/
                test/
                    img1, img2, img3, ......    
                    
    test_masks/
                test/
                    msk1, msk, msk3, ......

Use split-folders library (pip install split-folders) to make this process of splitting
files easy.",222 - Working with large data that doesn't fit your system memory - Semantic Segmentation,2021-06-16T07:00:17+00:00
1141000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

# TTA - Should be called prediction time augmentation
#We can augment each input image, predict augmented images and average all predictions.",223 - Test time augmentation for semantic segmentation,2021-06-23T07:00:11+00:00
1211000,"IoU and Binary Cross-Entropy are good loss functions for binary semantic segmentation. but Focal loss may be better. 

Focal loss is good for multiclass classification where some classes are easy and other difficult to classify. It is just an extension of the cross-entropy loss. It down-weights easy classes and focuses training on hard to classify classes. In summary, focal loss turns the models attention towards the difficult to classify examples.",220 - What is the best loss function for semantic segmentation?,2021-06-02T07:00:04+00:00
532000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

pip install split-folders

import splitfolders  # or import split_folders

input_folder = 'cell_images/'

# Split with a ratio.
# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.
#Train, val, test
splitfolders.ratio(input_folder, output=""cell_images2"", 
                   seed=42, ratio=(.7, .2, .1), 
                   group_prefix=None) # default values


# Split val/test with a fixed number of items e.g. 100 for each set.
# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.
# enable oversampling of imbalanced datasets, works only with fixed
splitfolders.fixed(input_folder, output=""cell_images2"", 
                   seed=42, fixed=(35, 20), 
                   oversample=False, group_prefix=None)","221 - Easy way to split data on your disk into train, test, and validation?",2021-06-09T07:00:06+00:00
548000,"This video briefly introduces you to the keras unet collection library that offers a few variants of the classic U-Net model. These variants include Attention U-Net, U-Net plus plus, and R2-U-Net. 

For more information about the library: https://github.com/yingkaisha/keras-unet-collection

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.epfl.ch/labs/cvlab/data/data-em/
Images and masks are divided into patches of 256x256.",227 - Various U-Net models using keras unet collection library - for semantic image segmentation,2021-07-21T07:00:09+00:00
965000,"Residual Networks:
Residual networks were proposed to overcome the problems of deep CNNs (e.g., VGG).Stacking convolutional layers and making the model deeper hurts the generalization ability of the network. To address this problem, ResNet architecture was introduced which adds the idea of skip connections.

In traditional neural networks, each layer feeds intothe next layer. In networks with residual blocks, each layer feeds into the next layer and directly into the layers about 23 hops away. Inputs can forward propagate faster through the residual connections (shortcuts) across layers.

Recurrent convolutional networks:
The recurrent network can use the feedback connection to store information over time. Recurrent networks use context information; as time steps increase, the network leverages more and more neighborhood information. Recurrent and CNNs can be combined for image-based applications. With recurrent convolution layers, the network can evolve over time though the input is static. Each unit is influenced by its neighboring units, includes the context information of an image.

U-net can be built using recurrent or residual or a combination block instead of the traditional double-convolutional block.",224 - Recurrent and Residual U-net,2021-06-30T07:00:06+00:00
896000,"What is attention and why is it needed for U-Net?

Attention in U-Net is a method to highlight only the relevant activations during training. It reduces the computational resources wasted on irrelevant activations and provides better generalization of the network. 

Two types of attention:

1. Hard attention
Highlight relevant regions by cropping.
One region of an image at a time; this implies it is non differentiable and needs reinforcement learning.  
Network can either pay attention or not, nothing in between.  
Backpropagation cannot be used. 

2. Soft attention
Weighting different parts of the image.
Relevant parts of image get large weights and less relevant parts get small weights. 
Can be trained with backpropagation. 
During training, the weights also get trained making the model pay more attention to relevant regions.  
In summary  it adds weights to pixels based on the relevance. 

Why is attention needed in U-Net?
U-net skip connection combines spatial information from the down-sampling path with the up-sampling path to retain good spatial information. But this process brings along the poor feature representation from the initial layers. Softattention implemented at the skip connections will actively suppress activations at irrelevant regions.",225 - Attention U-net. What is attention and why is it needed for U-Net?,2021-07-07T07:00:04+00:00
1626000,"Is there a clear advantage of modified U-Net modules such as Attention U-Net and Residual U-Net over the standard U-Net? Watch the video to find out.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.epfl.ch/labs/cvlab/data/data-em/
Images and masks are divided into patches of 256x256.",226 - U-Net vs Attention U-Net vs Attention Residual U-Net - should you care?,2021-07-14T07:00:13+00:00
543000,"Loading a keras model and continuing training
When using custom loss function and metrics.

No code to share with this video. 
Summary: Provide your custom optimizer or loss or metrics as custom objects during loading the model. 

my_model = load_model('your_trained_model.hdf5', 
                      custom_objects={'my_custom_loss': custom_loss,
                                      'custom_metric': my_custom_metric})",Python tips and tricks - 7:  Continuing keras model training when using custom loss and metrics,2021-05-08T08:00:08+00:00
1090000,"Dataset from: https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery

RGB to HEX: (Hexadecimel -- base 16)
This number divided by sixteen (integer division; ignoring any remainder) gives 
the first hexadecimal digit (between 0 and F, where the letters A to F represent 
the numbers 10 to 15). The remainder gives the second hexadecimal digit. 
0-9 -- 0-9
10-15 -- A-F

Example: RGB -- R=201, G=, B=

R = 201/16 = 12 with remainder of 9. So hex code for R is C9 (remember C=12)

Calculating RGB from HEX: #3C1098
3C = 3*16 + 12 = 60
10 = 1*16 + 0 = 16
90 = 9*16 + 8 = 152

###############################
#CODE
#########################
#Convert Hex to RGB (numpy array)
Building = '#3C1098'.lstrip('#')
Building = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152

Land = '#8429F6'.lstrip('#')
Land = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246

Road = '#6EC1E4'.lstrip('#') 
Road = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228

Vegetation =  'FEDD3A'.lstrip('#') 
Vegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58

Water = 'E2A929'.lstrip('#') 
Water = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41

Unlabeled = '#9B9B9B'.lstrip('#') 
Unlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155

#########################
# Now replace RGB to integer values to be used as labels.
#Find pixels with combination of RGB for the above defined arrays...
#if matches then replace all values in that pixel with a specific integer
def rgb_to_2D_label(label):
    """"""
    Supply our labale masks as input in RGB format. 
    Replace pixels with specific RGB values ...
    """"""
    label_seg = np.zeros(label.shape,dtype=np.uint8)
    label_seg [np.all(label == Building,axis=-1)] = 0
    label_seg [np.all(label==Land,axis=-1)] = 1
    label_seg [np.all(label==Road,axis=-1)] = 2
    label_seg [np.all(label==Vegetation,axis=-1)] = 3
    label_seg [np.all(label==Water,axis=-1)] = 4
    label_seg [np.all(label==Unlabeled,axis=-1)] = 5
    
    label_seg = label_seg[:,:,0]  #Just take the first channel, no need for all 3 channels
    
    return label_seg

labels = []
for i in range(mask_dataset.shape[0]):
    label = rgb_to_2D_label(mask_dataset[i])
    labels.append(label)    

labels = np.array(labels)   
labels = np.expand_dims(labels, axis=3)
 

print(""Unique labels in label dataset are: "", np.unique(labels))",Python tips and tricks - 8:  Working with RGB (and Hex) masks for semantic segmentation,2021-05-14T07:00:24+00:00
2517000,"This video demonstrates the process of pre-processing aerial imagery (satellite) data, including RGB labels to get them ready for U-net. The video also demonstrates the process of training a U-net and making predictions. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/228_semantic_segmentation_of_aerial_imagery_using_unet

My Github repo link: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery

The dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The total volume of the dataset is 72 images grouped into 6 larger tiles. The classes are:

Building: #3C1098
Land (unpaved area): #8429F6
Road: #6EC1E4
Vegetation: #FEDD3A
Water: #E2A929
Unlabeled: #9B9B9B

Images come in many sizes: 797x644, 509x544, 682x658, 1099x846, 1126x1058, 859x838, 1817x2061,  2149x1479

Need to preprocess so we can capture all images into numpy arrays. 
Crop to a size divisible by 256 and extract patches.

Masks are RGB and information provided as HEX color code.

Need to convert HEX to RGB values and then convert RGB labels to integer values and then to one hot encoded. 

Predicted (segmented) images need to converted back into original RGB colors. 

Predicted tiles need to be merged into a large image by minimizing blending artefacts (smooth blending). (Next video)",228 - Semantic segmentation of aerial (satellite) imagery using U-net,2021-07-28T07:00:20+00:00
1113000,"This video demonstrates the process of segmenting patches of images from a large image and blending patches back smoothly to minimize edge effects. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery

Original code for smooth blending is from here: https://github.com/Vooban/Smoothly-Blend-Image-Patches

The original code has been modified to fix a couple of bugs and chunks of code unnecessary for smooth tiling are removed.",229 - Smoothblending of patches for semantic segmentation of large images (using U-Net),2021-08-04T07:00:09+00:00
695000,"For example scaling inputs, performing preprocessing operations, converting masks to categorical, etc. 

Code snippet from the video...

#Libraries
import segmentation_models as sm
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
from keras.utils import to_categorical

#Some scaling operation to be applied to images
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
from keras.utils import to_categorical

#Some preprocessing operation on images
BACKBONE = 'resnet34'
preprocess_input = sm.get_preprocessing(BACKBONE)

#Define a function to perform additional preprocessing after datagen.
#For example, scale images, convert masks to categorical, etc. 

def preprocess_data(img, mask, num_class):
    #Scale images
    img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)
    img = preprocess_input(img)  #Preprocess based on the pretrained backbone...
    #Convert mask to one-hot
    mask = to_categorical(mask, num_class)
      
    return (img,mask)

#Define the generator.
#We are not doing any rotation or zoom to make sure mask values are not interpolated.
#It is important to keep pixel values in mask as 0, 1, 2, 3, .....
from tensorflow.keras.preprocessing.image import ImageDataGenerator
def trainGenerator(train_img_path, train_mask_path, num_class):
    
    img_data_gen_args = dict(horizontal_flip=True,
                      vertical_flip=True,
                      fill_mode='reflect')
    
    image_datagen = ImageDataGenerator(**img_data_gen_args)
    mask_datagen = ImageDataGenerator(**img_data_gen_args)
    
    image_generator = image_datagen.flow_from_directory(
        train_img_path,
        class_mode = None,
        batch_size = batch_size,
        seed = seed)
    
    mask_generator = mask_datagen.flow_from_directory(
        train_mask_path,
        class_mode = None,
        color_mode = 'grayscale',
        batch_size = batch_size,
        seed = seed)
    
    train_generator = zip(image_generator, mask_generator)
    
    for (img, mask) in train_generator:
        img, mask = preprocess_data(img, mask, num_class)
        yield (img, mask)

train_img_path = ""data/data_for_keras_aug/train_images/""
train_mask_path = ""data/data_for_keras_aug/train_masks/""
train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=4)

#Make sure the generator is working and that images and masks are indeed lined up. 
#Verify generator.... In python 3 next() is renamed as __next__()
x, y = train_img_gen.__next__()

for i in range(0,3):
    image = x[i]
    mask = np.argmax(y[i], axis=2)
    plt.subplot(1,2,1)
    plt.imshow(image)
    plt.subplot(1,2,2)
    plt.imshow(mask, cmap='gray')
    plt.show()",Python tips and tricks - 9: Performing additional tasks during data augmentation in keras,2021-05-24T07:00:07+00:00
2756000,"Semantic Segmentation of Landcover Dataset by loading images in batches from the drive.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/230_landcover_dataset_segmentation

For all code: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://landcover.ai/
Labels:
0: Unlabeled background 
1: Buildings
2: Woodlands
3: Water

You can use any U-net but this code demonstrates the use of pre-trained encoder in the U-net - available as part of segmentation models library. 

To install the segmentation models library: pip install -U segmentation-models

If you are running into generic_utils error when loading segmentation models library watch this video to fix it: https://youtu.be/syJZxDtLujs.

Prepare the data first: 
1. Read large images and corresponding masks, divide them into smaller patches. And write the patches as images to the local drive.  

2. Save only images and masks where masks have some decent amount of labels other than 0. Using blank images with label=0 is a waste of time and may bias the model towards unlabeled pixels. 

3. Divide the sorted dataset from above into train and validation datasets. 

4. You have to manually move some folders and rename them appropriately if you want to use ImageDataGenerator from keras. 

After training, you can use the smooth blending process to segment large images.",230 - Semantic Segmentation of Landcover Dataset using U-Net,2021-08-11T07:00:00+00:00
910000,"Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information  4 different volumes of the same region
Native (T1)
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1:Necrotic and non-enhancingtumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready 
Step 2: Define custom data generator
Step 3: Define the 3D U-net model
Step 4: Train and Predict

Step 1: Get the data ready
Download the dataset and unzip it.
Segmented file name in Folder 355 has a weird name. Rename it to match others.
Install nibabel library to handle nii files (https://pypi.org/project/nibabel/)
Scale all volumes (using MinMaxScaler).
Combine the three non-native volumes (T2, T1CE and Flair) into a single multi-channel volume.
Reassign pixels of value 4 to value 3 (as 3 is missing from original labels).
Crop volumes to remove useless blank regions around the actual volume of interest (Crop to 128x128x128).
Drop all volumes wherethe amount of annotated data is less that certain percentage. (To maximize training on real labeled volumes).
Save all useful volumes to the local drive as numpy arrays (npy).
Split image and mask volumes into train and validation datasets.

Step 2: Define custom data generator
Keras image data generator only works with jpg, png, and tif images. It will not recognize npy files. We need to define a custom generator to load our data from the disk.

Step 3: Define the 3D U-net model
Extend the standard 2D U-Net into 3D OR 
copy the code from online OR 
use 3D segmentation models library

Step 4: Train and Predict
Train by loading images in batches using our custom generator.
Predict and plot data for visualization.",231 - Semantic Segmentation of BraTS2020 - Part 0 - Introduction (and plan),2021-08-18T07:00:01+00:00
1473000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information  4 different volumes of the same region
Native (T1) 
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1: Necrotic and non-enhancing tumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready (this video)
Step 2: Define custom data generator (next video)
Step 3: Define the 3D U-net model
Step 4: Train and Predict

Step 1: Get the data ready
Download the dataset and unzip it. 
Segmented file name in Folder 355 has a weird name. Rename it to match others.
Install nibabel library to handle nii files (https://pypi.org/project/nibabel/)
Scale all volumes (using MinMaxScaler).
Combine the three non-native volumes (T2, T1CE and Flair) into a single multi-channel volume. 
Reassign pixels of value 4 to value 3 (as 3 is missing from original labels).
Crop volumes to remove useless blank regions around the actual volume of interest (Crop to 128x128x128).
Drop all volumes where the amount of annotated data is less that certain percentage. (To maximize training on real labeled volumes). 
Save all useful volumes to the local drive as numpy arrays (npy).
Split image and mask volumes into train and validation datasets.",232 - Semantic Segmentation of BraTS2020 - Part 1 - Getting the data ready,2021-08-25T07:00:13+00:00
856000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information  4 different volumes of the same region
Native (T1) 
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1: Necrotic and non-enhancing tumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready (Previous video)
Step 2: Define custom data generator (This video)
Step 3: Define the 3D U-net model
Step 4: Train and Predict",233 - Semantic Segmentation of BraTS2020 - Part 2 - Defining your custom data generator,2021-09-01T07:00:06+00:00
1841000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information  4 different volumes of the same region
Native (T1) 
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1: Necrotic and non-enhancing tumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready
Step 2: Define custom data generator (Previous video)
Step 3: Define the 3D U-net model  (This video)
Step 4: Train and Predict  (This video)",234 - Semantic Segmentation of BraTS2020 - Part 3 - Training and Prediction,2021-09-08T07:00:19+00:00
1700000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The video summarizes the concept of autoencoders and walks you though the code for using autoencoder to reconstruct a single image. It also walks you through the code for displaying feature responses of various layer in a deep learning model.",235 - Pre-training U-net using autoencoders - Part 1 - Autoencoders and visualizing features,2021-09-15T07:00:01+00:00
1951000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.epfl.ch/labs/cvlab/data/data-em/

The video walks you through the process of training an autoencoder model and using the encoder weights for U-net.",236 - Pre-training U-net using autoencoders - Part 2 - Generating encoder weights for U-net,2021-09-22T07:00:13+00:00
708000,"This video explains the process of loading images and masks in the right order (in python) for semantic segmentation . 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 10: Loading images and masks in the right order for semantic segmentation,2021-06-03T09:00:10+00:00
1169000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

To install opencv, you need to first install a lot of dependencies on the RPi.

These are the ones I installed to get it working on my RPi 3B.

sudo apt-get update 
sudo apt-get upgrade (consider full upgrade if you haven't used your Pi in a while)
                      
sudo apt-get install build-essential cmake pkg-config
sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng-dev
sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
sudo apt-get install libxvidcore-dev libx264-dev
sudo apt-get install libfontconfig1-dev libcairo2-dev
sudo apt-get install libgdk-pixbuf2.0-dev libpango1.0-dev
sudo apt-get install libgtk2.0-dev libgtk-3-dev
sudo apt-get install libatlas-base-dev gfortran
sudo apt-get install libhdf5-dev libhdf5-serial-dev libhdf5-103
sudo apt-get install libqtgui4 libqtwebkit4 libqt4-test python3-pyqt5
sudo apt-get install python3-dev

Now you can install opencv....
pip install opencv-contrib-python

Now, you need to install tflite interpreter.

You do not need full tensorflow to just run the tflite interpreter.
The package tflite_runtime only contains the Interpreter class which is what we need.
It can be accessed by tflite_runtime.interpreter.Interpreter.
To install the tflite_runtime package, just download the Python wheel
that is suitable for the Python version running on your RPi.

Here is the download link for the wheel files based on the Python version:
https://github.com/google-coral/pycoral/releases/
for Python 3.5, download: tflite_runtime-2.5.0-cp35-cp35m-linux_armv7l.whl (This is what I used in my video)

for Python 3.7, download: tflite_runtime-2.5.0-cp37-cp37m-linux_armv7l.whl


Download face and eye models:
Go to these links, click on RAW and save as... otherwise you'd be saving html files of Github page. '
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml","243 - Real time detection of facial emotion, age, and gender using TensorFlow Lite on RaspberryPi",2021-11-10T08:00:31+00:00
1841000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

First train a DL model and save it as h5. The convert to tflite. 
Dataset from: https://lhncbc.nlm.nih.gov/publication/pub9932

Binary problem: Question is: Is the cell in the image infected/parasited? 
If yes, probability is close to 1. 
If no, the probablility is close to 0. (uninfected)

This is because we added label 1 to parasited images. 
In summary, probability result close to 1 reflects infected (parasited) image 
and close to 0 reflects uninfected image",237 - What is Tensorflow Lite and how to convert keras model to tflite?,2021-09-29T07:00:05+00:00
1154000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Face and eye detection using opencv (Haar Cascade classificaion)

Download face and eye models:
Go to these links, click on RAW and save as... otherwise you'd be saving html files of Github page. '
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml",238 - Real time face detection using opencv (and video feed from a webcam),2021-10-06T07:00:03+00:00
754000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Train a deep learning model on facial emotion detection
Dataset from: https://www.kaggle.com/msambare/fer2013

This trained model will be later used towards real time emotion detection on Windows and raspberry Pi.",239 - Deep Learning training for facial emotion detection,2021-10-13T07:00:11+00:00
637000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Train deep learning models to predict age and gender.
Dataset from here: https://susanqq.github.io/UTKFace/

This trained model will be later used towards real time emotion detection on Windows and raspberry Pi.",240 - Deep Learning training for age and gender detection,2021-10-20T07:00:13+00:00
709000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Live prediction of emotion, age, and gender using pre-trained models. 
Uses haar Cascades classifier to detect face. Then, uses pre-trained models for emotion, gender, and age to predict them from live video feed.","241 - Real time detection of facial emotion, age, and gender (using video feed from a webcam)",2021-10-27T07:00:11+00:00
1068000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Live prediction of emotion, age, and gender using pre-trained models. 
Uses haar Cascades classifier to detect face. Then it uses pre-trained models for emotion, gender, and age to predict them from live video feed. 

Prediction is done using tflite models. Note that tflite with optimization takes too long on Windows, so not even try.
Try it on edge devices, including RPi (next video).","242 - Real time detection of facial emotion, age, and gender using TensorFlow Lite (on Windows10)",2021-11-03T07:00:13+00:00
394000,Amazon link to the book: https://www.amazon.com/Automated-Machine-Learning-AutoKeras-accessible/dp/1800567642/ref=sr_1_2?dchild=1&keywords=autokeras&qid=1624553154&sr=8-2,My review of the 'Automated Machine Learning with AutoKeras' book,2021-06-25T18:59:56+00:00
601000,Career tips on 5 things to check before applying for your first machine learning job.,5 things to check before applying for your first machine learning job,2021-07-19T07:00:04+00:00
393000,A couple of career tips for those wanting to become a machine learning engineer.,"You want to be a machine learning engineer, now what?",2021-07-12T07:00:03+00:00
1104000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Embedding layer...
Maps each value in the input array to a vector of a defined size.
The weights in this layer are learned during the training process.
Initialization is performed (just like other keras layers).

One-hot encoding is inefficient as most indices are zero. (e.g., Text with 1000 words means most of the elements are 0) 
Integer encoding does not reflect the relationship between words. 
Embedding allows for the representation of similar words with similar encoding.
Values are learned (trainable).",244 - What are embedding layers in keras?,2021-11-17T08:00:13+00:00
718000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Tips Tricks 14 - EasyOCR for text detection in images (using python),2021-08-02T07:00:28+00:00
1225000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",245 - Advantages of keras functional API in defining complex models,2021-11-24T08:00:27+00:00
1068000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",246 - Training a keras model by enumerating epochs and batches,2021-12-01T08:00:09+00:00
2391000,"Conditional Generative Adversarial Network  cGAN

A GAN model generates a random image from the domain. 
The relationship between points in the latent space and the generated images is hard to map.
A GAN can be trained so that both the generator and the discriminator models are conditioned on the class label (or other modalities). 
As a result, the trained generator model can be used to generate images of a given type using the class label (or another condition).
GAN can be conditioned using other image modalities (image to image translation).
The conditioning is performed by feeding the class label into both the discriminator and generator as an additional input layer.

A few applications:
Image-to-Image Translation:Pix2Pix GAN
CycleGAN: Transform images from one set into images that could belong to another set.
Super-resolution: Increase the resolution of images, adding detail where necessary to fill in blurry areas. 
Text-to-Image Synthesis: Take text as input and produce images as described by the text.",247 - Conditional GANs and their applications,2021-12-08T08:00:08+00:00
684000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Python tips and tricks - 13
How to plot keras models using plot_model on Windows10

We use the plot_model library:
from tensorflow.keras.utils import plot_model

Plot_model requires Pydot and graphviz libraries.

To install Graphviz: 
Download and install the latest version exe
https://gitlab.com/graphviz/graphviz/-/releases 

To check the installation,
go to the command prompt and enter: dot -V

Open Anaconda prompt for the desired environment 

pip install pydot
pip install graphviz",Tips Tricks 13 - How to visualize keras models on windows10,2021-07-26T07:00:17+00:00
328000,"Week of 12-18 July 2021

Links to the content referenced in the video:
https://iterative-refinement.github.io/
https://arxiv.org/pdf/2104.07636.pdf
https://www.nature.com/articles/s41598-021-93889-z.pdf
https://www.nature.com/articles/s41467-021-23952-w.pdf
https://cdn.openai.com/papers/jukebox.pdf",What I am reading this week about Machine Learning and AI - 16 July 2021,2021-07-16T07:00:12+00:00
1796000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Conditional Generative Adversarial Network  cGAN

A GAN model generates a random image from the domain. 
The relationship between points in the latent space and the generated images is hard to map.
A GAN can be trained so that both the generator and the discriminator models are conditioned on the class label (or other modalities). 
As a result, the trained generator model can be used to generate images of a given type using the class label (or other condition).
GAN can be conditioned using other image modalities (image to image translation).
The conditioning is performed by feeding the class label into both the discriminator and generator as additional input layer.

A few applications:
Image-to-Image Translation: Pix2Pix GAN
CycleGAN: Transform images from one set into images that could belong to another set. 
Super-resolution: Increase the resolution of images, adding detail where necessary to fill in blurry areas. 
Text-to-Image Synthesis: Take text as input and produce images as described by the text.",249 - keras implementation of Conditional GAN (cifar10 data set),2021-12-22T08:00:07+00:00
1869000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",248 - keras implementation of GAN to generate cifar10 images,2021-12-15T08:00:08+00:00
1973000,"A review of the original publication. https://arxiv.org/abs/1611.07004

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The discriminator in the Pix2Pix GAN is implemented as a PatchGAN.
PatchGAN discriminator tries to classify if eachNNpatch in an image is real or fake. (as opposed to classifying an entire image)
This discriminator is run convolutionally across the image, averaging all responses to provide the final output.
The receptive field in a PatchGAN represents the relationship between one output activation to an area on the input image. 
A 7070 PatchGAN will classify 7070 patches of the input image as real or fake.",250 - Image to image translation using Pix2Pix GAN,2021-12-29T08:00:08+00:00
1368000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Data from: http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz
Also find other datasets here: http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/

Original pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf
Github for the original paper: https://phillipi.github.io/pix2pix/",251 - Satellite image to maps translation using  pix2pix GAN,2022-01-05T08:00:05+00:00
1306000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset link: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view
(Please read the Readme document in the dataset folder for more information. )",252 - Generating realistic looking scientific images using pix2pix GAN,2022-01-12T08:00:30+00:00
1552000,"(No code in this tutorial, please watch the next tutorial for keras implementation)
Original paper: https://arxiv.org/abs/1703.10593

The model uses instance normalization layer:
Normalize the activations of the previous layer at each step,
i.e. applies a transformation that maintains the mean activation
close to 0 and the activation standard deviation close to 1.
Standardizes values on each output feature map rather than across features in a batch. 

Download instance normalization code from here: https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/normalization/instancenormalization.py
Or install keras_contrib using guidelines here: https://github.com/keras-team/keras-contrib",253 - Unpaired image to image translation using cycleGAN - An introduction,2022-01-19T08:00:08+00:00
2307000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Original CycleGAN paper: https://arxiv.org/abs/1703.10593

Dataset from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/",254 - Unpaired image to image translation using cycleGAN in keras,2022-01-26T08:00:00+00:00
1763000,"Single Image Super-Resolution Using SRGAN

Understanding the concept by walking through the original publication.
Original paper: https://arxiv.org/pdf/1609.04802.pdf",255 - Single image super resolution using SRGAN,2022-02-02T08:00:14+00:00
1366000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Original paper: https://arxiv.org/pdf/1609.04802.pdf

Dataset from:
    http://press.liacs.nl/mirflickr/mirdownload.html
    
All images resized to 128x128 to represent HR and 32x32 to represent LR.",256 - Super resolution GAN (SRGAN) in keras,2022-02-09T08:00:00+00:00
388000,"Week of 12-18 July 2021

https://www.biorxiv.org/content/10.1101/2021.07.19.452964v1.full.pdf
https://arxiv.org/pdf/2103.10697.pdf
https://bair.berkeley.edu/blog/2021/07/22/spml/
https://arxiv.org/pdf/2105.00957.pdf
https://arxiv.org/pdf/2101.06307.pdf
https://jisrc.szabist.edu.pk/JISRC/Papers/JISR-021-10.pdf
https://www.journals.resaim.com/ijresm/article/view/1018/983
https://irjmets.com/rootaccess/forms/uploads/IRJMETS149474.pdf",What I am reading this week about Machine Learning and AI - 23 July 2021,2021-07-23T17:00:22+00:00
2339000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

UTKFace dataset (Used in this video): https://susanqq.github.io/UTKFace/

Haarcascade models, if interested in detecting faces and extracting them into new images.
https://github.com/opencv/opencv/tree/master/data/haarcascades
Celeb Dataset (Not used in the video): 
https://www.kaggle.com/jessicali9530/celeba-dataset

Description: 
Latent space is hard to interpret unless conditioned using many classes.
But, the latent space can be exploited using generated images.
Here is how...

- Generate 10s of images using random latent vectors.
- Identify many images within each category of interest (e.g., smiling man, neutral man, etc. )
- Average the latent vectors for each category to get the mean representation in the latent space (for that category).
- Use these mean latent vectors to generate images with features of interest. 
In summary, you can find the latent vectors for Smiling Man, neutral face man, and a baby with a neutral face and then generate a smiling babyface by:

    Smiling Man + Neutral Man - Neutral baby = Smiling Baby",257 - Exploring GAN latent space to generate images with desired features,2022-02-16T08:00:13+00:00
1109000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Cross-entropy is a measure of the difference between two probability distributions.",Tips Tricks 15 - Understanding Binary Cross-Entropy loss,2021-08-09T07:00:10+00:00
52000,"Coyote Hills Regional Park - San Francisco East Bay

https://www.ebparks.org/parks/coyote_hills/", around the Coyote Hills Regional Park - San Francisco East Bay,2021-07-31T21:10:14+00:00
1008000,"Rough calculation to estimate the required memory (esp. GPU) to train a deep learning model. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Tips Tricks 16 - How much memory to train a DL model on large images,2021-08-16T07:00:30+00:00
2465000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset:
https://www.kaggle.com/uciml/pima-indians-diabetes-database

A decorator in python allows us to add new functionality to an existing 
object (function or class) by not requiring us to modify the object's structure. 

Decorators allow us to wrap another function to extend the behavior of the wrapped function, without permanently modifying it. They are typically called before defining another function that we'd like to decorate.

Functions are first-class objects in python. This means they support 
the following operations. 

- Stored in a variable. 
- Passed as an argument to another function. 
- Defined inside another function.
- Returned from another function. 
- Store in data structures such as lists.

Decorators leverage this behavior of functions.",Tips Tricks 17 - All you need to know about decorators in python,2021-08-23T07:00:16+00:00
372000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset: https://www.kaggle.com/jessicali9530/celeba-dataset

Haarcascade models...
https://github.com/opencv/opencv/tree/master/data/haarcascades",Tips Tricks 18 - Extracting faces from images for deep learning training,2021-08-30T07:00:01+00:00
450000,"References from the video:
Sketch Your Own GAN: 
https://arxiv.org/pdf/2108.02774.pdf

LARGE: Latent-Based Regression through GAN Semantics: 
https://arxiv.org/pdf/2107.11186.pdf

PathML: A unified framework for whole-slide image analysis with deep learning:
https://www.medrxiv.org/content/10.1101/2021.07.07.21260138v1.full.pdf

SofGAN: A GAN Face Generator That Offers Greater Control
https://www.unite.ai/sofgan-a-gan-face-generator-that-offers-greater-control/

BOOK: Making It Personal: How To Profit From Personalization Without Invading Privacy
Author: Bruce Kasanoff 
Please search your local book store for this book.",What I am reading this week about Machine Learning and AI - 13 August 2021,2021-08-13T07:00:12+00:00
2132000,"Confused about paying for cloud-based systems versus purchasing your own workstation? Hopefully, this video can shed some light.",Tips Tricks 19 - colab vs colab pro vs purchasing your own system,2021-09-06T07:00:09+00:00
2058000,"Semi-supervised learning with generative adversarial networks. 

Semi-supervised refers to the training process where the model gets trained only on a few labeled images but the data set contains a lot more unlabeled images. This can be useful in situations where you have a humongous data set but only partially labeled. 

In regular GAN the discriminator is trained in an unsupervised manner, where it predicts whether the image is real or fake (binary classification). In SGAN, in addition to unsupervised, the discriminator gets trained in a supervised manner on class labels for real images (multiclass classification). 

In essence, the unsupervised mode trains the discriminator to learn features and the supervised mode trains on corresponding classes (labels). The GAN
can be trained using only a handful of labeled examples. 

In a standard GAN our focus is on training a generator that we want to use to generate fake images. In SGAN, our goal is to train the discriminator to be an excellent classifier using only a few labeled images. We can still use the generator to generate fake images but our focus is on the discriminator. 

Why do we want to follow this path is CNNs can easily classify images?
Apparently, this approach achieves better accuracy for limited labeled data compared to CNNs. 
(https://arxiv.org/abs/1606.01583)

Another useful resource: https://arxiv.org/pdf/1606.03498.pdf",258 - Semi-supervised learning with GANs,2022-02-23T08:00:08+00:00
1753000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Semi-supervised learning with generative adversarial networks. 

Semi-supervised refers to the training process where the model gets trained only on a few labeled images but the data set contains a lot more unlabeled images. This can be useful in situations where you have a humongous data set but only partially labeled. 

In regular GAN the discriminator is trained in an unsupervised manner, where it predicts whether the image is real or fake (binary classification). In SGAN, in addition to unsupervised, the discriminator gets trained in a supervised manner on class labels for real images (multiclass classification). 

In essence, the unsupervised mode trains the discriminator to learn features and the supervised mode trains on corresponding classes (labels). The GAN
can be trained using only a handful of labeled examples. 

In a standard GAN our focus is on training a generator that we want to use to generate fake images. In SGAN, our goal is to train the discriminator to be an excellent classifier using only a few labeled images. We can still use the generator to generate fake images but our focus is on the discriminator. 

Why do we want to follow this path is CNNs can easily classify images?
Apparently, this approach achieves better accuracy for limited labeled data compared to CNNs. 
(https://arxiv.org/abs/1606.01583)

Another useful resource: https://arxiv.org/pdf/1606.03498.pdf",259 - Semi-supervised learning with GANs - in keras,2022-03-02T08:00:25+00:00
2009000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Detecting anomaly images using AutoEncoders. 
(Sorting an entire image as either normal or anomaly)

Here, we use both the reconstruction error and also the kernel density estimation
based on the vectors in the latent space. We will consider the bottleneck layer output
from our autoencoder as the latent space. 

This code uses the malarial data set but it can be easily applied to 
any application. 

Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html",260 - Identifying anomaly images using convolutional autoencoders,2022-03-09T08:00:16+00:00
1585000,"Anomaly localization in images using the global average pooling layer.

Binary classification - Good vs. bad images (Uninfected vs parasiized)
This code uses the malarial data set but it can be easily applied to 
any application. 

Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",262 - Localizing anomalies in images,2022-03-23T07:00:13+00:00
911000,"What is the Global Average Pooling (GAP layer) and how it can be used to summrize features in an image?

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",261 - What is global average pooling in deep learning?,2022-03-16T07:00:16+00:00
963000,"Object localization in an image by leveraging the global average pool layer. 

Imagenet classes can be obtained from here:
    https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a
    OR
    https://github.com/Waikato/wekaDeeplearning4j/blob/master/docs/user-guide/class-maps/IMAGENET.md

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",263 - Object localization in images using GAP layer,2022-03-30T07:00:08+00:00
1846000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Outlier detection using alibi-detect

Alibi Detect is an open source Python library focused on outlier, adversarial and drift detection. The package aims to cover both online and offline detectors for tabular data, text, images and time series. The outlier detection methods should allow the user to identify global, contextual, and collective outliers.

pip install alibi-detect

https://github.com/SeldonIO/alibi-detect
Documentation: https://docs.seldon.io/_/downloads/alibi-detect/en/v0.5.1/pdf/

We will be using VAE based outlier detection. Based on this paper:
https://arxiv.org/pdf/1312.6114.pdf
    
The Variational Auto-Encoder (VAE) outlier detector is first trained on a batch of unlabeled, but normal (inlier) data. Unsupervised training is desirable since labeled data is often scarce. The VAE detector tries to reconstruct the input it receives. If the input data cannot be reconstructed well, the reconstruction error is high and the data can be flagged as an outlier. The reconstruction error is either measured as the mean squared error (MSE) between the input and the reconstructed instance or as the probability that both the input and the reconstructed instance are generated by the same process.

Data set info: https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf
Data set link: https://www.mvtec.com/company/research/datasets/mvtec-ad",264 - Image outlier detection using alibi-detect,2022-04-06T07:00:11+00:00
2852000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The input to the Vgg 16 model is 224x224x3 pixels images. 
The Kernel size is 3x3 and the pool size is 2x2 for all the layers.

If our image size is different, can we still use transfer learning?
The answer is YES.

Input image size does not matter as the weights are associated with the filter kernel size. This does not change based on the input image size, for convolutional layers. 

The number of channels does matter, as it affects the number of weights for the first convolutional layer. We can still use transfer learning by copying weights for the first channels from the original model and then filling the additional channel weights with the mean of existing weights along the channels.",Tips Tricks 20 - Understanding transfer learning for different size and channel inputs,2021-09-13T07:00:04+00:00
1860000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

What is a better approach when working with small training data for semantic segmentation? Is it deep learning such as U-net or is it feature extraction followed by machine learning classification (e.g., Random Forest, LGBM, XGBoost, SVM, etc.)?",265 - Feature engineering or deep learning (for semantic segmentation),2022-04-13T07:00:08+00:00
1724000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Demo using H&E normalization and H / E signal separation.
Here, we use openslide to read a whole slide image. 
We will then extract a lower reolution version of the image to normalize it and then to extract H and E signals separately. 

We will also perform the exact operation on the entire whole slide image by extracting tilee, processing them, and saving processed images separately. 

Please note that this code will not cover putting tiles back into a whole slide image (image pyramid). You can explore pyvips or similar package to put together tiles into an image pyramid. 

For an introduction to openslide, please watch video 266: 
    
For details about H&E normalization, please watch my video 122: https://youtu.be/yUrwEYgZUsA
    
Useful references:
A method for normalizing histology slides for quantitative analysis. M. Macenko et al., ISBI 2009
http://wwwx.cs.unc.edu/~mn/sites/default/files/macenko2009.pdf

Efficient nucleus detector in histopathology images. J.P. Vink et al., J Microscopy, 2013

Other useful references:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5226799/
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169875",267 - Processing whole slide images (as tiles),2022-04-27T07:00:12+00:00
2017000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

OpenSlide can read virtual slides in several formats:
Aperio (.svs, .tif)
Hamamatsu (.ndpi, .vms, .vmu)
Leica (.scn)
MIRAX (.mrxs)
Philips (.tiff)
Sakura (.svslide)
Trestle (.tif)
Ventana (.bif, .tif)
Generic tiled TIFF (.tif)

OpenSlide allows reading a small amount of image data at the resolution 
closest to a desired zoom level.

pip install openslide-python

then download the latest windows binaries
https://openslide.org/download/

Extract the contents to a place that you can locate later.

If you are getting the error: [WinError 126] The specified module could not be found

Open the lowlevel.py file located in:
    lib\site-packages\openslide
    
Add this at the top, after from __future__ import division, in the lowlevel.py
os.environ['PATH'] = ""path+to+binary"" + "";"" + os.environ['PATH']
path+to+binary is the path to your windows binaries that you just downloaded.

In my case, it looks like this.

import os
os.environ['PATH'] = ""C:/Users/Admin/anaconda3/envs/py37/lib/site-packages/openslide/openslide-win64-20171122/bin"" + "";"" + os.environ['PATH']

A few useful commands to locate the sitepackages directory

import sys
for p in sys.path:
    print(p)",266 - Openslide library for whole slide images,2022-04-20T07:00:16+00:00
10000,,"Web-deployed deep learning model, on Heroku.",2021-09-15T23:37:16+00:00
1750000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video explains the process of using Flask to deploy your scikit-learn (or other) trained model into a web application.",268 - How to deploy your trained machine learning model into a local web application?,2022-05-04T07:00:07+00:00
1541000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video demonstrates the process of deploying your trained machine learning model as a browser-based app on Heroku (www.heroku.com)",269 - How to deploy your trained machine learning model as a web app on Heroku?,2022-05-11T07:00:27+00:00
2428000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video demonstrates the process of deploying your trained deep learning model as a browser-based app on Heroku. It uses HAM10000 trained model to predict skin cancer type for the image supplied by the user. 

Some useful commands:
Please install Heroku CLI on your system.

To log in to your Heroku account from CLI:
heroku login -i

To scale dynos for your app:
heroku ps:scale web=1 --app app_name",270 - How to deploy your trained machine learning model as a web app on Heroku (No Docker),2022-05-18T07:00:21+00:00
1954000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video demonstrates the process of deploying your trained deep learning model as a browser-based app on Heroku by deploying a Docker container. It uses HAM10000 trained model to predict skin cancer type for the image supplied by the user. This application is exactly the same as the one from video 270 except here we use Docker. 


Some useful commands:
Please install Heroku CLI on your system.

To log in to your Heroku account from CLI:
heroku login -i

To scale dynos for your app:
heroku ps:scale web=1 --app app_name

Docker:
Please install Docker on your system (e.g. Docker for Windows)
Our app name from this video is used as an example for the following commands. 

To build an image using the Docker file:
docker image build -t skincancer-app .

List Docker images:
docker image ls

To run Docker on a specific port:
docker run -p 5000:5000 -d skincancer-app

Open the URL to check your app.
http://localhost:5000/

To list docker containers:
docker container ls

To stop a container:
docker container stop container id

To clear all containers and cleanup:
docker system prune

To remove a Docker image: (check the image ID via: docker image ls)
docker image rm image_id

______________________

Heroku Docker deployment process from command prompt. 

heroku container:login

heroku create name-for-your-app

heroku container:push web --app name-for-your-app

heroku container:release web --app name-for-your-app

heroku ps:scale web=1 --app app_name",271 - How to deploy your trained machine learning model as a web app on Heroku (with docker),2022-05-25T07:00:09+00:00
1368000,"This video explains the process of exploring keras model saved as hdf5 (or .h5).

To download the HDF Viewer: https://www.hdfgroup.org/downloads/hdfview/",Tips Tricks 21 - Understanding the keras-trained model saved as hdf5 (or h5),2021-10-04T07:00:17+00:00
831000,"Link to the book on Amazon: https://www.amazon.com/Deep-Learning-fastai-Cookbook-easy/dp/1800208103/ref=sr_1_4?dchild=1&keywords=deep+learning+with+fastai&qid=1634237733&sr=8-4

Fastai API paper (pdf): https://arxiv.org/pdf/2002.04688.pdf",Book Review - Deep Learning with fastai Cookbook,2021-10-16T13:30:27+00:00
1728000,"Lung cancer subclassification using fastai 

Fastai API info: https://arxiv.org/pdf/2002.04688.pdf

Direct link to the colab notebook:
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_tricks_22_fastai_lung_cancer_classification.ipynb

Data set information:
https://github.com/tampapath/lung_colon_image_set/
https://arxiv.org/ftp/arxiv/papers/1912/1912.12142.pdf
LC25000 Lung and colon histopathological image dataset from: https://academictorrents.com/details/7a638ed187a6180fd6e464b3666a6ea0499af4af

The dataset contains color 25,000 images with 5 classes of 5,000 images each. All images are 768 x 768 pixels in size and are in jpeg file format. Our dataset can be downloaded as a 1.85 GB zip file LC25000.zip. After unzipping, the main folder lung_colon_image_set contains two subfolders: colon_image_sets and lung_image_sets.

The subfolder lung_image_sets contains three secondary subfolders: lung_aca subfolder with 5000 images of lung adenocarcinomas, lung_scc subfolder with 5000 images of lung squamous cell carcinomas, and lung_n subfolder with 5000 images of benign lung tissues.",Lung cancer subclassification using fastai (Tips Tricks 22),2021-10-23T07:00:06+00:00
1853000,"Code from this video is available here:
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_23_COVID_vaccine_analysis.ipynb

COVID vaccination data can be downloaded from here: 
https://www.kaggle.com/gpreda/covid-world-vaccination-progress",COVID Vaccine analysis using pandas in python (Tips Tricks 23),2021-11-06T07:00:09+00:00
1324000,"pyviz documentation: https://pyvis.readthedocs.io/en/latest/

Code used in this video is available here:
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_24_quick_intro_to_pyviz.ipynb",Tips Tricks 24 - Interactive network visualization using pyviz,2021-11-20T08:00:08+00:00
781000,"Locating objects in large images using template matching
(opencv in python)

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_25_locating_objects_in_large_images_via_template_matching.py",Tips Tricks 25 - Locating objects in large images using template matching,2021-11-22T22:42:46+00:00
751000,"7 best machine learning books in 2022

Basics:
The hundred-page machine learning book
Deep learning with python (Franois Chollet)
Automated machine learning with AutoKeras
Deep learning for computer vision with python

Advanced:
Deep Learning (Ian Goodfellow)
Pattern recognition and machine learning
Artificial intelligence, A modern approach - Fourth Edition",7 best machine learning books in 2022,2021-12-24T08:00:12+00:00
778000,"Just normalize your 16 bit image to its respective maximum pixel value and then convert to uint8 using numpy or opencv or scikit-image

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_26_proper-way_to_convert_16bit_to_8bit_image.py

For my other code:
https://github.com/bnsreenu/python_for_microscopists",Tips Tricks 26 - How to properly convert 16 bit to 8 bit images in python,2022-01-15T08:00:02+00:00
293000,Link to the book on Amazon: https://www.amazon.com/Machine-Learning-Biotechnology-Life-Sciences/dp/1801811911,Book Review - Machine Learning in Biotechnology and Life Sciences,2022-03-05T08:00:03+00:00
756000,"Do not waste your time doing manual things that can be automated. This video walks you through the simple python code to extract required information from your Outlook inbox. You can find the code to this video here:
https://github.com/bnsreenu/python_for_microscopists/tree/master/AMT01_extracting_information_from_outlook_emails

For other code, checkout my GitHub repo: https://github.com/bnsreenu/python_for_microscopists",AMT1 - Extracting required information from your Outlook inbox,2022-02-19T00:22:39+00:00
962000,"Do not waste your time doing manual things that can be automated. This video walks you through the simple python code to extract required information from your Gmail inbox. A great way pf compiling text for your natural language processing and other machine learning projects. 

You can find the code to this video and other videos here:
https://github.com/bnsreenu/python_for_microscopists",AMT2 - Extracting Emails from your Gmail Inbox using python,2022-02-26T08:00:05+00:00
1628000,"The code snippet for this video can be downloaded from:  
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_tricks_27_labeling_images_for_sem_segm_using_label_studio.py

For other code available on my GitHub:
https://github.com/bnsreenu/python_for_microscopists

For labeling your images using Label Studio:
https://labelstud.io/

Let us work in Anaconda command prompt. (You can use other command prompts)
Check environments: 
conda env list

Create a new environment to install Label Studio:
conda create --name give_some_name pip
(Need to specify pip as a dependency, otherwise it will not be available)

(To specify python version for your env..)
conda create -n give_some_name python=3.7

Now activate the env.
conda activate give_some_name

# Install the Label Studio package
pip install -U label-studio

# Launch it!
label-studio

Open your browser and go to the URL displayed on your screen, typically
http://0.0.0.0:8080/",Labeling images for semantic segmentation using Label Studio,2022-03-12T08:00:11+00:00
645000,"Part of the Tips and tricks series - Number: 28 

Download QuPath from:
https://qupath.github.io/

Download the groovy script from here:
https://raw.githubusercontent.com/stardist/stardist/master/extras/qupath_export_annotations.groovy",Labeling images using QuPath for semantic segmentation,2022-03-19T07:00:01+00:00
946000,"Part of the Tips and tricks series - Number: 29

Download FiJi from here:
https://imagej.net/software/fiji/

Install the LabKit plugin:
https://imagej.net/plugins/labkit/",Labeling images using LabKit for semantic segmentation,2022-03-26T07:00:05+00:00
1574000,"Part of the Tips and tricks series - Number: 30 
Random gets used quite often in python for data analysis and machine learning. This is an explainer video on the topic of 'random' and random seeds. Also, learn about the birthday paradox.

Python random uses Mersenne Twister algorithm:
https://en.wikipedia.org/wiki/Mersenne_Twister

Numpy random uses the Permuted congruential generator algorithm:
https://en.wikipedia.org/wiki/Permuted_congruential_generator

More about the Birthday paradox/problem:
https://en.wikipedia.org/wiki/Birthday_problem#Understanding_the_problem",Random is not so random - understanding random in python,2022-04-02T07:00:01+00:00
542000,"Tips tricks 31 - generating borders around objects

Code on my GitHub: https://github.com/bnsreenu/python_for_microscopists

Create border pixels from binary masks. We can include these border pixels as another class to train a multiclass semantic segmenter. 
What is the advantage? We can use border pixels to perform watershed and achieve 'instance' segmentation.",Generating borders around objects for use in semantic segmentation,2022-04-09T07:00:02+00:00
2028000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video goes through the process of adding borders to binary objects, then using them as masks to train a multiclass U-net model, and finally segmenting images using the trained model followed by watershed separation.",272 - Instance segmentation via semantic segmentation by using border class,2022-06-01T07:00:05+00:00
1213000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Object segmentation and analysis using voronoi otsu labeling in the pyclesperanto library in python

We will be using a multichannel CZI (Zeiss) input image for this exercise.
This requires czi file library. 

pip install czifile

For standard images (e.g., jpg, tif, etc.) use skimage, cv2, or tifffile to read
input images. 

# For installation instructions of the pyclesperanto package, 
please refer to the following link
# https://github.com/clEsperanto/pyclesperanto_prototype",275 - Object segmentation and analysis using voronoi otsu labeling,2022-06-22T07:00:12+00:00
616000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Grain segmentation using less than 10 lines of code in python
uses Voronoi labeling from the pyclesperanto library in python

# For installation instructions of the pyclesperanto package, 
please refer to the following link
# https://github.com/clEsperanto/pyclesperanto_prototype",276 - Grain segmentation using less than 10 lines of code in python,2022-06-29T07:00:12+00:00
614000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Do not waste your time with deep learning to segment relatively easy to segment objects against a background. This tutorial explains the simple process for 3D object segmentation by using Voronoi labeling from the pyclesperanto library in python

# For installation instructions of the pyclesperanto package, 
please refer to the following link
# https://github.com/clEsperanto/pyclesperanto_prototype",277 - 3D object segmentation in python,2022-07-06T07:00:00+00:00
758000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

A Voronoi diagram divides the plane into separate regions where each region contains exactly one generating point (seed) and every point in a given region is closer to its seed than to any other. The regions around the edge of the cluster of points extend out to infinity. 

This video explains Voronoi using python.",273 - What is Voronoi - explanation using python code,2022-06-08T07:00:13+00:00
877000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Step by step... 
Refer to the next tutorial for a single step function that does the job. 

Steps...
#Step 1: gaussian blur the image and detect maxima for each nuclei
#Step 2: threshold the input image after applying light gaussian blur (sigma=1)
#Step 3: Exclude maxima locations from the background, to make sure we only include the ones from nuclei
#Step 4: Separate maxima locations into labels using masked voronoi
#Step 5: Separate objects using watershed.

# For installation instructions of the pyclesperanto package, 
please refer to the following link
# https://github.com/clEsperanto/pyclesperanto_prototype",274 - Object segmentation using voronoi and otsu,2022-06-15T07:00:30+00:00
603000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

RGB to Haematoxylin-Eosin-DAB (HED) color space conversion followed by nuclei segmentation and analysis using Stardist.",282 -  IHC color separation followed by nuclei segmentation using StarDist in python,2022-08-10T07:00:06+00:00
853000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

RGB to Haematoxylin-Eosin-DAB (HED) color space conversion followed by nuclei segmentation and analysis using Voronoi otsu

Separate the immunohistochemical (IHC) staining from the hematoxylin counterstaining. The IHC staining expression of the FHL2 protein is here revealed with diaminobenzidine (DAB) which gives a brown color.

A. C. Ruifrok and D. A. Johnston, Quantification of histochemical staining by color deconvolution, Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001. PMID: 11531144

https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_ihc_color_separation.html#sphx-glr-auto-examples-color-exposure-plot-ihc-color-separation-py

Try WSI datasets from here
https://zenodo.org/record/1485967#.Yd31lv7MKbh
https://www.wouterbulten.nl/blog/tech/peso-dataset-whole-slide-image-prosate-cancer/",278 - IHC color separation followed by nuclei segmentation using python,2022-07-13T07:00:06+00:00
1244000,"For details on installing StarDist library and other examples:
https://github.com/stardist/stardist

Stardist helps with object detection of star-convex shapes, both in 2D and 3D. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/279_An_introduction_to_object_segmentation_using_StarDist.ipynb

All other code:
https://github.com/bnsreenu/python_for_microscopists",279 - An introduction to object segmentation using StarDist library in Python,2022-07-20T07:00:04+00:00
1471000,"Code generated in the video can be downloaded from here: 
Train: https://github.com/bnsreenu/python_for_microscopists/blob/master/280a_custom_object_segmentation_using_stardist_TRAIN.ipynb
Predict: https://github.com/bnsreenu/python_for_microscopists/blob/master/280b_custom_object_segmentation_using_stardist_PREDICT.ipynb

All other code: 
https://github.com/bnsreenu/python_for_microscopists

This video tutorial explains the process of training your own StarDist model for object segmentation. It walks you through the process of importing training images and corresponding masks, training a model, and segmenting (any size) images using the trained model. 

Warning: You may find this approach more efficient compared to U-net or Mask-RCNN",280 - Custom object segmentation using StarDist library in python,2022-07-27T07:00:06+00:00
819000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video tutorial walks you through the process of importing whole slide H&E stained images (e.g., .svs format), segmenting nuclei using a pre-trained model, and reporting the measurements of the segmented nuclei. 

Prepare to be amazed by the power of StarDist.",281 - Segmenting whole slide images (WSI) for nuclei using StarDist in python,2022-08-03T07:00:15+00:00
445000,"Tips Tricks - 32 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This tutorial explains the process of automating mouse movement in Windows using python. This technique can be used to keep any programs running without getting timed out (e.g., colab). This can also be used to keep your micromanaging manager happy but showing your status in teams (or similar s/w) as Active (green).",Automate periodic mouse movements using python,2022-04-22T07:00:16+00:00
636000,"Tips and tricks - 33: Learn about hyperparameters visually using the neural network playground

Link to the playground: http://playground.tensorflow.org/",Learn about neural network hyperparameters visually,2022-04-30T07:00:14+00:00
808000,"Tips and tricks video # 34: 
10 best image annotation tools for computer vision applications

Free:
1. Make Sense: https://www.makesense.ai/
2. VGG Image Annotator: https://www.robots.ox.ac.uk/~vgg/software/via/
3. Computer Vision Annotator Tool (CVAT): https://github.com/openvinotoolkit/cvat
4. Labelme: http://labelme.csail.mit.edu/
5. Dash Doodler: https://github.com/Doodleverse/dash_doodler
6. LabelImg: https://github.com/tzutalin/labelImg
7. Label Studio: https://labelstud.io/

Paid:
8. LabelBox: https://labelbox.com/
9. Scale: https://scale.com/
10. Superannotate: https://www.superannotate.com/",10 best annotation tools for computer vision applications,2022-05-07T07:00:05+00:00
645000,"Tips and Tricks 35 - Loading Kaggle data directly into Google Colab

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_tricks_35_loading_kaggle_data_to_colab.ipynb

My GitHub repo link:
https://github.com/bnsreenu/python_for_microscopists",Loading Kaggle data directly into Google Colab,2022-05-20T07:00:30+00:00
1426000,"283 What is mask RCNN - 

CNN  Convolutional Neural Network
R-CNN  Region-based Convolutional Neural Network
Faster R-CNN  Faster Region-based Convolutional Neural Network
Mask R-CNN  Mask (faster) Region-based Convolutional Neural Network

Mask R-CNN is built using Faster R-CNN
In addition to class label and bounding box, Mask R-CNN outputs an object mask. 
Uses a trick called ROIAlign to locate relevant areas down to pixel level. ",283 - What is Mask R-CNN?,2022-08-17T07:00:07+00:00
1304000,"Installing Mask RCNN for Windows on Python 3.7 and TensorFlow 2.2.

Link to the original repo from matterport that works on TF1.x:
https://github.com/matterport/Mask_RCNN

Link to the repo that works on TF2.x :
https://github.com/ahmedfgad/Mask-RCNN-TF2

requirements.txt from the file used in the video:
numpy==1.20.3
scipy==1.4.1
Pillow==8.4.0
cython==0.29.24
matplotlib
scikit-image==0.16.2
tensorflow==2.2.0
keras==2.3.1
opencv-python==4.5.4.60
h5py==2.10.0
imgaug==0.4.0
IPython[all]",284 - Installing Mask RCNN and troubleshooting errors,2022-08-24T07:00:10+00:00
1621000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video helps you get started with your first Mask RCNN project that uses existing annotated data. Annotated data used in this video can be downloaded from:
https://www.kaggle.com/datasets/mbkinaci/fruit-images-for-object-detection

XML data in PascalVOC format
XML annotation file for each image

Please note that this video assumes you have installed Mask RCNN on your system. For installation instructions, please watch the previous video (Video numbered 284).

coco weights can be downloaded from: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5",285 - Object detection using Mask RCNN (with XML annotated data),2022-08-31T07:00:05+00:00
1665000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/286-Object%20detection%20using%20mask%20RCNN%20-%20end%20to%20end

All other code:
https://github.com/bnsreenu/python_for_microscopists

This video helps you with end-to-end Mask RCNN project, all the way from annotations to training to prediction. Handling of VGG and Coco style JSON annotations is demonstrated in the video. Code is also made available for both approaches. 

For this video, I've used the annotation tool from https://www.makesense.ai/

You can try other annotation tools like:
https://www.makesense.ai/
https://labelstud.io/
https://github.com/Doodleverse/dash_doodler
http://labelme.csail.mit.edu/Release3.0/
https://github.com/openvinotoolkit/cvat
https://www.robots.ox.ac.uk/~vgg/software/via/

coco weights can be downloaded from: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5",286 - Object detection using Mask RCNN: end-to-end from annotation to prediction,2022-09-07T07:00:17+00:00
1226000,"Tips and Tricks 36: 

Code from this video is available at: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/tips_tricks_36_pyscript_python_in_the_browser

Useful resources: 
https://pyscript.net/
https://github.com/pyscript/pyscript

If we want to load an external python file as a module into the html file, the best way would be to start a local webserver. 

If the html file is located at /my_dir/my_file.html
if you just open the my_file.html in a browser, it will not recognize other files referenced.

You need to start a local webserver.

Open the command prompt (or conda prompt) - wherever you can just type python to execute python commands.
Now type: python -m http.server (or python3 - depending on how you access your local python)

This should start a webserver. Now just go to the browser and type: http://localhost:8000/
You should see a list of files in that local directory. Now, click the html file.",PyScript  Running python in your browser,2022-05-21T03:00:23+00:00
1437000,"Tips Tricks 37 - MAE vs MSE vs Huber
Understanding Mean Absolute Error and Mean Squared Error as ML metrics and loss functions

Code from this video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists/blob/master/tips_tricks_37_Understanding%20MAE%20and%20MSE.py

Use MSE if outliers are important.

USE MAE if outliers are not important (most cases).

Use Huber to get a balance between giving outliers some weight but not a lot (like in MSE). ",Understanding Mean Absolute Error and Mean Squared Error as ML metrics and loss functions,2022-05-28T07:00:11+00:00
863000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Want to learn about the basics of GMM and how to use it for image segmentation: https://youtu.be/kkAirywakmk","52b - Understanding Gaussian Mixture Model (GMM) using 1D, 2D, and 3D examples",2022-07-01T07:00:04+00:00
577000,"Installing Conda in Google Colab

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/tips_tricks_38_Installing_conda_in_Google_Colab.ipynb

All other code: https://github.com/bnsreenu/python_for_microscopists

This tutorial guides you through the process of installing Conda in your Colab.

Why do you need conda on Colab? Among other benefits, certain packages require Conda for installation. For example, trackpy installation: http://soft-matter.github.io/trackpy/dev/installation.html

Trackpy strongly recommends using Conda and not pip. Therefore, we need to set up conda environment for our colab notebook.

Miniconda system requirements: https://docs.conda.io/en/latest/miniconda.html#system-requirements",Installing Conda in Google Colab - Tips Tricks 38,2022-07-08T07:00:19+00:00
1861000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/287_tracking_particles_using_trackpy.ipynb

All other code: 
https://github.com/bnsreenu/python_for_microscopists

This video provides an introduction to the Trackpy library that can be used to segment objects using blob detection and then track them in a time series of images (video). 

Trackpy installation: http://soft-matter.github.io/trackpy/dev/installation.html

Trackpy strongly recommends using Conda and not pip. Therefore, we need to set up conda environment for our colab notebook.
Miniconda versions: https://docs.conda.io/en/latest/miniconda.html#system-requirements

This specific topic has been covered in a separate tutorial: https://youtu.be/v4qskw8EHXQ

For more examples of trackpy and to download sample data sets: https://github.com/soft-matter/trackpy-examples",287 - Tracking particles and objects using Trackpy in python,2022-09-14T07:00:32+00:00
1487000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/288_nuclei_tracking_trackpy_stardist.ipynb

All other code:
https://github.com/bnsreenu/python_for_microscopists

This video walks you through the process of segmenting nuclei in fluorescence microscope images using StarDist (pre-trained deep learning model), followed by tracking them using trackpy. 

Trackpy installation: http://soft-matter.github.io/trackpy/dev/installation.html

Trackpy strongly recommends using Conda and not pip. Therefore, we need to set up conda environment for our colab notebook.

This specific topic has been covered in a separate tutorial: https://youtu.be/v4qskw8EHXQ",288 - Nuclei segmentation using StarDist and tracking using Trackpy in python,2022-09-28T07:00:17+00:00
1204000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/289_tracking_particles_in_3D.ipynb

All other code: 
https://github.com/bnsreenu/python_for_microscopists

This video walks you through the process of tracking 3D objects using trackpy. 

Trackpy installation: http://soft-matter.github.io/trackpy/dev/installation.html

Trackpy strongly recommends using Conda and not pip. Therefore, we need to set up conda environment for our colab notebook.

We will be using this data set: https://github.com/soft-matter/trackpy-examples/blob/master/sample_data/pmma_colloids.zip

This specific topic has been covered in a separate tutorial: https://youtu.be/v4qskw8EHXQ",289 - 3D object tracking using trackpy in python,2022-10-12T07:00:16+00:00
744000,"Tips and tricks 39: Installing napari library in python for scientific image visualization

Napari allows for visualization-focused image analysis in python. Installation of napari is extremely easy, especially if you follow their directions. This video provides a walkthrough tutorial of installing napari and using it via Anaconda Spyder IDE. 

For more information about napari, please visit: https://napari.org/index.html",Installing napari library in python for scientific image visualization - Tips and Tricks 39,2022-07-15T07:00:16+00:00
256000,"Machine Learning with scikit-learn and scientific python toolkits

https://www.amazon.com/dp/B08BTFY8YW/ref=cm_sw_r_tw_dp_P34RZ4XB5CY0SNPB0T5S",Book Review - Machine Learning with scikit-learn and scientific python toolkits,2022-07-18T15:35:44+00:00
898000,"7 (+2) AI-powered fun and useful web applications



1. https://this-person-does-not-exist.com/en
2. https://bigspeak.ai/
3. https://www.magiceraser.io/
4. https://www.craiyon.com/
5. https://rytr.me/
6. https://namelix.com/
7. https://letsenhance.io/
8. https://imglarger.com/
9. https://experiments.withgoogle.com/thing-translator",7 (+2) AI-powered fun and useful web applications,2022-08-13T07:00:10+00:00
1290000,"What is the difference between Data science, data analytics, AI, machine learning, and deep learning? What skills do you need to learn these topics and how to structure your learning plan?",How to get started with Data Science and Machine Learning,2022-08-19T07:00:02+00:00
691000,"23b - Image segmentation using color spaces

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video walks you through the process of segmenting images using color spaces. Here, we use simple thresholding of a specific color by converting RGB image to HSL.

For example: Blue marbles in an image showing marbles of various colors.  

To select specific color range in HSL space...
#https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv/48367205#48367205",23b - Image segmentation using color spaces - in python,2022-08-26T07:00:01+00:00
420000,"tips tricks 42 - How to remove text from images

Code generated in the video can be downloaded from here: 
https://raw.githubusercontent.com/bnsreenu/python_for_microscopists/master/Tips_Tricks_42_How%20to%20remove%20text%20from%20images.py

Other code:
https://github.com/bnsreenu/python_for_microscopists

General Approach.....
Use keras OCR to detect text, define a mask around the text, and inpaint the masked regions to remove the text. To apply the mask we need to provide the coordinates of the starting and the ending points of the line, and the thickness of the line.",How to remove text from images using python?,2022-09-02T07:00:02+00:00
601000,"tips tricks 43 - Color segmentation of images followed by text removal in python

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Remove text from images, only from segmented regions
Here, we use simple thresholding of a specific color by converting RGB image to HSL.

For example: The yellow/orange part from the traffic sign. 

To select specific color range in HSL space...
#https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv/48367205#48367205",Color segmentation of images followed by text removal in python,2022-09-09T07:00:06+00:00
1175000,"Deep Learning based edge detection using holistically nested edge detection (HED)

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/290-Deep%20Learning%20based%20edge%20detection%20using%20HED

All other code: 
https://github.com/bnsreenu/python_for_microscopists

Original HED paper: https://arxiv.org/pdf/1504.06375.pdf

Caffe model is encoded into two files
1. Proto text file: https://github.com/s9xie/hed/blob/master/examples/hed/deploy.prototxt
2. Pretrained caffe model: http://vcl.ucsd.edu/hed/hed_pretrained_bsds.caffemodel
NOTE: In future, if these links do not work, I cannot help. Please Google 
and find updated links (information current as of October 2022)

HED is a deep learning model that uses fully convolutional neural networks and deeply-supervised nets to do image-to-image prediction.

The output of earlier layers is called side output. 
HED makes use of the side outputs of intermediate layers. 
The output of all 5 convolutional layers is fused to generate the final predictions. 
Since the feature maps generated at each layer is of different size, its effectively looking at the image at different scales. 

The model is VGGNet with few modifications:
Side output layer is connected to the last convolutional layer in each stage, respectively conv1_2, conv2_2, conv3_3, conv4_3,conv5_3. The receptive field size of each of these convolutional layers is identical to the corresponding side-output layer.

Last stage of VGGNet is removed including the 5th pooling layer and all the fully connected layers.

The final HED network architecture has 5 stages, with strides 1, 2, 4, 8 and 16, respectively, and with different receptive field sizes, all nested in the VGGNet. ",290 - Deep Learning based edge detection using HED,2022-10-26T07:00:17+00:00
1100000,"291 - Object segmentation using Deep Learning based edge detection (HED) followed by connected component based labeling in opencv

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Original HED paper: https://arxiv.org/pdf/1504.06375.pdf

Caffe model is encoded into two files
1. Proto text file: https://github.com/s9xie/hed/blob/master/examples/hed/deploy.prototxt
2. Pretrained caffe model: http://vcl.ucsd.edu/hed/hed_pretrained_bsds.caffemodel
Alternate Link: https://github.com/ashukid/hed-edge-detector/blob/master/hed_pretrained_bsds.caffemodel

NOTE: In future, if these links do not work, I cannot help. Please Google 
and find updated links (information current as of October 2022)

HED is a deep learning model that uses fully convolutional neural networks and deeply-supervised nets to do image-to-image prediction.",291 - Object segmentation using Deep Learning based edge detection (HED),2022-11-09T08:00:20+00:00
1016000,"Denoising images using deep learning (Noise2Void)
Do not let noise distract you from the truth

Classical denoising
Gaussian, Total Variation, Median, Non-local means (NLM), Block-matching and 3D filtering (BM3D)

Deep Learning denoising
Autoencoders, Deep CNNs, GANs, Noise2Noise, Noise2Void

Noise2Void:
https://arxiv.org/abs/1811.10980
https://github.com/juglab/n2v

Noise2Void learns directly from noisy images without the need for clean images, making it the ideal choice for denoising confocal images.

Only requires noisy images, no need for additional noisy or clean images. 

Assumption:
Signal has a structure and noise does not. 
Therefore, it is possible to predict signal by looking at the surrounding pixels but impossible to predict noise. 

For information about ZEN: https://www.zeiss.com/microscopy/en/products/software/zeiss-zen.html",292 - Denoising images using deep learning (Noise2Void),2022-11-23T08:00:21+00:00
1442000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb

All other code:
https://github.com/bnsreenu/python_for_microscopists

This video covers the topic training a Noise2Void denoising model using RGB images. Images need to be in lossless format (e.g., png or tiff). JPG format is not allowed if you're using the DataGenerator object but you can use skimage or other libraries to load them. The same approach can be used for grey-scale images, for example SEM or CT images. 

Next video in this series will cover multichannel and 3D image denoising. 

Noise2Void:
https://arxiv.org/abs/1811.10980
https://github.com/juglab/n2v

Noise2Void learns directly from noisy images without the need for clean images, making it the ideal choice for denoising confocal images.

Only requires noisy images, no need for additional noisy or clean images. 

Assumption:
Signal has a structure and noise does not. 
Therefore, it is possible to predict signal by looking at the surrounding pixels but impossible to predict noise. ",293  - Denoising RGB images using deep learning (Noise2Void),2022-12-07T08:00:04+00:00
1343000,"294 - Denoising 3D multi-channel scientific images using Noise2Void deep learning approach

This video explains the process of denoising 2D and 3D multichannel scientific images (e.g., CZI images) using Noise2Void deep learning approach. The same approach can be taken for any scientific image in any format, as long as you have an appropriate library to read the files. To read CZI images, we will be using czifile library. 

Please note that CZI files are from ZEISS light microscopes.

Noise2Void:
https://arxiv.org/abs/1811.10980
https://github.com/juglab/n2v

Noise2Void learns directly from noisy images without the need for clean images, making it the ideal choice for denoising confocal images.

Only requires noisy images, no need for additional noisy or clean images. 

Assumption:
Signal has a structure and noise does not. 
Therefore, it is possible to predict signal by looking at the surrounding pixels but impossible to predict noise. 

For information about ZEN: https://www.zeiss.com/microscopy/en/products/software/zeiss-zen.html

Code from this video:
2D multichannel: https://github.com/bnsreenu/python_for_microscopists/blob/master/294_n2v_2D_multi_ch_czi.ipynb

3D multichannel: https://github.com/bnsreenu/python_for_microscopists/blob/master/294_n2v_3D_multi_ch_czi.ipynb

All my code: https://github.com/bnsreenu/python_for_microscopists/",294 - Denoising 3D multi-channel scientific images using Noise2Void deep learning approach,2022-12-21T08:00:30+00:00
1264000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/296-Converting%20keras-trained%20model%20to%20ONNX%20format-Img%20Classification.py

All other code:
https://github.com/bnsreenu/python_for_microscopists

This tutorial covers the following topics...
1. Training a keras model for multiclass classification using the cifar10 dataset.
2. Saving the keras model as .h5
3. Classification using the saved keras .h5 model
4. Converting the keras model to onnx format
5.Classification using the onnx model (and comparison to the keras results)

pip install keras2onnx  #For older tensorflow (up to 2.3.1)
pip install tf2onnx  #For newer tensorflow (I tested on 2.4.4)
pip install onnxruntime
pip install h5py",296 - Converting keras trained model to ONNX format - Image Classification example,2023-01-18T08:00:22+00:00
1131000,"Code generated in the video can be downloaded from here: 
Main file: https://github.com/bnsreenu/python_for_microscopists/blob/master/297-Converting%20keras-trained%20model%20to%20ONNX-Sem%20Segm.py

Unet model: https://github.com/bnsreenu/python_for_microscopists/blob/master/297-simple_unet_model.py

All other code:
https://github.com/bnsreenu/python_for_microscopists

Semantic segmentation using ONNX model

A complete project that walks through the process of training a keras model using data augmentation, exporting it to ONNX, and segmenting using the ONNX model. 

In this project, we will be working with Mitochondria data set https://www.epfl.ch/labs/cvlab/data/data-em/
We will be using small dataset (12 images and masks of 768x1024 each - further divided into 256x256 patches)
Augmentation is used to artificially enhance the number of training images. 
NOTE: While augmentation helps, you cannot augment your way out of having limited training data.

We will use a simple 2D U-net model for segmentation.

The trained model will be saved as a keras (.h5) model.
We will segment a few images using the trained keras model. 

This model will then be saved as ONNX. 
The ONNX model will be used to segment some images.",297 - Converting keras trained model to ONNX format - Semantic Segmentation,2023-02-01T08:00:01+00:00
865000,"ONNX  open format for machine learning models.

ONNX is a new standard for exchanging deep learning models. 
It is an intermediary machine learning framework used to convert between different machine learning frameworks.

Useful blogs:
https://towardsdatascience.com/an-empirical-approach-to-speedup-your-bert-inference-with-onnx-torchscript-91da336b3a41
https://cloudblogs.microsoft.com/opensource/2022/04/19/scaling-up-pytorch-inference-serving-billions-of-daily-nlp-inferences-with-onnx-runtime/

Example scenario  You are deploying your trained model via an iOS app:
Modern Apple devices come with Apple Neural Engine (ANE) as part of the chip.
Deep learning models can work on CPU only, CPU+GPU, or all CPU/GPU/ANE
Fastest way is to run it on ANE, especially for heavy applications
The framework optimized for deep learning inference on iOS is CoreML
CoreML is built into the OS  No need to compile, link, or ship binaries of ML libraries with the app. ",295 - ONNX  open format for machine learning models,2023-01-04T08:00:00+00:00
1286000,"Tips and Tricks 44:

Code from this video can be found here:
https://github.com/bnsreenu/python_for_microscopists/blob/master/tips_tricks_44_underscores_in_python.ipynb

Learn about: Underscores in python

Single underscore ONLY: _
Commonly used for unused variables.

Single underscore after: abc_
Allows us to use reserved keywords as variables (e.g., id, def, class, etc.)

Single underscore before: _abc
Usually represents objects/variables that are used internally.

Double underscore before and after: __abc__
These are used under 'dunder' class methods and dunder literally stands for Double Underscore.
We are familiar with our constructor of class: __init__ which creates an instance of a class.

Double underscore before: __abc
Used for name mangling - process that overwrites identifiers in a class to avoid conflicts of names between the current class and its subclasses.
In other words, __abc will have a different name in the class.",What are various underscores used in python?,2022-10-22T07:00:01+00:00
2012000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Direct link: https://github.com/bnsreenu/python_for_microscopists/tree/master/301-Evaluating%20keras%20model%20using%20KFold%20cross%20validation

We will start with the normal way most of us approach the problem of binary classification using neural networks (deep learning). In this example, we will split our data set the normal way into train and test groups. 

Then, we will learn to divide data using K Fold splits.
We will iterate through each split to train and evaluate our model. 

Normally, we would use cross_val_score in sklearn to automatically evaluate
the model over all splits and report the cross validation score. But, that method is designed to handle traditional sklearn models such as SVM, RF, 
gradient boosting etc. - NOT deep learning models from TensorFlow or pytorch.

Therefore, in order to use cross_val_score, we will find a way to make our
keras model available to the function. This is done using the KerasClassifier
from tensorflow.keras.wrappers.scikit_learn

Note that the cross_val_score() function takes the dataset and cross-validation configuration and returns a list of scores calculated for each fold.


Wisconsin breast cancer example
Dataset link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data",301 - Evaluating keras model using KFold cross validation,2023-03-29T07:00:16+00:00
1150000,"298 What is k fold cross validation?

Cross validation (according to Wikipedia) is a resampling method that uses different portions of the data to test and train a model on different iterations.

In machine learning, cross validation is used to compare various models and its parameters.

K-fold is a specific data sampling method that splits data for training and testing used for cross validation. 

K refers to the number of groups the data gets split into. ",298 - What is k fold cross validation?,2023-02-15T08:00:07+00:00
1664000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Let us start by understanding the Binary classification using keras . This is the normal way most of us approach the problem of binary classification
using sklearn (SVM). In this example, we will split our data set the normal way into train and test groups. 

We will then learn to divide data using K Fold splits.
We will iterate through each split to train and evaluate our model. 

We will finally use the cross_val_score() function to perform the evaluation. 
It takes the dataset and cross-validation configuration and returns a list of 
scores calculated for each fold.

KFOLD is a model validation technique.

Cross-validation between multiple folds allows us to evaluate the model performance. 

KFold library in sklearn provides train/test indices to split data in train/test sets. Splits dataset into k consecutive folds (without shuffling by default).
Each fold is then used once as a validation while the k - 1 remaining folds 
form the training set.

Split method witin KFold generates indices to split data into training and test set. The split will divide the data into n_samples/n_splits groups. 
One group is used for testing and the remaining data used for training.
All combinations of n_splits-1 will be used for cross validation.  

Wisconsin breast cancer example
Dataset link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data",299 - Evaluating sklearn model using KFold cross validation in python,2023-03-01T08:00:02+00:00
1125000,"Tuning deep learning hyperparameters using Gridsearch

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/302-Tuning%20deep%20learning%20hyperparameters/302-Tuning%20deep%20learning%20hyperparameters%E2%80%8B.py

All other code:
https://github.com/bnsreenu/python_for_microscopists

The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter.

The GridSearchCV instance when fitting on a dataset, all the possible 
combinations of parameter values are evaluated, and the best combination is retained.

cv parameter can be defined for the cross-validation splitting strategy.

GridSearch is designed to work with models from sklearn. But, we can also use it to tune deep learning hyper parameters - at least for keras models. 

Wisconsin breast cancer example
Dataset link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data",302 - Tuning deep learning hyperparameters using GridSearchCV,2023-04-12T07:00:28+00:00
1129000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Picking the best model and corresponding hyperparameters
using cross validation inside a Gridsearch

The grid search provided by GridSearchCV exhaustively generates candidates 
from a grid of parameter values specified with the param_grid parameter
Example:
    param1 = {}
    param1['classifier__n_estimators'] = [10, 50, 100, 250]
    param1['classifier__max_depth'] = [5, 10, 20]
    param1['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:10}, {0:1,1:25}]
    param1['classifier'] = [RandomForestClassifier(random_state=42)]

The GridSearchCV instance when fitting on a dataset, all the possible 
combinations of parameter values are evaluated and the best combination is retained.

cv parameter can be defined for the cross-validation splitting strategy.

Wisconsin breast cancer example
Dataset link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data",300 - Picking the best model and corresponding hyperparameters using Gridsearch,2023-03-15T07:00:11+00:00
849000,"Tips and tricks 45 

Code from this video is available at: https://github.com/bnsreenu/python_for_microscopists/tree/master/Tips_Tricks_45_white-balance_using_python

The tutorial is about white balancing images using two different approaches:
    1. Gray-world algorithm
    2. White patch reference

Gray-world assumes that average pixel value is neutral gray (128) because of good distribution of colors. So, we can estimate pixel color by looking at the average color. 

White patch is about picking a patch from the image that is supposed to be white and using it as reference to rescale each channel in the image.",White balancing your pictures using python,2022-12-23T08:00:23+00:00
730000,"Reinhard color transfer 
Based on the paper: https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists/tree/master/303%20-%20Reinhard%20color%20transformation%E2%80%8B

All other code: 
https://github.com/bnsreenu/python_for_microscopists

This approach is suitable for stain normalization of pathology images where the 'look and feel' of all images can be normalized to a template image. This can be a good preprocessing step for machine learning and deep learning of pathology images.",303 - Reinhard color transformation,2023-06-07T07:00:11+00:00
1244000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/304%20-%20Augmentation%20of%20histology%20images%E2%80%8B

All other code:
https://github.com/bnsreenu/python_for_microscopists

Random Stain Normalization and Augmentation (RandStainNA) is a hybrid framework designed to fuse stain normalization (SN) and stain augmentation (SA) to generate more realistic stain variations. It incorporates randomness to stain normalization by automatically sorting out a random virtual template from pre-estimated stain style distributions. More specifically, from the perception of SNs viewpoint, stain styles visible to the deep neural network are enriched in the training stage. Meanwhile, from the perception from the SAs viewpoint, RandStainNA imposes a restriction on the distortion range and consequently, only a constrained practicable range is visible to CNN. 

https://github.com/yiqings/RandStainNA
https://arxiv.org/abs/2206.12694",304 - Augmentation of histology images to train stain-agnostic deep learning models,2023-06-21T07:00:15+00:00
1804000,"What is Cellpose algorithm for instance segmentation? and how to train your own Cellpose model?

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/305_What_is_Cellpose_algorithm_for_segmentation.ipynb

All other code: 
https://github.com/bnsreenu/python_for_microscopists

Useful links: 
https://cellpose.readthedocs.io/en/latest/
https://www.cellpose.org/
Data set used in this video can be downloaded from: https://www.kaggle.com/datasets/batuhanyil/electron-microscopy-particle-segmentation
Cellpose GitHub: Code: https://github.com/mouseland/cellpose

Original paper: https://www.biorxiv.org/content/10.1101/2020.02.02.931238v1.full.pdf
Cellpose 2.0 paper: https://www.nature.com/articles/s41592-022-01663-4

Cellpose is a generalist algorithm for cellular segmentation which can very precisely segment a wide range of image types out of-the-box and does not require model retraining or parameter adjustments. 

This video demonstrates the use of Cellpose in Google Colab on a custom data set.",305 - What is Cellpose algorithm for segmentation?,2023-07-26T07:00:22+00:00
1695000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/306%20-%20Content%20based%20image%20retrieval%E2%80%8B%20via%20feature%20extraction

In this video, we will create a content-based image retrieval system which is basically an image-based search. We achieve this task by storing features from images into a database that we will search to retrieve images. Features can be generated many ways. In this tutorial I will extract custom features using a few digital image filters. I will also show feature extraction using pre-trained VGG16 and ResNet50 networks on Imagenet database. 

You will learn about the importance of features along the way. 

The features from this query image are compared against features from the indexed database and a match score gets reported. The match is performed using the cosine distance method. 
https://en.wikipedia.org/wiki/Cosine_similarity

The top 3 matching image names are then printed on the screen.",306 - Content based image retrieval via feature extraction in python,2024-01-10T08:00:27+00:00
18000,"In the video, the outcome of two simulations using the genetic algorithm is displayed. In one simulation, the background was set to yellow, resembling the color of the African savannah. In the other simulation, the background was set to green, representing the color of a leaf. It becomes evident that as the generations progress, the evolutionary process reaches an equilibrium, where the antelope and caterpillar assume colors that match their respective backgrounds.

Be sure to keep an eye on my YouTube channel for some upcoming tutorials on this subject.",Camouflage simulation using the Genetic Algorithm,2023-02-02T19:01:29+00:00
1121000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/tips_tricks_46-Feature%20engineering%20vs%20feature%20learning

All other code:
https://github.com/bnsreenu/python_for_microscopists

Feature engineering refers to the process of selecting and designing relevant features from raw data to improve the performance of machine learning algorithms. It involves domain expertise and creativity to identify informative features that capture the underlying patterns in the data.

On the other hand, feature learning, also known as representation learning, is a technique that enables a machine learning model to automatically learn relevant features from raw data. It involves using neural networks to discover useful features that can be used for downstream tasks.

This video tutorial demonstrates that with enough knowledge, features can be engineered from images using handcrafted algorithms. However, the tutorial also shows that pre-trained networks such as VGG16, which were trained on large datasets, can automatically learn rich features from images with no prior knowledge. This illustrates the power of feature learning, where pre-trained models can be leveraged to extract informative features, making it a more efficient and effective method than feature engineering.

Related tutorials:
https://youtu.be/9GzfUzJeyi0
https://youtu.be/IuoEiemAuIY
https://youtu.be/5ct8Yqkiioo
https://youtu.be/vgdFovAZUzM",Feature engineering vs Feature Learning (tips tricks 46 ),2023-02-18T08:00:26+00:00
1013000,"Segment your images in python without training using Segment Anything Model (SAM) by Meta AI

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/tree/master/307%20-%20Segment%20your%20images%20in%20python%20without%20training

All other code: https://github.com/bnsreenu/python_for_microscopists

Useful links:
SAM demo page: https://segment-anything.com/demo
Blog page: https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/
Github: https://github.com/facebookresearch/segment-anything
Trained models: https://github.com/facebookresearch/segment-anything#model-checkpoints",307 - Segment your images in python without training using Segment Anything Model (SAM),2023-04-08T07:00:16+00:00
1596000,"Video 308: An introduction to language models, With a special focus on GPT

Language models are the foundation of many natural language processing (NLP) tasks.
They help machines understand and generate human language by predicting the likelihood of a sequence of words.
Over the years, advances in algorithms and computational power have driven progress in language modeling, enabling breakthroughs in NLP applications.

LSTM networks, introduced by Hochreiter and Schmidhuber in 1997, are a type of recurrent neural network (RNN) designed to handle long-term dependencies.
Traditional RNNs struggled with the vanishing gradient problem, making it difficult to capture context over longer sequences.
LSTMs addressed this issue with their unique gating mechanisms, which enabled them to retain information for more extended periods, paving the way for improved language modeling.
(Watch my video on this topic: https://youtu.be/zyCpntcVKSo)

The transformer architecture, introduced by Vaswani et al. in 2017, revolutionized NLP by utilizing self-attention mechanisms and parallel processing.

The Transformer model is based on the encoder-decoder architecture.
Encoder: Processes input sequence, generating contextualized representations of each token.
Decoder: Generates output sequence step by step, using encoder's output as context for informed predictions.

Self-attention allows the model to weigh the importance of different words in a sequence, enabling better context understanding.
Parallel processing overcomes the sequential processing limitations of RNNs, leading to faster training and improved performance on various NLP tasks.

BERT (Bidirectional Encoder Representations from Transformers) is well-suited for tasks that require understanding the context of both preceding and following tokens. Some good applications for BERT include:

Sentiment analysis
Named entity recognition
Question-answering systems
Text classification
Semantic role labeling

GPT (Generative Pre-trained Transformer) is primarily designed for text generation tasks, and it is a unidirectional model, meaning it processes text in a left-to-right fashion. Some good applications for GPT include:

Text completion
Machine translation
Summarization
Chatbots and conversational AI
Creative writing assistance

GPT, developed by OpenAI, is a transformer-based model with a focus on decoding and adaptability.

GPT models, particularly GPT-3, have demonstrated impressive capabilities in zero-shot and few-shot learning, where they can learn new tasks with minimal or no examples.

While GPT excels at text generation and learning from examples without fine-tuning, it is important to consider its limitations, such as the size and computational requirements of the model, when evaluating its practical applications.",308 - An introduction to language models with focus on GPT,2023-04-19T07:00:03+00:00
1368000,"309 - Training your own Chatbot using GPT

Code from this video is available on my GitHub page: 
https://github.com/bnsreenu/python_for_microscopists 

Direct link to the code: https://github.com/bnsreenu/python_for_microscopists/blob/master/309_Training_your_own_Chatbot_using_GPT%E2%80%8B.ipynb


This video tutorial explains how to implement a chatbot using the GPT-2 language model from Hugging Face's Transformers library.
https://huggingface.co/transformers/v2.2.0/pretrained_models.html",309 - Training your own Chatbot using GPT,2023-04-26T07:00:04+00:00
1936000,"310 - Understanding sub word tokenization used for NLP

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/310-Understanding%20sub-word%20tokenization%20used%20for%20NLP

All other code:
https://github.com/bnsreenu/python_for_microscopists

Subword tokenization algorithm's philosophy is
- frequently used words should not be split into smaller sub-words 
- rare words should be divided into meaningful sub-words. 

Example: DigitalSreeni is not a real word and a rare word (unless I get super famous). It may be divided as:
Digital (common word)
Sr
E
E
Ni (Common sub-word  Nice, Nickel, Nimble, etc.)

Advantages of sub-word tokenization:
Not very large vocabulary sizes while maintaining the ability to provide context-independent representations.
Handle rare and out-of-vocabulary words by breaking them into known sub-word units.

Byte Pair Encoding (BPE) reference:
https://arxiv.org/abs/1508.07909

BPE Starts with pre-tokenizer that splits the training data into words. Pre-tokenization can be just space tokenization where words separated by space are represented by individual tokens  (e.g., GPT-2). 

Using pre-tokenized tokens, it learns merge rules to form a new word (token) from two tokens of the base vocabulary. 

This process is iterated until the vocabulary has attained the desired vocabulary size, set by the user (hyperparameter). 

Both ByteLevelBPETokenizer and SentencePieceBPETokenizer are tokenizers used for subword tokenization, but they use different algorithms to learn the vocabulary and perform tokenization.

ByteLevelBPETokenizer is a tokenizer from the Hugging Face tokenizers library that learns byte-level BPE (Byte Pair Encoding) subwords. It starts by splitting each input text into bytes, and then learns a vocabulary of byte-level subwords.
using the BPE algorithm. This tokenizer is particularly useful for languages 
with non-Latin scripts, where a character-level tokenizer may not work well.

On the other hand, SentencePieceBPETokenizer is a tokenizer from the SentencePiece library that learns subwords using a unigram language model. It first tokenizes the input text into sentences, and then trains a unigram language model on the resulting sentence corpus to learn a vocabulary of subwords. This tokenizer can handle a wide range of languages and text types, and can learn both character-level 
and word-level subwords.

In terms of usage, both tokenizers are initialized and trained in a similar way.",310 - Understanding sub word tokenization used for NLP,2023-05-03T07:00:24+00:00
891000,"311 - Fine tuning GPT2 using custom documents

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/311_fine_tuning_GPT2.ipynb

All other code:
https://github.com/bnsreenu/python_for_microscopists

This tutorial explains the simple process of fine-tuning GPT2 using your own documents. It also demonstrates the advantages of structuring your training data as Q & A rather than long text.",311 - Fine tuning GPT2 using custom documents,2023-05-10T07:00:17+00:00
787000,"Genetic Algorithms (GA) are a type of evolutionary algorithm inspired by the process of natural selection in biological evolution.

They can be used to solve optimization problems, including finding the optimal values for various parameters.

GAs involve creating a population of candidate solutions, which are then evolved through the application of selection, crossover, and mutation operators.

The fittest individuals from each generation are selected to create the next generation, creating a process of natural selection over multiple generations.

GAs involve the application of selection, crossover, and mutation operators to create the next generation of individuals.

The selection operator involves selecting the fittest individuals from the current generation to create the next generation.

The crossover operator involves combining the genetic material of two individuals to create a new individual.

The mutation operator involves randomly changing the genetic material of an individual to introduce new variations in the population.

GAs have been successfully applied to various optimization problems, including finding the optimal values for various parameters in machine learning models.

They have also been used for feature selection, where the algorithm selects the best set of features for a particular task.

Other applications of GAs include optimization of engineering designs, scheduling problems, and financial forecasting.


",312 - What are genetic algorithms?,2024-01-24T08:00:23+00:00
1088000,"Genetic algorithms simulate evolution and natural selection to produce adaptive traits.

Camouflage is a common adaptation that helps animals blend into their environment and avoid predators.

Insects can be represented as a string of genes that code for specific visual features such as color, shape, and texture.

A fitness score is used to evaluate an individual's ability to blend in with their environment.

Individuals with higher fitness scores are selected for reproduction, passing on their genes to the next generation.

Over time, the population evolves and becomes better adapted to their environment, producing offspring that are increasingly difficult to detect by predators.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/313_GeneticAlgorithm_Camouflage.ipynb",313 - Using genetic algorithms to simulate evolution,2024-02-07T08:00:35+00:00
1219000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/314_How_to_code_the_genetic_algorithm_in_python.ipynb

The genetic algorithm is a stochastic method for function optimization inspired by the process of natural evolution - select parents to create children using the crossover and mutation processes.

The code is an implementation of the genetic algorithm for optimization. The algorithm is used to find the minimum value of a two-dimensional inverted Gaussian function centered at (7,9). The algorithm consists of the following steps:

Initialize a population of binary bitstrings with random values.
Decode the binary bitstrings into numerical values, and evaluate the fitness (the objective function) for each individual in the population.
Select the best individuals from the population using tournament selection based on the fitness scores.
Create new offsprings from the selected individuals using the crossover operation.
Apply the mutation operation on the offsprings to maintain diversity in the population.
Repeat steps 2 to 5 until a stopping criterion is met.
The implementation includes functions for decoding, selection, crossover, and mutation.",314 - How to code the genetic algorithm in python?,2024-02-21T08:00:12+00:00
1416000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/315_Optimization_using_Genetic_Algorithm_Heart_disease.ipynb

The genetic algorithm is a stochastic method for function optimization inspired by the process of natural evolution - select parents to create children using the crossover and mutation processes.

Coding it in python: The algorithm consists of the following key steps:

Initialize a population of binary bitstrings with random values.

Decode the binary bitstrings into numerical values and evaluate the fitness (the objective function) for each individual in the population.

Select the best individuals from the population using tournament selection based on the fitness scores.

Create new offsprings from the selected individuals using the crossover operation.

Apply the mutation operation on the offsprings to maintain diversity in the population.

Repeat steps 2 to 5 until a stopping criterion is met.

",315 - Optimization using Genetic Algorithm,2024-03-06T08:00:20+00:00
989000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/316_Optimizing_Steel_Strength_using_Metaheuristic_algo.ipynb

In this example, we will work with the steel alloy data set.
Download from here: https://www.kaggle.com/datasets/fuarresvij/steel-test-data


The data set contains the elemental composition of different alloys and their respective yield and tensile strengths. 

A machine learning model can be trained on this data, allowing us to predict the strength of an alloy based on its chemical composition. 

But, for this exercise, let us try to find the optimized alloy composition with the best yield strength.

Let us explore metaheuristic approaches, especially the genetic algorithm and the differential evolution algorithm.

Note: Differential evolution (DE) is quite similar to the genetic algorithm (GA) with a few differences. DE relies on the distance and directional information through unit vectors for reproduction. Also, in DE, the crossover is applied after mutation unlike GA. In addition, the mutation operator is not created from a probability distribution, but from the creation of the unit vector.","316  - Optimizing Steel Strength using Metaheuristic algorithms (e.g., Genetic)",2024-03-20T07:00:31+00:00
866000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/317_HyperParameter_Optimization_using_Genetic_algo.ipynb

In this example, we will use the same dataset (steel alloy strength) from the previous tutorial to fit and tune Random Forest Regressor.

The dataset can be downloaded from here: https://www.kaggle.com/datasets/fuarresvij/steel-test-data

The data set contains the elemental composition of different alloys and their respective yield and tensile strengths. 

A machine learning model can be trained on this data, allowing us to predict the strength of an alloy based on its chemical composition.",317 - HyperParameter Optimization using Genetic algorithms,2024-04-03T07:00:02+00:00
59000,"Tips and Tricks: 48
Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/tips_tricks_48_overlay_image_comparison.ipynb

All other code:
https://github.com/bnsreenu/python_for_microscopists

Link to leafmap: https://leafmap.org/",Overlaying images for easy comparison (in python),2023-05-09T07:00:17+00:00
2593000,"323 - How to train a chatbot on your own documents?
Using openAI and Langchain

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/323-Train%20a%20chatbot%20on%20your%20own%20documents

All other code:
https://github.com/bnsreenu/python_for_microscopists",323 - How to train a chatbot on your own documents?,2023-05-17T07:00:10+00:00
1043000,"324 - Chat-based data analysis using openAI and pandasAI

A very short introduction to pandasAI
Dataset from: https://www.kaggle.com/datasets/vivovinco/nba-player-stats

Link to  code: https://github.com/bnsreenu/python_for_microscopists/tree/master/324-Chat-based%20data%20analysis%E2%80%8B-pandasAI

pandasAI: https://github.com/gventuri/pandas-ai",324 - Chat-based data analysis using openAI and pandasAI,2023-05-24T07:00:09+00:00
819000,"Metaheuristic algorithms are optimization techniques that use iterative search strategies to explore the solution space and find optimal or near-optimal solutions.

They do not guarantee finding the global optimum, but instead aim to efficiently explore the search space and converge to a good solution. 

These algorithms use heuristic rules to guide the search and modify the solutions over iterations to improve the fitness.

Genetic algorithms, simulated annealing, and particle swarm optimization are three examples of metaheuristic algorithms that have been successfully applied in various optimization problems.",318 - Introduction to Metaheuristic Algorithms,2024-04-17T07:00:14+00:00
857000,"319 - What is Simulated Annealing Optimization?

Code link: https://github.com/bnsreenu/python_for_microscopists/blob/master/319_what_is_simulated_annealing.ipynb

Simulated annealing is inspired by the physical process of annealing, in which a material is gradually cooled to form a crystalline structure with a minimum energy state. 

It works by iteratively adjusting the temperature of the system and accepting or rejecting candidate solutions based on a probabilistic function that depends on the current temperature and the change in the objective function value.

At high temperatures, the algorithm accepts solutions with a worse fitness to explore the search space and avoid local optima. As the temperature decreases, the algorithm becomes more selective and converges to a better solution. 

The cooling schedule determines the rate of temperature reduction and plays an important role in the algorithm's performance.

Simulated annealing is well-suited for finding the global optimum in a large search space with many local optima, such as in combinatorial optimization and network design problems.

Simulated annealing has been used for image registration, object tracking, and texture synthesis in microscopy images.",319 - What is Simulated Annealing Optimization?,2024-05-01T07:00:02+00:00
652000,"320 - Understanding Simulated Annealing using steel optimization

Code link: https://github.com/bnsreenu/python_for_microscopists/blob/master/320_Optimizing_Steel_Strength_using_simulated_annealing.ipynb

Finding the best alloy with maximum yield strength using simulated annealing algorithm

In this example, we will work with the steel alloy data set.
Download from here: https://www.kaggle.com/datasets/fuarresvij/steel-test-data

The data set contains the elemental composition of different alloys and their respective yield and tensile strengths. A machine learning model can be trained on this data, allowing us to predict the strength of an alloy based on its chemical composition. But, for this exercise, let us try to find the optimized alloy composition with the best yield strength.

Let us explore simulated annealing algorithm for optimization.",320 - Understanding Simulated Annealing using steel optimization,2024-05-15T07:00:02+00:00
583000,"Code link: https://github.com/bnsreenu/python_for_microscopists/blob/master/321_what_is_particle_swarm_optimization.ipynb

Particle Swarm Optimization

PSO is a swarm intelligence algorithm that is inspired by the behavior of social organisms such as flocks of birds or schools of fish.

The algorithm creates a population of particles, each representing a candidate solution, that move through the search space based on their individual velocity and the collective influence of the best solutions found by the swarm.

The algorithm updates the particles' positions and velocities based on the fitness of the current solution and the local and global best solutions found so far. It aims to balance exploration and exploitation by encouraging particles to explore new regions of the search space while also following promising solutions.

PSO is suitable for solving nonlinear and dynamic optimization problems, such as in control systems, machine learning, and signal processing.

PSO has been used for feature selection, image segmentation, and classification in microscopy images. For example, it has been used to optimize the parameters of texture descriptors for image segmentation, and to select the most discriminative features for cell classification.",321 - What is Particle Swarm Optimization PSO?,2024-05-29T07:00:11+00:00
875000,"Code link: https://github.com/bnsreenu/python_for_microscopists/blob/master/322_Optimizing_Steel_Strength_using_PSO.ipynb

Finding the best alloy with maximum yield strength using Particle Swarm Optimization

In this example, we will work with the steel alloy data set.
Download from here: https://www.kaggle.com/datasets/fuarresvij/steel-test-data

The data set contains the elemental composition of different alloys and their respective yield and tensile strengths. A machine learning model can be trained on this data, allowing us to predict the strength of an alloy based on its chemical composition. But, for this exercise, let us try to find the optimized alloy composition with the best yield strength.

Let us explore PSO:

Particle Swarm Optimization

PSO is a swarm intelligence algorithm that is inspired by the behavior of social organisms such as flocks of birds or schools of fish.

The algorithm creates a population of particles, each representing a candidate solution, that move through the search space based on their individual velocity and the collective influence of the best solutions found by the swarm.

The algorithm updates the particles' positions and velocities based on the fitness of the current solution and the local and global best solutions found so far. It aims to balance exploration and exploitation by encouraging particles to explore new regions of the search space while also following promising solutions.

PSO is suitable for solving nonlinear and dynamic optimization problems, such as in control systems, machine learning, and signal processing.

PSO has been used for feature selection, image segmentation, and classification in microscopy images. For example, it has been used to optimize the parameters of texture descriptors for image segmentation, and to select the most discriminative features for cell classification.",322 - PSO Using steel optimization,2024-06-12T07:00:19+00:00
4170000,"325: Transcriptomics Unveiled  An In-Depth Exploration of Single Cell RNASeq Analysis using python

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists/blob/master/325_Transcriptomics_Unveiled.ipynb

Dataset from: https://cell2location.cog.sanger.ac.uk/tutorial/mouse_brain_visium_wo_cloupe_data.zip

Useful resources: 
mRNA-Seq whole-transcriptome analysis of a single cell: https://www.nature.com/articles/nmeth.1315

A practical guide to single-cell RNA-sequencing for biomedical research and clinical applications: https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-017-0467-4

An introduction to spatial transcriptomics for biomedical research: https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-022-01075-1

Spatially resolved, highly multiplexed RNA profiling in single cells: https://www.science.org/doi/10.1126/science.aaa6090

Comparative analysis of MERFISH spatial transcriptomics with bulk and single-cell RNA sequencing: https://www.biorxiv.org/content/10.1101/2022.03.04.483068v1

Spatial omics technologies at multimodal and single cell/subcellular level: https://pubmed.ncbi.nlm.nih.gov/36514162/

Description:
The DNA sequence is the same in almost all cells of a given type of organism (Genome). DNA sequence of an organisms genome contains the instructions necessary for development, functioning and maintenance of that organism. For a specific organism (e.g., Mouse), the DNA sequence is generally consistent across different cells within that organism. Every cell in the mouse's body contains the same set of genes with the same DNA sequence. However, cells may have specific (combination of) genes turned on or off, making them perform their specific function  gene expression or gene regulation. Different cell types have specific gene expression profiles (e.g., genes involved in muscle contraction will be turned on in muscle cells and not in skin cells). Cellular diversity and cell-specific function is best assessed not at the DNA level, but at the protein level. However, there are no commercially available methods for quantifying the thousands of proteins within individual cells of our bodies. In 2009 the first description came for entire mRNA (~20,000 genes) from a single cell, known as the transcriptome  which opened doors to transcriptomics. 

Why study RNA?
Studying RNA can provide insights into which genes are turned on (expressed) or off (not expressed) in a given cell as:
- RNA molecules are synthesized from DNA templates. 
- analyzing RNA in a cell can provide information on which genes are actively being transcribed, hence expressed. 

RNA Sequencing allows researchers to determine the identity and abundance of different RNA molecules, including messenger RNA (mRNA) transcripts

By comparing the RNA-seq data across different samples or conditions, researchers can identify which genes are upregulated (turned on) or downregulated (turned off) under specific circumstances.

What is scRNA-seq?
scRNA-seq permits comparison of the transcriptomes of individual cells to assess transcriptional similarities and differences within a population of cells and to identify rare cell populations that would otherwise go undetected in analyses of pooled cells. It can also help in tracing lineage and developmental relationships between heterogeneous, yet related, cellular states in scenarios such as embryonal development, cancer.

Why spatial transcriptomics? 
Sc-RNA Seq is good but comes with a requirement of liberating viable cells from whole tissue without inducing stress, cell death, and/or cell aggregation. But in spatial transcriptomics methods, spatial information is preserved by studying intact tissue.

Data processing:
For all techniques, including Visium, Slide-seq, SeqFISH, MERFISH, and Drop-seq, the end result is a table that represents the gene expression profiles of individual cells.

The table typically consists of rows representing individual cells or spatial locations within the tissue and columns representing genes. The values in the table correspond to the gene expression intensities or counts for each cell or location.

Downstream analysis includes, quality control, dimensionality reduction, clustering, differential expression analysis, cell type identification, spatial analysis, and visualization. 

These analyses help extract biological insights and understand the cellular composition, heterogeneity, and spatial organization within the tissue.",325: Transcriptomics Unveiled  An In-Depth Exploration of Single Cell RNASeq Analysis using python,2023-05-27T07:00:09+00:00
2604000,"326 - Cell type annotation for single-cell RNA-seq data

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/326_Cell_type_annotation_for_single_cell_RNA_seq_data%E2%80%8B.ipynb

Previous video: Transcriptomics Unveiled  An In-Depth Exploration of Single Cell RNASeq Analysis using python: https://youtu.be/IPePGXrSZHE

GitHub link for the scsa library: https://github.com/bioinfo-ibms-pumc/SCSA

Reference paper: Cao Y, Wang X and Peng G (2020) SCSA: A Cell Type Annotation Tool for Single-Cell RNA-seq Data. Front. Genet. 11:490. doi: https://doi.org/10.3389/fgene.2020.00490

https://www.frontiersin.org/articles/10.3389/fgene.2020.00490/full

Description: 

scRNA-seq permits comparison of the transcriptomes of individual cells that helps to assess transcriptional similarities and differences within a population of cells. It also helps in identifying rare cell populations that would otherwise go undetected in analyses of pooled cells. There are many techniques for scRNA-seq
including Visium, Slide-seq, SeqFISH, MERFISH, and Drop-seq. For all these techniques, the end result is a table that represents the gene expression profiles of individual cells.

The table typically consists of rows representing individual cells or spatial locations within the tissue and columns representing genes. The values in the table correspond to the gene expression intensities or counts for each cell or location. Downstream analysis includes, quality control, dimensionality reduction, clustering, differential expression analysis, cell type identification, spatial analysis, and visualization. 

This video explains the process of cell type identification using the scsa library in python. Cell type annotation is the process of assigning or identifying the specific cell types or cell identities present in a biological sample, based on gene expression patterns. 

The SCSA library allows for accurate cell type annotation by comparing scRNA-seq data to reference cell type profiles. It calculates specificity scores for each cell type, measuring the likelihood of a cell belonging to a specific cell type based on its gene expression profile. The library includes pre-built reference databases for various organisms, enabling cell type annotation in different biological contexts. Users can also create custom reference databases tailored to their specific experimental systems or incorporate external reference datasets.



",326 - Cell type annotation for single cell RNA seq data,2023-06-03T07:00:09+00:00
896000,"Interpolation for resizing 3D volumetric data (Tips and Tricks 50)

The video explains the process of interpolation on an input 3D image (array) to create a new image with adjusted pixel size and slice thickness by using RegularGridInterpolator from SciPy. 

https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.RegularGridInterpolator.html

The code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_and_Tricks_50_interpolate_images_in_a_stack.ipynb

RegularGridInterpolator takes a 3D grid of values with certain intervals, adjusts the grid dimensions and fills in the values using interpolation to create a new 3D image with the desired pixel size and slice thickness.

FIB-SEM and volumetric EM data are typically acquired with specific pixel sizes and slice thicknesses where slice thickness usually containing lower resolution (thicker slices) compared to x/y pixel dimensions. Therefore, it may be necessary to change these parameters to match specific requirements, such as downstream analysis methods requiring isometric voxels. The code allows for adjusting the pixel size and slice thickness, ensuring the resulting data is consistent with the desired parameters.

Dataset from: https://paperswithcode.com/dataset/3d-platelet-em",Interpolation for resizing 3D volumetric data (Tips and Tricks 50),2023-06-16T07:00:07+00:00
746000,"327 - An introduction to Single Molecule Fluorescence In Situ Hybridization (smFISH)

smFISH uses a set of short, fluorescently labeled oligonucleotide probes that bind to specific target RNA molecules. Each probe is designed to hybridize to a specific sequence within the target RNA. The probes are designed to be complementary to the target RNA, allowing for specific and precise binding. Upon hybridization, the fluorescent probes generate a signal that can be visualized using fluorescence microscopy.

Why smFISH for studying gene expression?

High sensitivity: smFISH enables the detection of individual RNA molecules, providing high sensitivity and resolution. 
Single-cell resolution: It allows the examination of gene expression patterns at the single-cell level, providing insights into cellular heterogeneity.
Quantitative analysis: smFISH enables the quantification of gene expression levels in individual cells or tissues. 
Spatial information: It provides spatial localization of RNA molecules within cells, allowing for the investigation of subcellular distribution patterns.

smFISH experimental workflow  at a high level
Sample preparation: Cells or tissues are fixed, permeabilized, and then hybridized with fluorescently labeled probes.
Imaging: The samples are imaged using fluorescence microscopy, capturing the signals emitted by the labeled probes.
Image analysis: The acquired images are processed and analyzed to extract quantitative data on gene expression patterns.

Analyzing smFISH data
Specific analysis approach depends on the research question and the desired insights. However, a general workflow typically involves a few key steps:

Spot and Cluster detection: Detect individual spots or clusters corresponding to the bound fluorescent probes.
Cellular and nuclear segmentation: crucial for assigning spots to specific cellular compartments and extracting cell-specific information.
Statistical analysis: such as average distance of mRNA spots from the cell boundary, proportion of mRNA spots within the nucleus versus the cytoplasm. Note that multiplex analysis involves additional steps such as colocalization
Visualization and interpretation: heatmaps or scatter plots to interpret the analyzed data effectively.",327 - An introduction to Single Molecule Fluorescence In Situ Hybridization (smFISH),2023-06-28T07:00:34+00:00
2989000,"328 - smFISH Analysis using Big FISH library in python

Code from this video is available at:
1. https://github.com/bnsreenu/python_for_microscopists/blob/master/328a_smFISH_analysis_using_Big_FISH_singleplex.ipynb

2. https://github.com/bnsreenu/python_for_microscopists/blob/master/328b_smFISH_analysis_using_Big_FISH_multiplex.ipynb

This video tutorial is a walkthrough of smFISH (single-molecule fluorescence in situ hybridization) analysis using the Big-FISH library. The code demonstrates various steps involved in the analysis, including image reading, normalization and filtering, spot detection, segmentation of nuclei and cells, extraction of cell-level results, and computation of features for each cell - in a singleplex (and multiplex) dataset.

By analyzing the extracted features and the spatial distribution of spots within cells, researchers can gain insights into various aspects of cellular processes, including gene expression, RNA localization, and spatial organization. These insights can contribute to understanding the functional organization of cells and uncovering potential relationships between gene expression patterns and cellular phenotypes.

The ""spots"" in smFISH analysis refer to individual mRNA molecules that have been labeled with fluorescent probes and can be visualized as discrete signals in the images. Each spot represents the presence of a specific mRNA molecule within the cell.

The brightness of a spot generally corresponds to the abundance or level of the mRNA molecule it represents. Bright spots indicate a higher concentration of the mRNA molecule, suggesting higher expression levels of the corresponding gene. Conversely, dimmer spots may indicate lower expression levels.

""Clusters"" refer to groups of spots that are in close proximity to each other. Clusters can arise due to various reasons, such as multiple mRNA molecules originating from the same gene or co-localization of mRNA molecules from different genes. The presence of clusters may indicate co-regulation or co-localization of specific mRNA molecules within the cell.

Spots that are classified as ""inside"" the nucleus are localized within the nuclear boundary, indicating that the corresponding mRNA molecules are likely involved in nuclear processes, such as transcription, splicing, or RNA processing. On the other hand, spots classified as ""outside"" the nucleus are located in the cytoplasm, suggesting that the corresponding mRNA molecules have been transported out of the nucleus and are involved in cytoplasmic processes, such as translation.

Analyzing the distribution of spots inside and outside the nucleus can provide insights into gene expression regulation and mRNA localization. For example, certain genes may exhibit preferential nuclear localization, indicating their involvement in nuclear processes. On the other hand, cytoplasmic localization may be associated with mRNA molecules that are actively being translated or are involved in cytoplasmic functions.

Researchers often work with fluorescence in situ hybridization (FISH) images that involve multiple channels containing spots from multiple RNA molecules. FISH techniques can be designed to target specific RNA molecules of interest using fluorescently labeled probes. Each RNA molecule can be labeled with a different fluorophore, allowing researchers to distinguish and visualize multiple RNA species simultaneously.

There are multiple data sets to play with but for this exercise we will be using two datasets. 

1. The first one (singleplex) involves example images provided via the big-fish library.

Running this line: stack.check_input_data(path_input, input_segmentation=True) will place four images in your directory of choice.
experiment_1_dapi_fov_1.tif experiment_1_smfish_fov_1.tif example_nuc_full.tif example_cell_full.tif

experiment_1_dapi_fov_1.tif
experiment_1_smfish_fov_1.tif
example_nuc_full.tif
example_cell_full.tif
We will be primarily working with the first two images where dapi is used to segment nuclei and smfish image gets used to segment cells and spot detection. As mntioned above, in real situations you may be working with multichannel images representing signals from multiple RNA molecules.

2. The second one (multiplex) can be downloaded from: https://github.com/LieberInstitute/dotdotdot/blob/master/images/Mouse1.czi

The z-stack consists of 14 z slices, each 201 x 201 pixels and 4 channels: ""Cy5"", ""DsRed"" (red), ""EGFP"" (green), and ""DAPI"" (blue) in that order. Scaling is 0.31 um x 0.31 um x 0.40 um. The data was collected on ZEISS LSM700, AxioObserver microscope with plan Apochromat objective at 40x/1.3 oil DIC.

Other datasets of use:
This is a good reference paper that mentions a few datasets: https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-022-01669-y/MediaObjects/41592_2022_1669_MOESM1_ESM.pdf",328 - smFISH Analysis using Big FISH library in python,2023-07-12T07:00:17+00:00
1435000,"This video provides an introduction to Detectron2 in python using pre-trained models for instance and panoptic segmentation. 

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists/blob/master/329_Detectron2_intro.ipynb

All other code:
https://github.com/bnsreenu/python_for_microscopists

Detectron2 repo: https://github.com/facebookresearch/detectron2

What is Detectron2?
An open-source object detection and segmentation framework developed by Facebook AI Research.

Built on top of PyTorch and provides a unified API for a variety of tasks, including object detection, instance segmentation, and panoptic segmentation.

Designed to be flexible and easy-to-use, it puts a focus on enabling rapid research.

It includes high-quality implementations of state-of-the-art algorithms like Mask R-CNN, RetinaNet, and DensePose.

It includes a Model Zoo with models for object detection, instance segmentation, and more. ",329 - What is Detectron2? An introduction.,2023-08-09T07:00:08+00:00
3022000,"This video tutorial explains the process of fine tuning Detectron2 for instance segmentation using custom data. It walks you through the entire process, from annotating your data, to training a model, to segmenting images, to measuring object morphological parameters, to exporting individual masks (results) as images for further processing. 

Code generated in the video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists/blob/master/330_Detectron2_Instance_3D_EM_Platelet.ipynb

All other code:
https://github.com/bnsreenu/python_for_microscopists

Detectron2 repo: https://github.com/facebookresearch/detectron2

Annotations were done using Makesense: https://www.makesense.ai/

Dataset from: https://leapmanlab.github.io/dense-cell/
Direct link to the dataset: https://www.dropbox.com/s/68yclbraqq1diza/platelet_data_1219.zip

Data courtesy of:
Guay, M.D., Emam, Z.A.S., Anderson, A.B. et al. 
Dense cellular segmentation for EM using 2D3D neural network ensembles. Sci Rep 11, 2561 (2021). 

Data annotated for 4 classes:
1: Cell
2: Mitochondria
3: Alpha granule
4: Canalicular vessel",330 - Fine tuning Detectron2 for instance segmentation using custom data,2023-08-23T07:00:14+00:00
2647000,"This tutorial walks you through the process of fine-tuning a Segment Anything Model (SAM) using custom data.

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb

What is SAM?
SAM is an image segmentation model developed by Meta AI. It was trained over 11 billion segmentation masks from millions of images. It is designed to take human prompts, in the form of points, bounding boxes or even a text prompt describing what should be segmented.

What are the key features of SAM?
Zero-shot generalization: SAM can be used to segment objects that it has never seen before, without the need for additional training.

Flexible prompting: SAM can be prompted with a variety of input, including points, boxes, and text descriptions.

Real-time mask computation: SAM can generate masks for objects in real time. This makes SAM ideal for applications where it is necessary to segment objects quickly, such as autonomous driving and robotics.

Ambiguity awareness: SAM is aware of the ambiguity of objects in images. This means that SAM can generate masks for objects even when they are partially occluded or overlapping with other objects.

How does SAM work?
SAM works by first encoding the image into a high-dimensional vector representation. The prompt is encoded into a separate vector representation. The two vector representations are then combined and passed to a mask decoder, which outputs a mask for the object specified by the prompt.

The image encoder is a vision transformer (ViT-H) model, which is a large language model that has been pre-trained on a massive dataset of images. The prompt encoder is a simple text encoder that converts the input prompt into a vector representation. The mask decoder is a lightweight transformer model that predicts the object mask from the image and prompt embeddings.

SAM paper: https://arxiv.org/pdf/2304.02643.pdf

Link to the dataset used in this demonstration: https://www.epfl.ch/labs/cvlab/data/data-em/
Courtesy: EPFL

This code has been heavily adapted from this notebook but modified to work with a truly custom dataset where we have a bunch of images and binary masks. https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb",331 - Fine-tune Segment Anything Model (SAM) using custom data,2023-09-06T07:00:03+00:00
1596000,"This video tutorial walks you through the process of converting binary or labeled masks, often associated with scientific images, into coco style json annotations using python code. It also walks you through the process of converting the coco json annotations to YOLO v8 compatible annotations. As a bonus, I share code to display YOLO v8 annotations using python code. 

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/tree/master/332%20-%20All%20about%20image%20annotations%E2%80%8B


Part 1: Convert binary annotations to COCO JSON

For each binary mask, the code extracts contours using OpenCV. 
These contours represent the boundaries of objects within the images. This is a key step in converting binary masks to polygon-like annotations. 

Then, convert the contours into annotations, including bounding boxes, area, and segmentation information. Each annotation is associated with an image ID, category ID, and other properties required by the COCO format.

The code also creates an images section containing metadata about the images, such as their filenames, widths, and heights. In my example, I have used exactly the same file names for all images and masks so that a given mask can be easily mapped to the image. 

All the annotations, images, and categories are assembled into a dictionary that follows the COCO JSON format. This includes sections for ""info,"" ""licenses,"" ""images,"" ""categories,"" and ""annotations.""

Finally, the assembled COCO JSON data is saved to a file, making it ready to be used with tools and frameworks that support the COCO data format.

Part 2: Converting COCO JSON annotation to YOLO v8 

It reads coco style json annotations supplied as a single json file and also 
images as input. 

Here are the key steps in the code:

1. Convert Images to YOLO Format: The convert_to_yolo function takes paths for input images and annotations (in JSON format), and directories to store the output images and labels. It then performs the following operations:
- Reads the input JSON file containing annotations.
- Copies all PNG images from the input directory to the output directory.
- Normalizes the polygon segmentation data related to each image and writes them to text files, mapping them to the appropriate category (e.g., Alpha, Cells, Mito, Vessels).
- The resulting text files contain information about the object category and the normalized coordinates of the polygons that describe the objects.

2. Create YAML Configuration File: The create_yaml function takes paths to the input JSON file containing categories, training, validation, and optional test paths. It then:
- Extracts the category names and the number of classes.
- Constructs a dictionary containing information about class names, the number of classes, and paths to the training, validation, and test datasets.
- Writes this dictionary to a YAML file, which can be used as a configuration file for training a model (e.g., a YOLO model).",332 - All about image annotations,2023-09-20T07:00:09+00:00
2127000,"This video walks you through the process of training a custom YOLO v8 model using your own data. 

Code generated in this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/334_training_YOLO_V8_EM_platelets_converted_labels.ipynb

In this exercise, I use a public dataset that shows multiple classes for segmentation. This is the same dataset from tutorial 330 (Detectron2) - https://youtu.be/cEgF0YknpZw

Dataset from: https://leapmanlab.github.io/dense-cell/
Direct link to the dataset: https://www.dropbox.com/s/68yclbraqq1diza/platelet_data_1219.zip

Data courtesy of: Guay, M.D., Emam, Z.A.S., Anderson, A.B. et al. Dense cellular segmentation for EM using 2D3D neural network ensembles. Sci Rep 11, 2561 (2021). 

To prepare this dataset for YOLO, the binary masks were converted to the YOLO format. Please follow this tutorial to learn about this process.
(https://youtu.be/NYeJvxe5nYw)

If you already have annotations in COCO format JSON file, for example by annotating using makesense (https://www.makesense.ai/) then the annotations can be imported to Roboflow for conversion to YOLO format. Otherwise, if you are starting from scratch, just annotate datasets on Roboflow. (https://roboflow.com/). You just need to upload your images along with the JSON file and Roboflow will convert them to any other format, in our case YOLO v8.

For information about YOLO models:
https://docs.ultralytics.com/models/yolov8/#key-features",334 - Training custom instance segmentation model using YOLO v8,2023-11-01T07:00:33+00:00
1124000,"This video explains the basics of YOLO v8 and walks you through a few lines of code to help explore YOLO v8 for object detection and instance segmentation using pre-trained weights. More information about YOLO v8 can be found here.
https://docs.ultralytics.com/models/yolov8/#key-features

Code generated in this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/333_Intro_to_YOLO_V8.ipynb",333 - An introduction to YOLO v8,2023-10-18T07:00:13+00:00
2751000,"Tips and Tricks 51 :  A Holistic View of Software Languages, Databases, and Frameworks

The aim for this video is to provide clarity, helping you understand when and why certain tools are chosen over others. The video walks you through the most common languages and frameworks used in various contexts including, interfacing with hardware, software for desktop, web, and Apps. It also provides an introduction to various tools for Big Data handling and processing. Finally, a walkthrough of learning plan to build a hypothetical product is included. 


","A Holistic View of Software Languages, Databases, and Frameworks",2023-09-13T07:00:30+00:00
1719000,"335 - Converting COCO JSON annotations to labeled masks

This video walks you through the process of converting COCO JSON annotations to labeled mask images. The four main parts of this video tutorial are: 
1. Downloading data
2. Opening the (large) JSON file in python to understand the data
3. Visualizing a few annotations on respective images to confirm the quality of annotations
4. Converting JSON annotations to labeled masks

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/tree/master/335%20-%20Converting%20COCO%20JSON%20annotations%20to%20labeled%20mask%20images

Dataset from: https://github.com/sartorius-research/LIVECell/tree/main
Note that the dataset comes with: Creative Commons Attribution - NonCommercial 4.0 International Public License
In summary, you are good to use it for research purposes but for commercial
use you need to investigate whether trained models using this data must also comply with this license - it probably does apply to any derivative work so please be mindful. 

You can directly download from the source github page. Links below.
Training json: http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json
Validation json: http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_val.json
Test json: http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test.json
Images: Download images.zip by following the link: http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/images.zip

If these links do not work, follow the instructions on their github page.",335 - Converting COCO JSON annotations to labeled mask images,2023-10-04T07:00:11+00:00
419000,"Tips and Tricks 52  : Simplifying code with defaultdict in python

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/tips_tricks_52_Hidden%20Gems%20-%20defaultdict/Tips%20Tricks%2052%20-Hidden%20Gems%20-%20defaultdict.py

Simplifying Code with defaultdict

You start with an empty notebook, and you want to add three types of fruits: 
    apples, bananas, and cherries.

To do this, you open the notebook and look for the page for ""apples."" 
But the notebook is empty, so there's no page for ""apples.""

You have to create a page for ""apples"" first and write down that you have 2 apples.

Next, you want to add bananas. You look for the page for ""bananas,"" 
but it's not there, so you create a new page for ""bananas"" and write down that you have 1 banana.

You repeat this process for cherries, creating a page for them and writing 
that you have 3 cherries.

Now, imagine someone asks you, ""How many dates do you have?"" You try to look for a ""dates"" page in your notebook, but it doesn't exist.

You have to be careful not to make mistakes and accidentally forget to create a page for any fruit or forget to update the count correctly. 
This manual checking and updating make your notebook longer and more error-prone.

The need for defaultdict:

You start with a magical notebook (a defaultdict) that automatically creates a page for any fruit you mention.

You confidently write down the counts of your fruits without worrying if the 
pages exist or not. For example, you just say, ""I have 2 apples,"" and the notebook creates an ""apples"" page for you if it doesn't already exist.

You add bananas and cherries the same way, without needing to manually check or create pages.

Now, when someone asks about dates, you confidently check the ""dates"" page in your notebook. If it doesn't exist, the notebook kindly tells you that you have 0 dates.

Your magical notebook simplifies your record-keeping. You don't have to write extra code to handle missing pages or worry about making mistakes, making your record-keeping shorter and more accurate.",Simplifying code with defaultdict in python,2023-09-27T07:00:06+00:00
785000,"Tips and Tricks 53:   Extracting a Targeted Subset from a COCO JSON

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/tree/master/tips_tricks_53_Extracting%20a%20Targeted%20Subset%20from%20a%20COCO%20JSON

This video tutorial is all about extracting a subset dataset from a mega-dataset with coco json annotations. The extracted subset can be used for rapid testing of your code or help you in targeting a specific subset category of data within the large dataset.",Extracting a Targeted Subset from a COCO JSON annotated dataset,2023-10-11T07:00:19+00:00
3428000,"This video tutorial is an entire project spanning from data download to training object detection models to analysis and plotting. It covers the following key tasks, with downloadable code for every task:



- Downloading data from Kaggle
- Cleaning up the data
- Converting masks to coco json and YOLOv8 annotations
- Visualizing annotations
- Training Detectron2 (Mask R-CNN) for object detection
- Training YOLOv8 for object detection


Code is available here: https://github.com/bnsreenu/python_for_microscopists/tree/master/336-Nuclei-Instance-Detectron2.0_YOLOv8_code


Dataset downloaded from: https://www.kaggle.com/datasets/ipateam/nuinsseg?resource=download
Dataset description: https://arxiv.org/abs/2308.01760


Summary of the dataset:
The NuInsSeg dataset contains more than 30k manually segmented nuclei from 31 human and mouse organs and 665 image patches extracted from H&E-stained whole slide images. We also provide ambiguous area masks for the entire dataset to show in which areas manual semantic/instance segmentation were impossible.


Human organs:


cerebellum, cerebrum (brain), colon (rectum), epiglottis, jejunum, kidney, liver, lung, melanoma, muscle, oesophagus, palatine tonsil, pancreas, peritoneum, placenta, salivary gland, spleen, stomach (cardia), stomach (pylorus), testis, tongue, umbilical cord, and urinary bladder


Mouse organs:


cerebellum, cerebrum, colon, epiglottis, lung, melanoma, muscle, peritoneum, stomach (cardia), stomach (pylorus), testis, umbilical cord, and urinary bladder)",336 - Nuclei segmentation and analysis using Detectron2 & YOLOv8,2023-11-15T08:00:14+00:00
1771000,"This tutorial builds upon the exercise covered in our previous tutorial (https://youtu.be/R-N-YXzvOmY), where we trained Detectron2 and YOLOv8 models for Nuclei detection. In this continuation, we will utilize these trained models for segmenting whole slide images with .svs extensions. Additionally, we will demonstrate the use of memory-mapping to efficiently handle large arrays.

Sample svs file used in the tutorial: https://github.com/camicroscope/Distro/blob/master/images/sample.svs
A few large svs images can be downloaded from here: https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/

Code from this tutorial is available here: (Just look for number 337 directory on my GitHub page: https://github.com/bnsreenu/python_for_microscopists/

Direct link here: https://github.com/bnsreenu/python_for_microscopists/tree/master/337%20-%20Whole%20Slide%20Image%20segmentation%20for%20nuclei%E2%80%8B%20using%20Detectron2%20and%20YOLOv8",337 - Whole Slide Image segmentation for nuclei using Detectron2 and YOLOv8,2023-11-29T08:00:13+00:00
2597000,"Tips Tricks 54 : Exploring Metadata in Scientific Images

What is Metadata?
It is the additional information about a file, providing details such as the 
creation date, author, location, pixel size, experimental settings, etc. 

Why is Metadata important? 
For your travel images, it is important, so you know when and where the image was taken. Please note that metadata is not necessary but useful when some information is needed at a future date. May be your grandkids want to take a picture in future at the same location on your 100th birthday!

For scientific images, it is important to ensure traceability, interoperability,
and reproducibility. 

Metadata provides a detailed history of the image, including acquisition parameters, equipment settings, and processing steps. This traceability is crucial for tracking the origin and evolution of the data, ensuring accountability and transparency in scientific research.

Standardized metadata formats enable different software and systems to understand and interpret information consistently. This promotes interoperability, allowing researchers to share and collaborate on data across various platforms and tools without losing critical details.

Metadata contains essential information about the experimental setup and conditions. Reproducing scientific experiments requires accurate knowledge of these factors. With comprehensive metadata, other researchers can precisely replicate experiments, verify results, and build upon existing work, contributing to the reliability and credibility of scientific findings.

Images come in many formats, let us explore metadata from a few most-common image formats including JPG, DICOM, TIFF, GEO-TIFF, OME-TIFF, and .CZI

Code from this video can be downloaded from here: https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_54_Exploring_metadata_in_scientific_images.py

Useful links: 
DICOM metadata tags: https://www.dicomlibrary.com/dicom/dicom-tags/
TIFF tags: https://www.loc.gov/preservation/digital/formats/content/tiff_tags.shtml
Open microscopy standard: https://docs.openmicroscopy.org/ome-model/6.0.1/",Exploring Metadata in Scientific Images,2023-12-06T08:00:05+00:00
3009000,"This video tutorial is all about perfecting your prompts for generative AI (e.g., chatGPT) by asking the right questions to get the best output on your marketing related activities. This includes Creating a Marketing Plan, Defining a Campaign for a New Product, Composing Social Media Posts for Your New Product, SEO Keywords Analysis, and even generating python code to perform user analysis. 

All documents from this video, including the presentation slides, are available here: https://github.com/bnsreenu/python_for_microscopists/tree/master/Prompt%20Engineering%20for%20Marketers%20-%20Tips%20Tricks%2055

If you are too lazy to watch the video, he is the essence:
Please make sure you incorporate these 5 elements in your prompts: Persona, Context, Action, Tone and the Output Format. 

Example of a good prompt below. Noice how it has all the 5 elements for a good prompt: 
You are a content expert at a startup, Webgenix, with a software product that generates web page designs using AI and no coding necessary. 

You are tasked to write a product caption no longer than 5 words. 

Write 10 product captions for Webgenix and provide a short explanation recommending each caption.

Please keep the tone professional. 

Please format the output in a table that can be copied to Excel. 

The video primarily uses chatGPT and Microsoft Copilot.",Generative AI and Prompt Engineering for Marketers,2023-12-15T08:00:37+00:00
1704000,"A Deep Dive into Why GPUs Outpace CPUs - A Hands-On Tutorial

FLOPS is commonly used to quantify the computational power of processors and other computing devices. It is an important metric for tasks that involve complex mathematical calculations, such as scientific simulations, artificial intelligence and machine learning algorithms.

FLOPS stands for ""Floating Point Operations Per Second"" which means the number of floating-point calculations a computer system can perform in one second. The higher the FLOPS value, the faster the computer or processor can perform floating-point calculations, indicating better computational performance.

In this tutorial, let us use FLOPS as a metric to evaluate the performance of CPU versus GPU. We will begin by employing the DAXPY (Double-precision A*X plus Y) operation, a commonly used operation in numerical computing. This operation involves multiplying a scalar (A) with a vector (X) and adding the result to another vector (Y). We will calculate FLOPS to perform the DAXPY operation using both the CPU and GPU, respectively.

The DAXPY operation is executed using NumPy operations (A * X + Y). NumPy can leverage optimized implementations, and the actual computation may occur in optimized C or Fortran libraries. Therefore, a more effective way to compare speeds is by conducting matrix multiplications using TensorFlow. The second part of our code is designed to accomplish precisely this task. We will perform matrix multiplications of various-sized matrices and explore how the true advantage of GPUs lies in working with large matrices (datasets in general).

In the second part of this tutorial, we will verify the GPU speed advantage over CPU for different matrix sizes. The relative efficiency of the GPU compared to the CPU can vary based on the computational demands of the specific task.

In order to make sure we start with a common base line for each matrix multiplication task, we will clear the default graph and release the GPU memory. We will also disable the eager execution in TensorFlow for the matrix multiplication task. Please note that eager execution is a mode that allows operations to be executed immediately as they are called, instead of requiring them to be explicitly executed within a session. Eager execution is enabled by default in TensorFlow 2.x. By disabling eager execution, operations are added to a computation graph, and the graph is executed within a session.

Finally, Forget FLOPS, it's all about the memory bandwidth!!!

Memory bandwidth is a measure of how quickly data can be transferred between the processor (CPU or GPU) and the memory.

High memory bandwidth is crucial for tasks that involve frequent access to large datasets (e.g., deep learning training)

Memory bandwidth becomes particularly important when dealing with large matrices, as transferring data between the processor and memory efficiently can significantly impact overall performance.

Code used in this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_56_CPU_vs_GPU_performance_test.ipynb

Original title: Why GPUs Outpace CPUs? (tips tricks 56)",Why GPUs Outpace CPUs?,2024-01-31T10:00:32+00:00
1251000,"Benford's Law, also known as the first-digit law, is a statistical phenomenon 
observed in many sets of numerical data. It states that in certain naturally 
occurring datasets, the leading digits (1, 2, 3, etc.) occur with a higher frequency than larger digits (4, 5, 6, etc.). 

According to Benford's Law, the distribution of leading digits follows a logarithmic pattern, where smaller digits are more likely to be the first digit in a number. This surprising and counterintuitive property is frequently encountered in diverse datasets such as financial transactions, population numbers, and scientific data, making Benford's Law a useful tool for 
detecting anomalies and irregularities in numerical datasets.

In this tutorial, we analyze the distribution of leading digits in tax deduction, population, GDP, COVID numbers and also pixel distribution in images, with the objective of verifying whether the data adheres to Benford's Law. 

The observed frequencies of the leading digits are computed and compared against the expected frequencies predicted by Benford's Law. 

Relevant python code is available here: https://github.com/bnsreenu/python_for_microscopists/tree/master/323-Benfords%20Law

Additional Notes:
For the image data:
The code reads images in grayscale using opencv library, computes the DCT coefficients, and plots the observed Benford's Law distribution for each image. 

In case you wonder why go through the pain of converting pixel values to DCT...

In the context of Benford's Law, the distribution of leading digits is expected to follow a logarithmic pattern, where smaller digits (1, 2, 3) occur more frequently than larger digits (4, 5, 6, 7, 8, 9).  When pixel values are confined to a small range, it can disrupt this natural logarithmic distribution. For example, in 8 bit images, our pixels have values between 0 to 255. So any bright pixel will always have a leading digit of 2 and never have values 3 or greater.",338 - Understanding the Benford's Law of Probability,2024-07-10T07:00:18+00:00
1898000,"Surrogate optimization is a method used to solve optimization problems that are expensive or time-consuming to evaluate directly. It relies on constructing a surrogate model (also known as a metamodel) that approximates the objective function based on a limited number of evaluations. The surrogate model is then used to guide the search for the optimal solution. This approach is particularly useful when dealing with complex simulations, physical experiments, or other computationally expensive tasks.

Unlike traditional metaheuristic approaches like Particle Swarm Optimization (PSO) and Simulated Annealing (SA), surrogate optimization is not strictly a metaheuristic. It can be combined with metaheuristics or other optimization techniques to enhance their efficiency.

Surrogate optimization is often considered a Bayesian approach because it incorporates Bayesian principles in its methodology, particularly through the use of Gaussian Processes (GPs) and Bayesian optimization techniques.

To get a practical understanding, watch this video where we implement a simple example of surrogate optimization using the same objective function from our PSO tutorial (https://youtu.be/FRXsQ6qbJbs). We'll use a popular surrogate model called Gaussian Process (GP) and the Bayesian Optimization framework.

Code Link: https://github.com/bnsreenu/python_for_microscopists/blob/master/339_surrogate_optimization.ipynb
Video Number: 339",339 - Surrogate Optimization explained using simple python code,2024-06-26T07:00:02+00:00
2320000,"A video walkthrough of up an experiment testing various large language models for generating hashtag #Python code for scientific image analysis. 

340 - Comparing Top Large Language Models for Python Code Generation

The task: segment nuclei in a multichannel hashtag #microscopy image (proprietary format), measure mean intensities of other channels in the segmented nuclei regions, calculate ratios and report in csv.

All well-hyped models are tested:
- Claude 3.5 Sonnet
- ChatGPT-4o
- Meta AI Llama 3.1 405-B
- Google Gemini 1.5 Flash
- Microsoft Copilot (in God's name, how do I find the model name?)

All generated code can be found here: https://github.com/bnsreenu/python_for_microscopists/tree/master/340-Comparing%20Top%20Large%20Language%20Models%20for%20Python%20Code%20Generation

A summary of my findings:
- When I didn't specify how to segment, all needed a bit of hand-holding. Claude did the best here.
- When I asked for Stardist segmentation, ChatGPT-4 nailed it on the first try, with minimal code. 
- Claude and Meta AI weren't far behind, just needed a small tweak (normalize pixel values).
- Gemini and Copilot... well, I'm super disappointed with their performance. Didn't manage to run the code at all, even after many prompts. 

With Stardist-based segmentation, the code generated by ChatGPT, Claude, and Meta AI produced statistically identical results for the intensity measurements. 

While AI is making rapid progress, the need for detailed prompting to obtain reliable results underscores the continued importance of domain expertise and basic coding skills. 

Title: 
340 - Comparing Top Large Language Models for Python Code Generation",340 - Comparing Top Large Language Models for Python Code Generation,2024-07-31T10:00:36+00:00
1306000,"What is Sholl Analysis?
Sholl analysis is a method used in neuroscience to quantify the complexity of neuronal branching patterns. It is performed to measure how neuron dendrites or axons branch out from the cell body.

How is it performed?
A series of concentric circles is overlaid on an image of a neuron.
The circles are centered on the cell body (soma).
The number of times neuronal processes intersect each circle is counted.
When performing automated analysis, the soma is assumed to be the largest, brightest object in the image.

Concentric circles are Placed around the soma because branching typically radiates outward. Usually spaced at regular intervals (e.g., 10 m apart). Extend far enough to cover the entire dendritic field.

Analysis:
The intersection counts are plotted against circle radius. This plot shows how branching complexity changes with distance from the soma.

This type of analysis is usually performed when ...
Comparing neuron morphology between different cell types or conditions.
Assessing changes in neuronal structure during development or disease.

Image credit:
https://www.mbfbioscience.com/wp-content/uploads/2022/06/neurontracingconfocalmicroscopy-661x1024.png

Code: https://github.com/bnsreenu/python_for_microscopists/tree/master/341-342%20-%20Sholl%20Analysis",341 - Sholl Analysis using python coding,2024-08-28T09:01:00+00:00
825000,"This video tutorial cover the topics of:
- performing Sholl analysis on individual images in a directory and combining them
- performing Sholl analysis on individual soma in a single image showing multiple soma and combining them.

When combining Sholl profiles from multiple neurons, we often encounter a challenge: different neurons may have different sizes, leading to varying maximum radii in their Sholl analyses. Tdifferent neurons may have different sizes, leading to varying maximum radii in their Sholl analyses..

To address this:

We first determine the largest radius used across all neurons.
We then create a common set of radii points, typically ranging from the smallest to the largest radius used in any of the analyses.
For each neuron's profile, we use interpolation to estimate what the intersection counts would be at these common radii points. Interpolation essentially ""fills in"" values between the actual measured points, allowing us to estimate intersection counts at radii that weren't directly measured for that particular neuron. This process results in each neuron having estimated intersection counts for the same set of radii, even if the original measurements didn't extend to all of these radii.

With this standardized data, we can now directly compare and average the profiles across all neurons, calculating mean intersection counts and standard deviations at each radius point.

This interpolation step is crucial because it allows us to meaningfully combine data from neurons of different sizes or from images with different scales. It ensures that when we calculate the average profile or compare neurons, we're comparing equivalent points along the radius, regardless of the absolute size of each neuron.

Code:",342 Sholl Analysis: Aggregating Sholl Profiles from Multiple Soma,2024-09-11T07:00:14+00:00
731000,"This tutorial walks you through the process of extracting high resolution images of individual cores from a Tissue Microarray (TMA). The process involves detecting cores in QuPath, saving the locations in a text file and saving core images using custom python code. 

QuPath download link: https://qupath.github.io/

Python code: https://github.com/bnsreenu/python_for_microscopists/tree/master/343-344-TMA%20Core%20Extraction%20and%20Analysis",343 De-arraying Tissue Microarrays (TMA) using Qupath and python code,2024-09-25T09:00:28+00:00
751000,"This tutorial walks you through the process of extracting high resolution core images from a Tissue Microarray (TMA) followed by analysis. 

The analysis part includes, color separation followed by nuclei segmentation using StarDist, pretrained deep learning model and calculating mean intensity of brown pixels in the nuclear and extra-nuclear (cytosol) regions, respectively.

Code: https://github.com/bnsreenu/python_for_microscopists/blob/master/343-344-TMA%20Core%20Extraction%20and%20Analysis/344-color_separate_and_nuclei_analysis_multi_cores_automated_V3.0.py",344 Color separation and nuclei segmentation in cores extracted from TMA,2024-10-09T07:00:22+00:00
1924000,"Image Annotation Made Easy with DigitalSreeni's Python Tool
In this video, I walk you through my Python-based image annotation application and its associated tools, providing a step-by-step demo to help you get started.

Topics Covered:

--Installation of the Python library for image annotation, along with setting up Anaconda and configuring your environment.
- Creating new projects and adding 2D and multi-dimensional images (TIFF, CZI).
- Manual annotation of 2D images and slices from multi-dimensional images using polygon and rectangle tools.
- Semi-automatic annotations with the Segment Anything Model (SAM).
- Renaming and assigning colors to classes for better organization.
- Exporting annotations to various formats: COCO JSON, YOLO v8, labeled images, semantic images, Pascal VOC bounding boxes.
- Verifying exported annotations by reloading them into the program.

Additional Tools:
- Annotation statistics
- Combining JSON annotations
- Data splitting
- Patch extraction
- Data augmentation of images and annotations

Links:
GitHub repository: https://github.com/bnsreenu/digitalsreeni-image-annotator
PyPI for pip install info: https://pypi.org/project/digitalsreeni-image-annotator

To Install:
pip install digitalsreeni-image-annotator

Once installed, simply type sreeni in your command prompt within the correct environment to launch the application.

You can download SAM models from the following links. Please be cautious about the large model on systems with limited memory. 
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_t.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_s.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_b.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_l.pt

It is recommended to place the SAM models in a directory from where you normally start the application to avoid multiple downloads of the same models from the Ultralytics server.",Annotate Images Like a Pro: Python Image Annotation Tool Walkthrough,2024-09-28T18:00:34+00:00
3275000,"Image Annotation Made Easy with DigitalSreeni's Python Tool:

In this video, I walk you through my Python-based image annotation application and its associated tools, providing a step-by-step demo to help you get started.

Topics Covered:

- Creating new projects and adding 2D and multi-dimensional images (TIFF, CZI).
- Searching for projects based on metadata
- Manual annotation of 2D images and slices from multi-dimensional images using polygon and rectangle tools.
- Semi-automatic annotations with the Segment Anything Model (SAM).
- Renaming and assigning colors to classes for better organization.
- Exporting annotations to various formats: COCO JSON, YOLO v8, labeled images, semantic images, Pascal VOC bounding boxes.
- Training custom model using YOLO
- Using the trained model to predict on images and converting predictions to annotations

Additional Tools:
- Annotation statistics
- Combining JSON annotations
- Data splitting
- Patch extraction
- Data augmentation of images and annotations

Links:
GitHub repository: https://github.com/bnsreenu/digitalsreeni-image-annotator
PyPI for pip install info: https://pypi.org/project/digitalsreeni-image-annotator

To Install:
pip install digitalsreeni-image-annotator

Once installed, simply type sreeni in your command prompt within the correct environment to launch the application.

You can download SAM models from the following links. Please be cautious about the large model on systems with limited memory. 
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_t.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_s.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_b.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_l.pt

It is recommended to place the SAM models in a directory from where you normally start the application to avoid multiple downloads of the same models from the Ultralytics server.",Annotate Images Like a Pro: Python Image Annotation Tool Demo,2024-10-17T01:49:35+00:00
199000,"In this video, I walk you through the installation process of my Python-based image annotation application

Links:
GitHub repository: https://github.com/bnsreenu/digitalsreeni-image-annotator
PyPI for pip install info: https://pypi.org/project/digitalsreeni-image-annotator

To Install:
pip install digitalsreeni-image-annotator

Once installed, simply type sreeni in your command prompt within the correct environment to launch the application.

You can download SAM models from the following links. Please be cautious about the large model on systems with limited memory. 
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_t.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_s.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_b.pt
https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_l.pt

It is recommended to place the SAM models in a directory from where you normally start the application to avoid multiple downloads of the same models from the Ultralytics server.",How to install DigitalSreeni Image Annotator,2024-10-17T03:00:12+00:00
733000,"In this video, I break down our latest research on a new blood test for distinguishing benign prostatic hyperplasia (BPH) from prostate cancer using GASP-1 protein levels. Collaborating with Halcyon Diagnostics and Dr. Rick Siderits from Rutgers University, we leveraged advanced tissue analysis, including dearraying TMAs, color separation, and intensity measurements using Python tools. 

Our study reveals significant differences in GASP-1 expression between prostate cancer, BPH, and healthy tissues, highlighting its potential as a diagnostic marker. Watch to see how this breakthrough could complement PSA testing, reduce unnecessary biopsies, and improve prostate cancer detection. 

For more details, find the full study in the October 2024 issue of CANCERS: https://www.mdpi.com/2072-6694/16/21/3659

GitHub Link: https://github.com/bnsreenu/TMA-dearray-stain-separation

Halcyon information: https://www.halcyonrx.com/


Video: 345",A New Blood Test to Differentiate Prostate Cancer from BPH Using GASP-1 Protein,2024-11-08T08:00:57+00:00
928000,"This tutorial is a walkthrough of 3D Sholl analysis using python. The code creates a series of concentric spherical shells around the soma center by calculating the distance of each point in 3D space from the soma center. For each spherical shell, it then counts how many times the skeletonized neuron intersects with that shell by performing a logical AND operation between the neuron skeleton and the shell, effectively counting the number of neuron branches at each distance from the soma.

In simpler terms: Imagine creating a series of increasingly larger hollow spheres centered on the soma, and for each sphere, counting how many times the neuron's branches pass through that sphere's surface.

Link to the code: https://github.com/bnsreenu/python_for_microscopists/tree/master/346-3D%20Sholl%20Analysis

Video 346",346 - 3D Sholl Analysis Using Python,2024-11-12T05:56:39+00:00
678000,"A demonstration of image retrieval system built with python using Vision transformer and FAISS. 

A Python-based content-based image retrieval (CBIR) system using Vision Transformer (ViT) features and FAISS indexing. This application provides both a graphical user interface (GUI) and programmatic API for indexing and searching similar images.

Key features:
Vision Transformer Features: Utilizes ViT-B/16 model pre-trained on ImageNet for robust feature extraction
Fast Similarity Search: Implements FAISS IVF indexing for efficient similarity search
Cross-Platform Support: Works on Windows, macOS, and Linux
User-Friendly GUI:
Interactive interface for feature extraction and image search
Double-click or right-click to open images and containing folders
Progress tracking for batch operations
Multiple Image Format Support: Handles PNG, JPG, JPEG, and WebP formats
GPU Acceleration: Optional GPU support for faster processing when available

For code and additional details:
https://github.com/bnsreenu/vit-image-retrieval
https://pypi.org/project/vit-image-retrieval/",351 - Image Retrieval Made Easy With GUI. Uses ViT and FAISS,
913000,"Automate Google Search Results to Excel Using python
Video: 347 

Link to python code: https://github.com/bnsreenu/python_for_microscopists/tree/master/347-Automate%20Google%20Search%20Results%20to%20Excel",347 - Automate Google Search Results to Excel,2024-12-04T08:00:00+00:00
1327000,"Image Similarity Search with VGG16 and Cosine Distance in python.

This is a walkthrough python tutorial for image similarity search using VGG16 as feature extractor and cosine distance as a metric for similarity. 

Cosine Similarity can be used to compare vectors:
- Cosine similarity compares how ""aligned"" two vectors are (like comparing directions they point)
- It measures the cosine of the angle between two vectors:
- Formula: cos() = (AB)/(||A||||B||)
- Result ranges from -1 (opposite) to 1 (identical)
- Value of 0 indicates perpendicularity (no similarity)
- We compare query vector against all database vectors

Link to python code:",348 - Image Similarity Search with VGG16 and Cosine Distance,
1542000,"What is FAISS?

- Faiss is a library for efficient similarity search and clustering of dense vectors. 
- Optimized for searching through millions or billions of high-dimensional vectors quickly
- Faiss contains several methods for similarity search.

Two main approaches (that we will be focusing on):
IndexFlatL2: Exact L2 matching but faster than manual implementation
IndexIVF (Inverted file): Clusters similar features together, only searches relevant clusters

IndexFlatL2 is similar to our cosine distance matching from the previous tutorial.
You may not notice any speed difference both, especially for smaller datasets. 
For large datasets, IndexFlatL2 will still be slow since it does exhaustive search.

That's where IndexIVF becomes valuable 
 (by reducing the number of comparisons needed through clustering.)

References: 
https://arxiv.org/abs/1702.08734
https://arxiv.org/abs/2401.08281

Python code available here:",349 - Understanding FAISS for efficient similarity search of dense vectors,
1131000,"This is a walkthrough python tutorial to build an Image Retrieval System using Vision Transformer (ViT) and FAISS.

Here, we implement a system for finding similar images using feature-based similarity search. 
It extracts visual features from images using a neural network and enables fast similarity 
search through the following main components:

1. Feature Extraction: Converts images into numerical feature vectors that capture their 
   visual characteristics (handled by a separate ImageFeatureExtractor class)

2. Indexing: 
   - Processes a directory of images and extracts their features
   - Stores these features in a FAISS index (Facebook AI Similarity Search) 
   - Maintains metadata about each indexed image (path, filename, indexing date)

3. Search: 
   - Takes a query image and finds the k most similar images from the indexed collection
   - Uses IndexIVFFlat to measure similarity between images
   - Returns matched images sorted by similarity score

Note about IndexIVFFlat:
    - Uses a ""divide and conquer"" approach
    - First divides vectors into clusters/regions
    - When searching:
      * First finds which clusters are most relevant
      * Only searches within those chosen clusters
    - Requires two extra steps:
      * Training: Learning how to divide vectors into clusters
      * nprobe: Choosing how many clusters to check (tradeoff between speed and accuracy)
    - Usually much faster for large datasets
    - Might miss some matches (approximate search) but usually good enough

Python code link:",350 - Efficient Image Retrieval with Vision Transformer (ViT) and FAISS,
